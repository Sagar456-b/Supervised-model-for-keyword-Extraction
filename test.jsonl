{"id":"2007","title":"The creation of a high-fidelity finite element model of the kidney for use in trauma research","abstract":"A detailed finite element model of the human kidney for trauma research has been created directly from the National Library of Medicine Visible Human Female (VHF) Project data set. An image segmentation and organ reconstruction software package has been developed and employed to transform the 2D VHF images into a 3D polygonal representation. Nonuniform rational B-spline (NURBS) surfaces were then mapped to the polygonal surfaces, and were finally utilized to create a robust 3D hexahedral finite element mesh within a commercially available meshing software. The model employs a combined viscoelastic and hyperelastic material model to successfully simulate the behaviour of biological soft tissues. The finite element model was then validated for use in biomechanical research","tok_text":"the creation of a high-fidel finit element model of the kidney for use in trauma research \n a detail finit element model of the human kidney for trauma research ha been creat directli from the nation librari of medicin visibl human femal ( vhf ) project data set . an imag segment and organ reconstruct softwar packag ha been develop and employ to transform the 2d vhf imag into a 3d polygon represent . nonuniform ration b-spline ( nurb ) surfac were then map to the polygon surfac , and were final util to creat a robust 3d hexahedr finit element mesh within a commerci avail mesh softwar . the model employ a combin viscoelast and hyperelast materi model to success simul the behaviour of biolog soft tissu . the finit element model wa then valid for use in biomechan research","ordered_present_kp":[18,56,74,193,268,285,303,381,362,433,468,523,634,692,761],"keyphrases":["high-fidelity finite element model","kidney","trauma research","National Library of Medicine","image segmentation","organ reconstruction","software package","2D VHF images","3D polygonal representation","NURBS","polygonal surfaces","3D hexahedral finite element mesh","hyperelastic material model","biological soft tissues","biomechanical research","Visible Human Female project","medical data set","physically based animation","nonuniform rational B-spline surfaces","viscoelastic model"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","M","U","R","R"]}
{"id":"2042","title":"Hybrid simulation of space plasmas: models with massless fluid representation of electrons. IV. Kelvin-Helmholtz instability","abstract":"For pt.III. see Prikl. Mat. Informatika, MAKS Press, no. 4, p. 5-56 (2000). This is a survey of the literature on hybrid simulation of the Kelvin-Helmholtz instability. We start with a brief review of the theory: the simplest model of the instability - a transition layer in the form of a tangential discontinuity; compressibility of the medium; finite size of the velocity shear region; pressure anisotropy. We then describe the electromagnetic hybrid model (ions as particles and electrons as a massless fluid) and the main numerical schemes. We review the studies on two-dimensional and three-dimensional hybrid simulation of the process of particle mixing across the magnetopause shear layer driven by the onset of a Kelvin-Helmholtz instability. The article concludes with a survey of literature on hybrid simulation of the Kelvin-Helmholtz instability in finite-size objects: jets moving across the magnetic field in the middle of the field reversal layer; interaction between a magnetized plasma flow and a cylindrical plasma source with zero own magnetic field","tok_text":"hybrid simul of space plasma : model with massless fluid represent of electron . iv . kelvin-helmholtz instabl \n for pt . iii . see prikl . mat . informatika , mak press , no . 4 , p. 5 - 56 ( 2000 ) . thi is a survey of the literatur on hybrid simul of the kelvin-helmholtz instabl . we start with a brief review of the theori : the simplest model of the instabl - a transit layer in the form of a tangenti discontinu ; compress of the medium ; finit size of the veloc shear region ; pressur anisotropi . we then describ the electromagnet hybrid model ( ion as particl and electron as a massless fluid ) and the main numer scheme . we review the studi on two-dimension and three-dimension hybrid simul of the process of particl mix across the magnetopaus shear layer driven by the onset of a kelvin-helmholtz instabl . the articl conclud with a survey of literatur on hybrid simul of the kelvin-helmholtz instabl in finite-s object : jet move across the magnet field in the middl of the field revers layer ; interact between a magnet plasma flow and a cylindr plasma sourc with zero own magnet field","ordered_present_kp":[0,16,42,86,368,399,485,526,674,744,988,1028,1053],"keyphrases":["hybrid simulation","space plasmas","massless fluid representation","Kelvin-Helmholtz instability","transition layer","tangential discontinuity","pressure anisotropy","electromagnetic hybrid model","three-dimensional hybrid simulation","magnetopause shear layer","field reversal layer","magnetized plasma flow","cylindrical plasma source"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"308","title":"On-line Homework\/Quiz\/Exam applet: freely available Java software for evaluating performance on line","abstract":"The Homework\/Quiz\/Exam applet is a freely available Java program that can be used to evaluate student performance on line for any content authored by a teacher. It has database connectivity so that student scores are automatically recorded. It allows several different types of questions. Each question can be linked to images and detailed story problems. Three levels of feedback are provided to student responses. It allows teachers to randomize the sequence of questions and to randomize which of several options is the correct answer in multiple-choice questions. The creation and editing of questions involves menu selections, button presses, and the typing of content; no programming knowledge is required. The code is open source in order to encourage modifications that will meet individual pedagogical needs","tok_text":"on-lin homework \/ quiz \/ exam applet : freeli avail java softwar for evalu perform on line \n the homework \/ quiz \/ exam applet is a freeli avail java program that can be use to evalu student perform on line for ani content author by a teacher . it ha databas connect so that student score are automat record . it allow sever differ type of question . each question can be link to imag and detail stori problem . three level of feedback are provid to student respons . it allow teacher to random the sequenc of question and to random which of sever option is the correct answer in multiple-choic question . the creation and edit of question involv menu select , button press , and the type of content ; no program knowledg is requir . the code is open sourc in order to encourag modif that will meet individu pedagog need","ordered_present_kp":[39,251,380,389,427,580,647,661,799],"keyphrases":["freely available Java software","database connectivity","images","detailed story problems","feedback","multiple-choice questions","menu selections","button presses","individual pedagogical needs","online Homework\/Quiz\/Exam applet","online student performance evaluation","teacher authored content","automatic student score recording","randomized question sequence","question editing","question creation","typing content"],"prmu":["P","P","P","P","P","P","P","P","P","M","M","R","R","R","R","R","R"]}
{"id":"215","title":"A conceptual framework for evaluation of information technology investments","abstract":"The decision to acquire a new information technology poses a number of serious evaluation and selection problems to technology managers, because the new system must not only meet current information requirements of the organisation, but also the needs for future expansion. Tangible and intangible benefits factors, as well as risks factors, must be identified and evaluated. The paper provides a review of ten major evaluation categories and available models, which fall under each category, showing their advantages and disadvantages in handling the above difficulties. This paper describes strategic implications involved in the selection decision, and the inherent difficulties in: (1) choosing or developing a model, (2) obtaining realistic inputs for the model, and (3) making tradeoffs among the conflicting factors. It proposes a conceptual framework to help the decision maker in choosing the most appropriate methodology in the evaluation process. It also offers a new model, called GAHP, for the evaluation problem combining integer goal linear programming and analytic hierarchy process (AHP) in a single hybrid multiple objective multi-criteria model. A goal programming methodology, with zero-one integer variables and mixed integer constraints, is used to set goal target values against which information technology alternatives are evaluated and selected. AHP is used to structure the evaluation process providing pairwise comparison mechanisms to quantify subjective, nonmonetary, intangible benefits and risks factors, in deriving data for the model. A case illustration is provided showing how GAHP can be formulated and solved","tok_text":"a conceptu framework for evalu of inform technolog invest \n the decis to acquir a new inform technolog pose a number of seriou evalu and select problem to technolog manag , becaus the new system must not onli meet current inform requir of the organis , but also the need for futur expans . tangibl and intang benefit factor , as well as risk factor , must be identifi and evalu . the paper provid a review of ten major evalu categori and avail model , which fall under each categori , show their advantag and disadvantag in handl the abov difficulti . thi paper describ strateg implic involv in the select decis , and the inher difficulti in : ( 1 ) choos or develop a model , ( 2 ) obtain realist input for the model , and ( 3 ) make tradeoff among the conflict factor . it propos a conceptu framework to help the decis maker in choos the most appropri methodolog in the evalu process . it also offer a new model , call gahp , for the evalu problem combin integ goal linear program and analyt hierarchi process ( ahp ) in a singl hybrid multipl object multi-criteria model . a goal program methodolog , with zero-on integ variabl and mix integ constraint , is use to set goal target valu against which inform technolog altern are evalu and select . ahp is use to structur the evalu process provid pairwis comparison mechan to quantifi subject , nonmonetari , intang benefit and risk factor , in deriv data for the model . a case illustr is provid show how gahp can be formul and solv","ordered_present_kp":[34,155,222,337,419,599,735,815,987,1031,1078,1109,1135,1172,1203,1298,302],"keyphrases":["information technology investments","technology managers","information requirements","intangible benefits","risks factors","evaluation categories","selection decision","tradeoffs","decision maker","analytic hierarchy process","hybrid multiple objective multi-criteria model","goal programming methodology","zero-one integer variables","mixed integer constraints","goal target values","information technology alternatives","pairwise comparison mechanisms","nonmonetary benefits","group decision process"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","M"]}
{"id":"250","title":"Aim for the enterprise: Microsoft Project 2002","abstract":"A long-time favorite of project managers, Microsoft Project 2002 is making its enterprise debut. Its new Web-based collaboration tools and improved scalability with OLAP support make it much easier to manage multiple Web projects with disparate workgroups and budgets","tok_text":"aim for the enterpris : microsoft project 2002 \n a long-tim favorit of project manag , microsoft project 2002 is make it enterpris debut . it new web-bas collabor tool and improv scalabl with olap support make it much easier to manag multipl web project with dispar workgroup and budget","ordered_present_kp":[24,146,179,192,266,280],"keyphrases":["Microsoft Project 2002","Web-based collaboration tools","scalability","OLAP support","workgroups","budgets","multiple Web project management"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"1996","title":"Quality image metrics for synthetic images based on perceptual color differences","abstract":"Due to the improvement of image rendering processes, and the increasing importance of quantitative comparisons among synthetic color images, it is essential to define perceptually based metrics which enable to objectively assess the visual quality of digital simulations. In response to this need, this paper proposes a new methodology for the determination of an objective image quality metric, and gives an answer to this problem through three metrics. This methodology is based on the LLAB color space for perception of color in complex images, a modification of the CIELab1976 color space. The first metric proposed is a pixel by pixel metric which introduces a local distance map between two images. The second metric associates, to a pair of images, a global value. Finally, the third metric uses a recursive subdivision of the images to obtain an adaptative distance map, rougher but less expensive to compute than the first method","tok_text":"qualiti imag metric for synthet imag base on perceptu color differ \n due to the improv of imag render process , and the increas import of quantit comparison among synthet color imag , it is essenti to defin perceptu base metric which enabl to object assess the visual qualiti of digit simul . in respons to thi need , thi paper propos a new methodolog for the determin of an object imag qualiti metric , and give an answer to thi problem through three metric . thi methodolog is base on the llab color space for percept of color in complex imag , a modif of the cielab1976 color space . the first metric propos is a pixel by pixel metric which introduc a local distanc map between two imag . the second metric associ , to a pair of imag , a global valu . final , the third metric use a recurs subdivis of the imag to obtain an adapt distanc map , rougher but less expens to comput than the first method","ordered_present_kp":[0,24,45,90,171,207,261,279,491,562,616,655,741,786,827],"keyphrases":["quality image metrics","synthetic images","perceptual color differences","image rendering","color images","perceptually based metrics","visual quality","digital simulations","LLAB color space","CIELab1976 color space","pixel by pixel metric","local distance map","global value","recursive subdivision","adaptative distance map"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2162","title":"More constructions for Boolean algebras","abstract":"We construct Boolean algebras with prescribed behaviour concerning depth for the free product of two Boolean algebras over a third, in ZFC using pcf; assuming squares we get results on ultraproducts. We also deal with the family of cardinalities and topological density of homomorphic images of Boolean algebras (you can translate it to topology-on the cardinalities of closed subspaces); and lastly we deal with inequalities between cardinal invariants, mainly d(B)\/sup kappa \/<|B| implies ind(B)>\/sup kappa \/V Depth(B)>or=log(|B|)","tok_text":"more construct for boolean algebra \n we construct boolean algebra with prescrib behaviour concern depth for the free product of two boolean algebra over a third , in zfc use pcf ; assum squar we get result on ultraproduct . we also deal with the famili of cardin and topolog densiti of homomorph imag of boolean algebra ( you can translat it to topology-on the cardin of close subspac ) ; and lastli we deal with inequ between cardin invari , mainli d(b)\/sup kappa \/<|b| impli ind(b)>\/sup kappa \/v depth(b)>or = log(|b| )","ordered_present_kp":[19,71,112,166,209,286,427],"keyphrases":["Boolean algebras","prescribed behaviour","free product","ZFC","ultraproducts","homomorphic images","cardinal invariants"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"2127","title":"System embedding. Polynomial equations","abstract":"The class of solutions of the polynomial equations including their generalizations in the form of the Bezout matrix identities was constructed analytically using the technology of constructive system embedding. The structure of a solution depends on the number of steps of the Euclidean algorithm and is obtained explicitly by appropriate substitutions. Illustrative and descriptive examples are presented","tok_text":"system embed . polynomi equat \n the class of solut of the polynomi equat includ their gener in the form of the bezout matrix ident wa construct analyt use the technolog of construct system embed . the structur of a solut depend on the number of step of the euclidean algorithm and is obtain explicitli by appropri substitut . illustr and descript exampl are present","ordered_present_kp":[15,111,172,257],"keyphrases":["polynomial equations","Bezout matrix identities","constructive system embedding","Euclidean algorithm","determinate systems"],"prmu":["P","P","P","P","M"]}
{"id":"290","title":"MEMS applications in computer disk drive dual-stage servo systems","abstract":"We present a decoupled discrete time pole placement design method, which can be combined with a self-tuning scheme to compensate variations in the microactuator's (MA's) resonance mode. Section I of the paper describes the design and fabrication of a prototype microactuator with an integrated gimbal structure. Section II presents a decoupled track-following controller design and a self-tuning control scheme to compensate for the MA's resonance mode variations","tok_text":"mem applic in comput disk drive dual-stag servo system \n we present a decoupl discret time pole placement design method , which can be combin with a self-tun scheme to compens variat in the microactu 's ( ma 's ) reson mode . section i of the paper describ the design and fabric of a prototyp microactu with an integr gimbal structur . section ii present a decoupl track-follow control design and a self-tun control scheme to compens for the ma 's reson mode variat","ordered_present_kp":[14,0,190,70,149,365],"keyphrases":["MEMS","computer disk drive dual-stage servo systems","decoupled discrete time pole placement design method","self-tuning scheme","microactuator","track-following controller design","servo control","hard disk drives","electrostatic design","fabrication process"],"prmu":["P","P","P","P","P","P","R","M","M","M"]}
{"id":"1956","title":"Dynamical transition to periodic motions of a recurrent bus induced by nonstops","abstract":"We study the dynamical behavior of a recurrent bus on a circular route with many bus stops when the recurrent bus passes some bus stops without stopping. The recurrent time (one period) is described in terms of a nonlinear map. It is shown that the recurrent bus exhibits the complex periodic behaviors. The dynamical transitions to periodic motions occur by increasing nonstops. The periodic motions depend on the property of an attractor of the nonlinear map. The period n of the attractor varies sensitively with the number of nonstops","tok_text":"dynam transit to period motion of a recurr bu induc by nonstop \n we studi the dynam behavior of a recurr bu on a circular rout with mani bu stop when the recurr bu pass some bu stop without stop . the recurr time ( one period ) is describ in term of a nonlinear map . it is shown that the recurr bu exhibit the complex period behavior . the dynam transit to period motion occur by increas nonstop . the period motion depend on the properti of an attractor of the nonlinear map . the period n of the attractor vari sensit with the number of nonstop","ordered_present_kp":[0,17,36,55,113,201,252,311,446],"keyphrases":["dynamical transition","periodic motions","recurrent bus","nonstops","circular route","recurrent time","nonlinear map","complex periodic behaviors","attractor"],"prmu":["P","P","P","P","P","P","P","P","P"]}
{"id":"228","title":"Rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel electrophoresis of DNA","abstract":"Large-scale genotyping, mapping and expression profiling require affordable, fully automated high-throughput devices enabling rapid, high-performance analysis using minute quantities of reagents. In this paper, we describe a new combination of microwell polymerase chain reaction (PCR) based DNA amplification technique with automated ultrathin-layer gel electrophoresis analysis of the resulting products. This technique decreases the reagent consumption (total reaction volume 0.75-1 mu L), the time requirement of the PCR (15-20 min) and subsequent ultrathin-layer gel electrophoresis based fragment analysis (5 min) by automating the current manual procedure and reducing the human intervention using sample loading robots and computerized real time data analysis. Small aliquots (0.2 mu L) of the submicroliter size PCR reaction were transferred onto loading membranes and analyzed by ultrathin-layer gel electrophoresis which is a novel, high-performance and automated microseparation technique. This system employs integrated scanning laser-induced fluorescence-avalanche photodiode detection and combines the advantages of conventional slab and capillary gel electrophoresis. Visualization of the DNA fragments was accomplished by \"in migratio\" complexation with ethidium bromide during the electrophoresis process also enabling real time imaging and data analysis","tok_text":"rapid microwel polymeras chain reaction with subsequ ultrathin-lay gel electrophoresi of dna \n large-scal genotyp , map and express profil requir afford , fulli autom high-throughput devic enabl rapid , high-perform analysi use minut quantiti of reagent . in thi paper , we describ a new combin of microwel polymeras chain reaction ( pcr ) base dna amplif techniqu with autom ultrathin-lay gel electrophoresi analysi of the result product . thi techniqu decreas the reagent consumpt ( total reaction volum 0.75 - 1 mu l ) , the time requir of the pcr ( 15 - 20 min ) and subsequ ultrathin-lay gel electrophoresi base fragment analysi ( 5 min ) by autom the current manual procedur and reduc the human intervent use sampl load robot and computer real time data analysi . small aliquot ( 0.2 mu l ) of the submicrolit size pcr reaction were transfer onto load membran and analyz by ultrathin-lay gel electrophoresi which is a novel , high-perform and autom microsepar techniqu . thi system employ integr scan laser-induc fluorescence-avalanch photodiod detect and combin the advantag of convent slab and capillari gel electrophoresi . visual of the dna fragment wa accomplish by \" in migratio \" complex with ethidium bromid dure the electrophoresi process also enabl real time imag and data analysi","ordered_present_kp":[0,53,345,95,124,466,715,736,949,1193,1265],"keyphrases":["rapid microwell polymerase chain reaction","ultrathin-layer gel electrophoresis","large-scale genotyping","expression profiling","DNA amplification","reagent consumption","sample loading robots","computerized real time data analysis","automated microseparation","complexation with ethidium bromide","real time imaging","rapid high-performance analysis","automated electrophoresis analysis","integrated scanning LIF APD detection"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","M"]}
{"id":"370","title":"Virtual borders, real laws [Internet activity and treaties]","abstract":"National governments are working to tame activity on the Internet. They have worked steadily to extend control over online activities that they believe affect their interests, even when the activities occur outside their borders. These usually involve what governments regard as their domain: protecting public order, enforcing commercial laws, and, occasionally, protecting consumer interests. Methods have included assertions or legal jurisdiction based on where material is accessible instead of where it originates, and the blocking of sites, service providers, or entire high level domains from access by citizens. Such instances are mentioned in this article. Whilst larger companies are able to defend themselves against overseas lawsuits, individuals and smaller organizations lack the resources to defend what are often normal business activities at home, but could violate the laws of local jurisdictions in countries around the world. The problems of libel are discussed as are the blocking of certain sites by certain countries. Efforts to draw up Internet treaties are also mentioned","tok_text":"virtual border , real law [ internet activ and treati ] \n nation govern are work to tame activ on the internet . they have work steadili to extend control over onlin activ that they believ affect their interest , even when the activ occur outsid their border . these usual involv what govern regard as their domain : protect public order , enforc commerci law , and , occasion , protect consum interest . method have includ assert or legal jurisdict base on where materi is access instead of where it origin , and the block of site , servic provid , or entir high level domain from access by citizen . such instanc are mention in thi articl . whilst larger compani are abl to defend themselv against oversea lawsuit , individu and smaller organ lack the resourc to defend what are often normal busi activ at home , but could violat the law of local jurisdict in countri around the world . the problem of libel are discuss as are the block of certain site by certain countri . effort to draw up internet treati are also mention","ordered_present_kp":[58,28,160,434,708,994],"keyphrases":["Internet activity","national governments","online activities","legal jurisdiction","lawsuits","Internet treaties","public order protection","commercial laws enforcement","consumer interests protection","Internet sites blocking"],"prmu":["P","P","P","P","P","P","R","R","R","R"]}
{"id":"2082","title":"Managing safety and strategic stocks to improve materials requirements planning performance","abstract":"This paper provides a methodology for managing safety and strategic stocks in materials requirements planning (MRP) environments to face uncertainty in market demand. A set of recommended guidelines suggest where to position, how to dimension and when to replenish both safety and strategic stocks. Trade-offs between stock positioning and dimensioning and between stock positioning and replenishment order triggering are outlined. The study reveals also that most of the decisions are system specific, so that they should be evaluated in a quantitative manner through simulation. A case study is reported, where the benefits from adopting the new proposed methodology lie in achieving the target service level even under peak demand conditions, with the value of safety stocks as a whole growing only by about 20 per cent","tok_text":"manag safeti and strateg stock to improv materi requir plan perform \n thi paper provid a methodolog for manag safeti and strateg stock in materi requir plan ( mrp ) environ to face uncertainti in market demand . a set of recommend guidelin suggest where to posit , how to dimens and when to replenish both safeti and strateg stock . trade-off between stock posit and dimens and between stock posit and replenish order trigger are outlin . the studi reveal also that most of the decis are system specif , so that they should be evalu in a quantit manner through simul . a case studi is report , where the benefit from adopt the new propos methodolog lie in achiev the target servic level even under peak demand condit , with the valu of safeti stock as a whole grow onli by about 20 per cent","ordered_present_kp":[159,41,196,17,736,674,698],"keyphrases":["strategic stocks","materials requirements planning","MRP","market demand","service level","peak demand","safety stocks","inventory management","variance control","stock replenishment"],"prmu":["P","P","P","P","P","P","P","M","U","R"]}
{"id":"335","title":"Fresh voices, big ideas [IBM internship program]","abstract":"IBM is matching up computer-science and MBA students with its business managers in an 11-week summer internship program and challenging them to develop innovative technology ideas","tok_text":"fresh voic , big idea [ ibm internship program ] \n ibm is match up computer-sci and mba student with it busi manag in an 11-week summer internship program and challeng them to develop innov technolog idea","ordered_present_kp":[28],"keyphrases":["internship program","IBM business managers","MBA college students","computer-science students","patents"],"prmu":["P","R","M","R","U"]}
{"id":"249","title":"Randomized two-process wait-free test-and-set","abstract":"We present the first explicit, and currently simplest, randomized algorithm for two-process wait-free test-and-set. It is implemented with two 4-valued single writer single reader atomic variables. A test-and-set takes at most 11 expected elementary steps, while a reset takes exactly 1 elementary step. Based on a finite-state analysis, the proofs of correctness and expected length are compressed into one table","tok_text":"random two-process wait-fre test-and-set \n we present the first explicit , and current simplest , random algorithm for two-process wait-fre test-and-set . it is implement with two 4-valu singl writer singl reader atom variabl . a test-and-set take at most 11 expect elementari step , while a reset take exactli 1 elementari step . base on a finite-st analysi , the proof of correct and expect length are compress into one tabl","ordered_present_kp":[0,98,180,259,341],"keyphrases":["randomized two-process wait-free test-and-set","randomized algorithm","4-valued single writer single reader atomic variables","expected elementary steps","finite-state analysis","correctness proofs","symmetry breaking","asynchronous distributed protocols","fault-tolerance","shared memory","wait-free read\/write registers"],"prmu":["P","P","P","P","P","R","U","U","U","U","M"]}
{"id":"1972","title":"Online auctions: dynamic pricing and the lodging industry","abstract":"The traditional channels of distribution for overnight accommodation are rapidly being displaced by Web site scripting, online intermediaries, and specialty brokers. Businesses that pioneered Internet usage relied on it as a sales and marketing alternative to predecessor product distribution channels. As such, Web sites replace the traditional trading model to the Internet. Web-enabled companies are popular because the medium renders the process faster, less costly, highly reliable, and secure. Auction-based models impact business models by converting the price setting mechanism from supplier-centric to market-centric and transforming the trading model from \"one to many\" to \"many to many.\" Historically, pricing was based on the cost of production plus a margin of profit. Traditionally, as products and services move through the supply chain, from the producer to the consumer, various intermediaries added their share of profit to the price. As Internet based mediums of distribution become more prevalent, traditional pricing models are being supplanted with dynamic pricing. A dynamic pricing model represents a flexible system that changes prices not only from product to product, but also from customer to customer and transaction to transaction. Many industry leaders are skeptical of the long run impact of online auctions on lodging industry profit margins, despite the fact pricing theory suggests that an increase in the flow of information results in efficient market pricing. The future of such endeavors remains promising, but controversial","tok_text":"onlin auction : dynam price and the lodg industri \n the tradit channel of distribut for overnight accommod are rapidli be displac by web site script , onlin intermediari , and specialti broker . busi that pioneer internet usag reli on it as a sale and market altern to predecessor product distribut channel . as such , web site replac the tradit trade model to the internet . web-en compani are popular becaus the medium render the process faster , less costli , highli reliabl , and secur . auction-bas model impact busi model by convert the price set mechan from supplier-centr to market-centr and transform the trade model from \" one to mani \" to \" mani to mani . \" histor , price wa base on the cost of product plu a margin of profit . tradit , as product and servic move through the suppli chain , from the produc to the consum , variou intermediari ad their share of profit to the price . as internet base medium of distribut becom more preval , tradit price model are be supplant with dynam price . a dynam price model repres a flexibl system that chang price not onli from product to product , but also from custom to custom and transact to transact . mani industri leader are skeptic of the long run impact of onlin auction on lodg industri profit margin , despit the fact price theori suggest that an increas in the flow of inform result in effici market price . the futur of such endeavor remain promis , but controversi","ordered_present_kp":[0,16,36,88,133,151,176,213,243,252,346,517,543,788],"keyphrases":["online auctions","dynamic pricing","lodging industry","overnight accommodations","Web site scripting","online intermediaries","specialty brokers","Internet usage","sales","marketing","trading model","business models","price setting mechanism","supply chain"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"1937","title":"Nonlinear extrapolation algorithm for realization of a scalar random process","abstract":"A method of construction of a nonlinear extrapolation algorithm is proposed. This method makes it possible to take into account any nonlinear random dependences that exist in an investigated process and are described by mixed central moment functions. The method is based on the V. S. Pugachev canonical decomposition apparatus. As an example, the problem of nonlinear extrapolation is solved for a moment function of third order","tok_text":"nonlinear extrapol algorithm for realiz of a scalar random process \n a method of construct of a nonlinear extrapol algorithm is propos . thi method make it possibl to take into account ani nonlinear random depend that exist in an investig process and are describ by mix central moment function . the method is base on the v. s. pugachev canon decomposit apparatu . as an exampl , the problem of nonlinear extrapol is solv for a moment function of third order","ordered_present_kp":[0,45,189,266,337,278],"keyphrases":["nonlinear extrapolation algorithm","scalar random process","nonlinear random dependences","mixed central moment functions","moment function","canonical decomposition apparatus"],"prmu":["P","P","P","P","P","P"]}
{"id":"2146","title":"Trusted...or...trustworthy: the search for a new paradigm for computer and network security","abstract":"This paper sets out a number of major questions and challenges which include: (a) just what is meant by `trusted' or `trustworthy' systems after 20 years of experience, or more likely, lack of business level experience, with the 'trusted computer system' criteria anyway; (b) does anyone really care about the adoption of international standards for computer system security evaluation by IT product and system manufacturers and suppliers (IS 15408) and, if so, how does it all relate to business risk management anyway (IS 17799); (c) with the explosion of adoption of the microcomputer and personal computer some 20 years ago, has the industry abandoned all that it learnt about security during the `mainframe era'; or - `whatever happened to MULTICS' and its lessons; (d) has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases, particularly where Web or Internet-based information services are being proposed as the major `next best thing' in the IT industry; (e) has the `fourth generation' of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last `generation', the professionals who developed and created the applications for shared mainframe and minicomputer systems?","tok_text":"trust ... or ... trustworthi : the search for a new paradigm for comput and network secur \n thi paper set out a number of major question and challeng which includ : ( a ) just what is meant by ` trust ' or ` trustworthi ' system after 20 year of experi , or more like , lack of busi level experi , with the ' trust comput system ' criteria anyway ; ( b ) doe anyon realli care about the adopt of intern standard for comput system secur evalu by it product and system manufactur and supplier ( is 15408 ) and , if so , how doe it all relat to busi risk manag anyway ( is 17799 ) ; ( c ) with the explos of adopt of the microcomput and person comput some 20 year ago , ha the industri abandon all that it learnt about secur dure the ` mainfram era ' ; or - ` whatev happen to multic ' and it lesson ; ( d ) ha educ kept up with secur requir by industri and govern alik in the need for safe and secur oper of larg scale and network inform system on nation and intern base , particularli where web or internet-bas inform servic are be propos as the major ` next best thing ' in the it industri ; ( e ) ha the ` fourth gener ' of comput profession inherit the spirit of inform system manag and control that resid by necess with the last ` gener ' , the profession who develop and creat the applic for share mainfram and minicomput system ?","ordered_present_kp":[76,396,493,542,567,618,634,774,808,990,997,1165],"keyphrases":["network security","international standards","IS 15408","business risk management","IS 17799","microcomputer","personal computer","MULTICS","education","Web","Internet-based information services","information systems management","computer security","trusted systems","trustworthy systems","IT manufacturers","large scale information systems","fourth generation computer professionals","information systems control"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","R","R","R","R","R","R","R"]}
{"id":"2103","title":"The social impact of Internet gambling","abstract":"Technology has always played a role in the development of gambling practices and continues to provide new market opportunities. One of the fastest growing areas is that of Internet gambling. The effect of such technologies should not be accepted uncritically, particularly as there may be areas of potential concern based on what is known about problem gambling offline. This article has three aims. First, it overviews some of the main social concerns about the rise of Internet gambling. Second, it looks at the limited research that has been carried out in this area. Third, it examines whether Internet gambling is doubly addictive, given research that suggests that the Internet can be addictive itself. It is concluded that technological developments in Internet gambling will increase the potential for problem gambling globally, but that many of the ideas and speculations outlined in this article need to be addressed further by large-scale empirical studies","tok_text":"the social impact of internet gambl \n technolog ha alway play a role in the develop of gambl practic and continu to provid new market opportun . one of the fastest grow area is that of internet gambl . the effect of such technolog should not be accept uncrit , particularli as there may be area of potenti concern base on what is known about problem gambl offlin . thi articl ha three aim . first , it overview some of the main social concern about the rise of internet gambl . second , it look at the limit research that ha been carri out in thi area . third , it examin whether internet gambl is doubli addict , given research that suggest that the internet can be addict itself . it is conclud that technolog develop in internet gambl will increas the potenti for problem gambl global , but that mani of the idea and specul outlin in thi articl need to be address further by large-scal empir studi","ordered_present_kp":[4,21,127,702,605],"keyphrases":["social impact","Internet gambling","market opportunities","addiction","technological developments","electronic cash","psychology"],"prmu":["P","P","P","P","P","U","U"]}
{"id":"4","title":"Industry insiders loading up on cheap company stock","abstract":"A surge of telecom executives and directors purchasing their own companies, stock in the last two months points toward a renewed optimism in the beleaguered sector, say some observers, who view the rash of insider buying as a vote of confidence from management. Airgate PCS, Charter Communications, Cox Communications, Crown Castle International, Nextel Communications and Nortel Networks all have seen infusions of insider investment this summer, echoing trends in both the telecom industry and the national economy","tok_text":"industri insid load up on cheap compani stock \n a surg of telecom execut and director purchas their own compani , stock in the last two month point toward a renew optim in the beleagu sector , say some observ , who view the rash of insid buy as a vote of confid from manag . airgat pc , charter commun , cox commun , crown castl intern , nextel commun and nortel network all have seen infus of insid invest thi summer , echo trend in both the telecom industri and the nation economi","ordered_present_kp":[443,394],"keyphrases":["insider investment","telecom industry"],"prmu":["P","P"]}
{"id":"354","title":"Fault-tolerant Hamiltonian laceability of hypercubes","abstract":"It is known that every hypercube Q\/sub n\/ is a bipartite graph. Assume that n>or=2 and F is a subset of edges with |F|<or=n-2. We prove that there exists a Hamiltonian path in Q\/sub n\/-F between any two vertices of different partite sets. Moreover, there exists a path of length 2\/sup n\/-2 between any two vertices of the same partite set. Assume that n>or=3 and F is a subset of edges with |F|<or=n-3. We prove that there exists a Hamiltonian path in Q\/sub n\/-{v}-F between any two vertices in the partite set without v. Furthermore, all bounds are tight","tok_text":"fault-toler hamiltonian laceabl of hypercub \n it is known that everi hypercub q \/ sub n\/ is a bipartit graph . assum that n > or=2 and f is a subset of edg with |f|<or = n-2 . we prove that there exist a hamiltonian path in q \/ sub n\/-f between ani two vertic of differ partit set . moreov , there exist a path of length 2 \/ sup n\/-2 between ani two vertic of the same partit set . assum that n > or=3 and f is a subset of edg with |f|<or = n-3 . we prove that there exist a hamiltonian path in q \/ sub n\/-{v}-f between ani two vertic in the partit set without v. furthermor , all bound are tight","ordered_present_kp":[0,35,94,204,253,270],"keyphrases":["fault-tolerant Hamiltonian laceability","hypercubes","bipartite graph","Hamiltonian path","vertices","partite sets","edge subset","tight bounds"],"prmu":["P","P","P","P","P","P","R","R"]}
{"id":"311","title":"Information architecture without internal theory: an inductive design process","abstract":"This article suggests that Information Architecture (IA) design is primarily an inductive process. Although top-level goals, user attributes and available content are periodically considered, the process involves bottom-up design activities. IA is inductive partly because it lacks internal theory, and partly because it is an activity that supports emergent phenomena (user experiences) from basic design components. The nature of IA design is well described by Constructive Induction (CI), a design process that involves locating the best representational framework for the design problem, identifying a solution within that framework and translating it back to the design problem at hand. The future of IA, if it remains inductive or develops a body of theory (or both), is considered","tok_text":"inform architectur without intern theori : an induct design process \n thi articl suggest that inform architectur ( ia ) design is primarili an induct process . although top-level goal , user attribut and avail content are period consid , the process involv bottom-up design activ . ia is induct partli becaus it lack intern theori , and partli becaus it is an activ that support emerg phenomena ( user experi ) from basic design compon . the natur of ia design is well describ by construct induct ( ci ) , a design process that involv locat the best represent framework for the design problem , identifi a solut within that framework and translat it back to the design problem at hand . the futur of ia , if it remain induct or develop a bodi of theori ( or both ) , is consid","ordered_present_kp":[46,257,27,379,397,480],"keyphrases":["internal theory","inductive design process","bottom-up design activities","emergent phenomena","user experiences","constructive induction","information architecture design"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"369","title":"Nissan v. Nissan [trademark dispute]","abstract":"Is a trademark dispute a case of David v. Goliath or a corporation fending off a greedy opportunist? This paper discusses the case of Uzi Nissan, who is locked in a multimillion-dollar legal battle over whether or not his use of the nissan.com Internet domain name infringes upon Japan's Nissan Motor Co.'s trademark. At the heart of the matter is the impact of the global Internet on trademark law, which traditionally has been strongly influenced by geographic considerations. The paper discusses the background to the case from both sides and the issues involved","tok_text":"nissan v. nissan [ trademark disput ] \n is a trademark disput a case of david v. goliath or a corpor fend off a greedi opportunist ? thi paper discuss the case of uzi nissan , who is lock in a multimillion-dollar legal battl over whether or not hi use of the nissan.com internet domain name infring upon japan 's nissan motor co. 's trademark . at the heart of the matter is the impact of the global internet on trademark law , which tradit ha been strongli influenc by geograph consider . the paper discuss the background to the case from both side and the issu involv","ordered_present_kp":[19,163,259,393,412],"keyphrases":["trademark dispute","Uzi Nissan","nissan.com Internet domain name","global Internet","trademark law","Nissan Motor Company trademark"],"prmu":["P","P","P","P","P","M"]}
{"id":"401","title":"A digital-to-analog converter based on differential-quad switching","abstract":"A high-conversion-rate high-resolution oversampling digital-to-analog converter (DAC) for direct digital modulation is addressed in this paper. A new type of switching scheme, called differential-quad switching, is presented. To verify the feasibility of this scheme, essential parts with some auxiliary circuitry for interfacing were fabricated in a 0.8- mu m CMOS technology. Measured results show that the switching scheme provides 11-b resolution at 100 MSamples\/s and 6-b at 1 GSamples\/s. The degradation in signal-to-noise ratio is not observed for the variation of the supply voltage down to 1.5 V, which means the proposed scheme is suitable for low-voltage applications","tok_text":"a digital-to-analog convert base on differential-quad switch \n a high-conversion-r high-resolut oversampl digital-to-analog convert ( dac ) for direct digit modul is address in thi paper . a new type of switch scheme , call differential-quad switch , is present . to verifi the feasibl of thi scheme , essenti part with some auxiliari circuitri for interfac were fabric in a 0.8- mu m cmo technolog . measur result show that the switch scheme provid 11-b resolut at 100 msampl \/ s and 6-b at 1 gsampl \/ s. the degrad in signal-to-nois ratio is not observ for the variat of the suppli voltag down to 1.5 v , which mean the propos scheme is suitabl for low-voltag applic","ordered_present_kp":[2,385,144,36,520,599],"keyphrases":["digital-to-analog converter","differential-quad switching","direct digital modulation","CMOS technology","signal-to-noise ratio","1.5 V","high-conversion-rate DAC","high-resolution DAC","oversampling DAC","SNR","0.8 micron"],"prmu":["P","P","P","P","P","P","R","R","R","U","U"]}
{"id":"2023","title":"Diffraction limit for a circular mask with a periodic rectangular apertures array","abstract":"A mask with periodic apertures imaging system is adopted very widely and plays a leading role in modern technology for uses such as pinhole cameras, coded imaging systems, optical information processing, etc. because of its high resolution, its infinite depth of focus, and its usefulness over a broad frequency spectra ranging from visible light to X-rays and gamma rays. While the masks with periodic apertures investigated in the literature are limited only to far-field diffraction, they do not take the shift of apertures within the mask into consideration. Therefore the derivation of the far-field diffraction for a single aperture cannot be applied to a mask with periodic apertures. The far-field diffraction formula modified for a multiaperture mask has been proposed in the past, the analysis remains too complicated to offer some practical guidance for mask design. We study a circular mask with periodic rectangular apertures and develop an easier way to interpret it. First, the near-field diffraction intensity of a circular aperture is calculated by means of Lommel's function. Then the convolution of the circular mask diffraction with periodic rectangular apertures is put together, and we can present a simple mathematical tool to analyze the mask properties including the intensity distribution, blurring aberration, and the criterion of defining the far- or near-field diffraction. This concept can also be expanded to analyze different types of masks with the arbitrarily shaped apertures","tok_text":"diffract limit for a circular mask with a period rectangular apertur array \n a mask with period apertur imag system is adopt veri wide and play a lead role in modern technolog for use such as pinhol camera , code imag system , optic inform process , etc . becaus of it high resolut , it infinit depth of focu , and it use over a broad frequenc spectra rang from visibl light to x-ray and gamma ray . while the mask with period apertur investig in the literatur are limit onli to far-field diffract , they do not take the shift of apertur within the mask into consider . therefor the deriv of the far-field diffract for a singl apertur can not be appli to a mask with period apertur . the far-field diffract formula modifi for a multiapertur mask ha been propos in the past , the analysi remain too complic to offer some practic guidanc for mask design . we studi a circular mask with period rectangular apertur and develop an easier way to interpret it . first , the near-field diffract intens of a circular apertur is calcul by mean of lommel 's function . then the convolut of the circular mask diffract with period rectangular apertur is put togeth , and we can present a simpl mathemat tool to analyz the mask properti includ the intens distribut , blur aberr , and the criterion of defin the far- or near-field diffract . thi concept can also be expand to analyz differ type of mask with the arbitrarili shape apertur","ordered_present_kp":[0,21,42,192,208,227,269,287,329,362,388,89,479,30,621,688,728,42,1067,1083,967,1397],"keyphrases":["diffraction limit","circular mask","mask","periodic rectangular apertures array","periodic rectangular apertures","periodic apertures","pinhole cameras","coded imaging systems","optical information processing","high resolution","infinite depth of focus","broad frequency spectra","visible light","gamma rays","far-field diffraction","single aperture","far-field diffraction formula","multiaperture mask","near-field diffraction","convolution","circular mask diffraction","arbitrarily shaped apertures","x rays"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"394","title":"Model intestinal microflora in computer simulation: a simulation and modeling package for host-microflora interactions","abstract":"The ecology of the human intestinal microflora and its interaction with the host are poorly understood. Though more and more data are being acquired, in part using modern molecular methods, development of a quantitative theory has not kept pace with this increase in observing power. This is in part due to the complexity of the system and to the lack of simulation environments in which to test what the ecological effect of a hypothetical mechanism of interaction would be, before resorting to laboratory experiments. The MIMICS project attempts to address this through the development of a cellular automaton for simulation of the intestinal microflora. In this paper, the design and evaluation of this simulator is discussed","tok_text":"model intestin microflora in comput simul : a simul and model packag for host-microflora interact \n the ecolog of the human intestin microflora and it interact with the host are poorli understood . though more and more data are be acquir , in part use modern molecular method , develop of a quantit theori ha not kept pace with thi increas in observ power . thi is in part due to the complex of the system and to the lack of simul environ in which to test what the ecolog effect of a hypothet mechan of interact would be , befor resort to laboratori experi . the mimic project attempt to address thi through the develop of a cellular automaton for simul of the intestin microflora . in thi paper , the design and evalu of thi simul is discuss","ordered_present_kp":[118,6,259,343,73,291,563],"keyphrases":["intestinal microflora","host-microflora interactions","human intestines","molecular methods","quantitative theory","observing power","MIMICS project","system complexity","microbial ecology","parallel computing","complex microbial ecosystem"],"prmu":["P","P","P","P","P","P","P","R","M","M","M"]}
{"id":"2066","title":"Product and process innovations in the life cycle of an industry","abstract":"Filson (2001) uses industry-level data on firm numbers, price, quantity and quality along with an equilibrium model of industry evolution to estimate the nature and effects of quality and cost improvements in the personal computer industry and four other new industries. This paper studies the personal computer industry in more detail and shows that the model explains some peculiar patterns that cannot be explained by previous life-cycle models. The model estimates are evaluated using historical studies of the evolution of the personal computer industry and patterns that require further model development are described","tok_text":"product and process innov in the life cycl of an industri \n filson ( 2001 ) use industry-level data on firm number , price , quantiti and qualiti along with an equilibrium model of industri evolut to estim the natur and effect of qualiti and cost improv in the person comput industri and four other new industri . thi paper studi the person comput industri in more detail and show that the model explain some peculiar pattern that can not be explain by previou life-cycl model . the model estim are evalu use histor studi of the evolut of the person comput industri and pattern that requir further model develop are describ","ordered_present_kp":[461,160,181],"keyphrases":["equilibrium model","industry evolution","life-cycle models","technological change","industry dynamics","personal computer market","microelectronics","PC industry","production cost"],"prmu":["P","P","P","U","M","M","U","M","R"]}
{"id":"27","title":"A formal model of computing with words","abstract":"Classical automata are formal models of computing with values. Fuzzy automata are generalizations of classical automata where the knowledge about the system's next state is vague or uncertain. It is worth noting that like classical automata, fuzzy automata can only process strings of input symbols. Therefore, such fuzzy automata are still (abstract) devices for computing with values, although a certain vagueness or uncertainty are involved in the process of computation. We introduce a new kind of fuzzy automata whose inputs are instead strings of fuzzy subsets of the input alphabet. These new fuzzy automata may serve as formal models of computing with words. We establish an extension principle from computing with values to computing with words. This principle indicates that computing with words can be implemented with computing with values with the price of a big amount of extra computations","tok_text":"a formal model of comput with word \n classic automata are formal model of comput with valu . fuzzi automata are gener of classic automata where the knowledg about the system 's next state is vagu or uncertain . it is worth note that like classic automata , fuzzi automata can onli process string of input symbol . therefor , such fuzzi automata are still ( abstract ) devic for comput with valu , although a certain vagu or uncertainti are involv in the process of comput . we introduc a new kind of fuzzi automata whose input are instead string of fuzzi subset of the input alphabet . these new fuzzi automata may serv as formal model of comput with word . we establish an extens principl from comput with valu to comput with word . thi principl indic that comput with word can be implement with comput with valu with the price of a big amount of extra comput","ordered_present_kp":[2,18,93,549,569,674],"keyphrases":["formal model","computing with words","fuzzy automata","fuzzy subsets","input alphabet","extension principle","pushdown automata"],"prmu":["P","P","P","P","P","P","M"]}
{"id":"289","title":"Noise effect on memory recall in dynamical neural network model of hippocampus","abstract":"We investigate some noise effect on a neural network model proposed by Araki and Aihara (1998) for the memory recall of dynamical patterns in the hippocampus and the entorhinal cortex; the noise effect is important since the release of transmitters at synaptic clefts, the operation of gate of ion channels and so on are known as stochastic phenomena. We consider two kinds of noise effect due to a deterministic noise and a stochastic noise. By numerical simulations, we find that reasonable values of noise give better performance on the memory recall of dynamical patterns. Furthermore we investigate the effect of the strength of external inputs on the memory recall","tok_text":"nois effect on memori recal in dynam neural network model of hippocampu \n we investig some nois effect on a neural network model propos by araki and aihara ( 1998 ) for the memori recal of dynam pattern in the hippocampu and the entorhin cortex ; the nois effect is import sinc the releas of transmitt at synapt cleft , the oper of gate of ion channel and so on are known as stochast phenomena . we consid two kind of nois effect due to a determinist nois and a stochast nois . by numer simul , we find that reason valu of nois give better perform on the memori recal of dynam pattern . furthermor we investig the effect of the strength of extern input on the memori recal","ordered_present_kp":[61,31,0,15,189,229,305,332,375,439,462,481],"keyphrases":["noise effect","memory recall","dynamical neural network model","hippocampus","dynamical patterns","entorhinal cortex","synaptic clefts","gate of ion channels","stochastic phenomena","deterministic noise","stochastic noise","numerical simulations","brain functions","synaptic strength","inhibitory connection"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","U","R","U"]}
{"id":"2186","title":"Strategies for high throughput, templated zeolite synthesis","abstract":"The design and redesign of high throughput experiments for zeolite synthesis are addressed. A model that relates materials function to the chemical composition of the zeolite and the structure directing agent is introduced. Using this model, several Monte Carlo-like design protocols are evaluated. Multi-round protocols are bound to be effective, and strategies that use a priori information about the structure-directing libraries are found to be the best","tok_text":"strategi for high throughput , templat zeolit synthesi \n the design and redesign of high throughput experi for zeolit synthesi are address . a model that relat materi function to the chemic composit of the zeolit and the structur direct agent is introduc . use thi model , sever mont carlo-lik design protocol are evalu . multi-round protocol are bound to be effect , and strategi that use a priori inform about the structure-direct librari are found to be the best","ordered_present_kp":[31,160,183,221,279,322,390],"keyphrases":["templated zeolite synthesis","materials function","chemical composition","structure directing agent","Monte Carlo-like design protocols","multi-round protocols","a priori information","high throughput strategies","catalytic activity","catalytic selectivity","organo-cation template molecules","combinatorial methods","random energy model","figure of merit","material discovery","small molecule design","Voronoi diagram","phase-dependent random Gaussian variables","Metropolis-type method","ligand libraries","reflecting boundary conditions"],"prmu":["P","P","P","P","P","P","P","R","U","U","M","U","M","M","M","M","U","U","U","M","U"]}
{"id":"231","title":"Writing the fulfillment RFP [publishing]","abstract":"For the uninitiated, writing a request for proposal can seem both mysterious and daunting. Here's a format that will make you look like a pro the first time out","tok_text":"write the fulfil rfp [ publish ] \n for the uniniti , write a request for propos can seem both mysteri and daunt . here 's a format that will make you look like a pro the first time out","ordered_present_kp":[61,10,23],"keyphrases":["fulfillment","publisher","request for proposal"],"prmu":["P","P","P"]}
{"id":"274","title":"MTD-PLS: a PLS-based variant of the MTD method. II. Mapping ligand-receptor interactions. Enzymatic acetic acid esters hydrolysis","abstract":"The PLS variant of the MTD method (T.I. Oprea et al., SAR QSAR Environ. Res. 2001, 12, 75-92) was applied to a series of 25 acetylcholinesterase hydrolysis substrates. Statistically significant MTD-PLS models (q\/sup 2\/ between 0.7 and 0.8) are in agreement with previous MTD models, with the advantage that local contributions are understood beyond the occupancy\/nonoccupancy interpretation in MTD. A \"chemically intuitive\" approach further forces MTD-PLS coefficients to assume only negative (or zero) values for fragmental volume descriptors and positive (or zero) values for fragmental hydrophobicity descriptors. This further separates the various kinds of local interactions at each vertex of the MTD hypermolecule, making this method suitable for medicinal chemistry synthesis planning","tok_text":"mtd-pl : a pls-base variant of the mtd method . ii . map ligand-receptor interact . enzymat acet acid ester hydrolysi \n the pl variant of the mtd method ( t.i. oprea et al . , sar qsar environ . re . 2001 , 12 , 75 - 92 ) wa appli to a seri of 25 acetylcholinesteras hydrolysi substrat . statist signific mtd-pl model ( q \/ sup 2\/ between 0.7 and 0.8 ) are in agreement with previou mtd model , with the advantag that local contribut are understood beyond the occup \/ nonoccup interpret in mtd . a \" chemic intuit \" approach further forc mtd-pl coeffici to assum onli neg ( or zero ) valu for fragment volum descriptor and posit ( or zero ) valu for fragment hydrophob descriptor . thi further separ the variou kind of local interact at each vertex of the mtd hypermolecul , make thi method suitabl for medicin chemistri synthesi plan","ordered_present_kp":[305,11,84,247,593,650,760,803],"keyphrases":["PLS-based variant","enzymatic acetic acid esters hydrolysis","acetylcholinesterase hydrolysis substrates","MTD-PLS models","fragmental volume descriptors","fragmental hydrophobicity descriptors","hypermolecule","medicinal chemistry synthesis planning","minimum topological difference method","ligand-receptor interactions mapping","chemically intuitive approach","steric misfit","additive approach","intermolecular force categories","regression coefficients","ligand binding affinity","hydrogen bonding","polarizabilities","statistical model stability"],"prmu":["P","P","P","P","P","P","P","P","M","R","R","U","M","M","M","U","U","U","M"]}
{"id":"2021","title":"One-step digit-set-restricted modified signed-digit adder using an incoherent correlator based on a shared content-addressable memory","abstract":"An efficient one-step digit-set-restricted modified signed-digit (MSD) adder based on symbolic substitution is presented. In this technique, carry propagation is avoided by introducing reference digits to restrict the intermediate carry and sum digits to {1,0} and {0,1}, respectively. The proposed technique requires significantly fewer minterms and simplifies system complexity compared to the reported one-step MSD addition techniques. An incoherent correlator based on an optoelectronic shared content-addressable memory processor is suggested to perform the addition operation. In this technique, only one set of minterms needs to be stored, independent of the operand length","tok_text":"one-step digit-set-restrict modifi signed-digit adder use an incoher correl base on a share content-address memori \n an effici one-step digit-set-restrict modifi signed-digit ( msd ) adder base on symbol substitut is present . in thi techniqu , carri propag is avoid by introduc refer digit to restrict the intermedi carri and sum digit to { 1,0 } and { 0,1 } , respect . the propos techniqu requir significantli fewer minterm and simplifi system complex compar to the report one-step msd addit techniqu . an incoher correl base on an optoelectron share content-address memori processor is suggest to perform the addit oper . in thi techniqu , onli one set of minterm need to be store , independ of the operand length","ordered_present_kp":[0,61,86,197,279,307,327,419,440,535,613,703],"keyphrases":["one-step digit-set-restricted modified signed-digit adder","incoherent correlator","shared content-addressable memory","symbolic substitution","reference digits","intermediate carry","sum digits","minterms","system complexity","optoelectronic shared content-addressable memory processor","addition operation","operand length"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"396","title":"Convolution-based global simulation technique for millimeter-wave photodetector and photomixer circuits","abstract":"A fast convolution-based time-domain approach to global photonic-circuit simulation is presented that incorporates a physical device model in the complete detector or mixer circuit. The device used in the demonstration of this technique is a GaAs metal-semiconductor-metal (MSM) photodetector that offers a high response speed for the detection and generation of millimeter waves. Global simulation greatly increases the accuracy in evaluating the complete circuit performance because it accounts for the effects of the millimeter-wave embedding circuit. Device and circuit performance are assessed by calculating optical responsivity and bandwidth. Device-only simulations using GaAs MSMs are compared with global simulations that illustrate the strong interdependence between device and external circuit","tok_text":"convolution-bas global simul techniqu for millimeter-wav photodetector and photomix circuit \n a fast convolution-bas time-domain approach to global photonic-circuit simul is present that incorpor a physic devic model in the complet detector or mixer circuit . the devic use in the demonstr of thi techniqu is a gaa metal-semiconductor-met ( msm ) photodetector that offer a high respons speed for the detect and gener of millimet wave . global simul greatli increas the accuraci in evalu the complet circuit perform becaus it account for the effect of the millimeter-wav embed circuit . devic and circuit perform are assess by calcul optic respons and bandwidth . device-onli simul use gaa msm are compar with global simul that illustr the strong interdepend between devic and extern circuit","ordered_present_kp":[101,141,198,42,75,634,652,0,311],"keyphrases":["convolution-based global simulation","millimeter-wave photodetector","photomixer","convolution-based time-domain approach","global photonic-circuit simulation","physical device model","GaAs","optical responsivity","bandwidth","GaAs MSM photodetector","MM-wave embedding circuit"],"prmu":["P","P","P","P","P","P","P","P","P","R","M"]}
{"id":"2064","title":"The effects of emotions on bounded rationality: a comment on Kaufman","abstract":"Bruce Kaufman's article (1999), \"Emotional arousal as a source of bounded rationality\", objective is to present an additional source of bounded rationality, one that is not due to cognitive constraints, but to high emotional arousal. In doing so, Kaufman is following a long tradition of thinkers who have contrasted emotion with reason, claiming, for the most part, that emotions are a violent force hindering rational thinking. This paper aims to challenge Kaufman's unidimensional idea regarding the connection between high emotional arousal and decision making","tok_text":"the effect of emot on bound ration : a comment on kaufman \n bruce kaufman 's articl ( 1999 ) , \" emot arous as a sourc of bound ration \" , object is to present an addit sourc of bound ration , one that is not due to cognit constraint , but to high emot arous . in do so , kaufman is follow a long tradit of thinker who have contrast emot with reason , claim , for the most part , that emot are a violent forc hinder ration think . thi paper aim to challeng kaufman 's unidimension idea regard the connect between high emot arous and decis make","ordered_present_kp":[22,533,416,14],"keyphrases":["emotion","bounded rationality","rational thinking","decision making","psychology","Yerkes-Dodson law"],"prmu":["P","P","P","P","U","U"]}
{"id":"403","title":"High-voltage transistor scaling circuit techniques for high-density negative-gate channel-erasing NOR flash memories","abstract":"In order to scale high-voltage transistors for high-density negative-gate channel-erasing NOR flash memories, two circuit techniques were developed. A proposed level shifter with low operating voltage is composed of three parts, a latch holding the negative erasing voltage, two coupling capacitors connected with the latched nodes in the latch, and high-voltage drivers inverting the latch, resulting in reduction of the maximum internal voltage by 0.5 V. A proposed high-voltage generator adds a path-gate logic to a conventional high-voltage generator to realize both low noise and low ripple voltage, resulting in a reduction of the maximum internal voltage by 0.5 V. As a result, these circuit techniques along with high coupling-ratio cell technology can scale down the high-voltage transistors by 15% and can realize higher density negative-gate channel-erase NOR flash memories in comparison with the source-erase NOR flash memories","tok_text":"high-voltag transistor scale circuit techniqu for high-dens negative-g channel-eras nor flash memori \n in order to scale high-voltag transistor for high-dens negative-g channel-eras nor flash memori , two circuit techniqu were develop . a propos level shifter with low oper voltag is compos of three part , a latch hold the neg eras voltag , two coupl capacitor connect with the latch node in the latch , and high-voltag driver invert the latch , result in reduct of the maximum intern voltag by 0.5 v. a propos high-voltag gener add a path-gat logic to a convent high-voltag gener to realiz both low nois and low rippl voltag , result in a reduct of the maximum intern voltag by 0.5 v. as a result , these circuit techniqu along with high coupling-ratio cell technolog can scale down the high-voltag transistor by 15 % and can realiz higher densiti negative-g channel-eras nor flash memori in comparison with the source-eras nor flash memori","ordered_present_kp":[246,409,512,536,597,610,735],"keyphrases":["level shifter","high-voltage drivers","high-voltage generator","path-gate logic","low noise","low ripple voltage","high coupling-ratio cell technology","HV transistor scaling circuit techniques","high-density NOR flash memories","negative-gate channel-erasing flash memories","low operating voltage shifter","HV generator"],"prmu":["P","P","P","P","P","P","P","M","R","R","R","M"]}
{"id":"2099","title":"Analyzing the potential of a firm: an operations research approach","abstract":"An approach to analyzing the potential of a firm, which is understood as the firm's ability to provide goods or (and) services to be supplied to a marketplace under restrictions imposed by a business environment in which the firm functions, is proposed. The approach is based on using linear inequalities and, generally, mixed variables in modelling this ability for a broad spectrum of industrial, transportation, agricultural, and other types of firms and allows one to formulate problems of analyzing the potential of a firm as linear programming problems or mixed programming problems with linear constraints. This approach generalizes a previous one which was proposed for a more narrow class of models, and allows one to effectively employ a widely available software for solving practical problems of the considered kind, especially for firms described by large scale models of mathematical programming","tok_text":"analyz the potenti of a firm : an oper research approach \n an approach to analyz the potenti of a firm , which is understood as the firm 's abil to provid good or ( and ) servic to be suppli to a marketplac under restrict impos by a busi environ in which the firm function , is propos . the approach is base on use linear inequ and , gener , mix variabl in model thi abil for a broad spectrum of industri , transport , agricultur , and other type of firm and allow one to formul problem of analyz the potenti of a firm as linear program problem or mix program problem with linear constraint . thi approach gener a previou one which wa propos for a more narrow class of model , and allow one to effect employ a wide avail softwar for solv practic problem of the consid kind , especi for firm describ by larg scale model of mathemat program","ordered_present_kp":[34,160,315,522,548,822],"keyphrases":["operations research","OR","linear inequalities","linear programming","mixed programming","mathematical programming","firm potential analysis","industrial firms","transportation firms","agricultural firms","large-scale models"],"prmu":["P","P","P","P","P","P","M","R","R","R","M"]}
{"id":"2184","title":"The evolution of information systems: Their impact on organizations and structures","abstract":"Information systems and organization structures have been highly interconnected with each other. Over the years, information systems architectures as well as organization structures have evolved from centralized to more decentralized forms. This research looks at the evolution of both information systems and organization structures. In the process, it looks into the impact of computers on organizations, and examines the ways organization structures have changed, in association with changes in information system architectures. It also suggests logical linkages between information system architectures and their \"fit\" with certain organization structures and strategies. It concludes with some implications for emerging and future organizational forms, and provides a quick review of the effect of the Internet on small businesses traditionally using stand-alone computers","tok_text":"the evolut of inform system : their impact on organ and structur \n inform system and organ structur have been highli interconnect with each other . over the year , inform system architectur as well as organ structur have evolv from central to more decentr form . thi research look at the evolut of both inform system and organ structur . in the process , it look into the impact of comput on organ , and examin the way organ structur have chang , in associ with chang in inform system architectur . it also suggest logic linkag between inform system architectur and their \" fit \" with certain organ structur and strategi . it conclud with some implic for emerg and futur organiz form , and provid a quick review of the effect of the internet on small busi tradit use stand-alon comput","ordered_present_kp":[164],"keyphrases":["information system architectures","information systems evolution"],"prmu":["P","R"]}
{"id":"233","title":"The Canadian National Site Licensing Project","abstract":"In January 2000, a consortium of 64 universities in Canada signed a historic inter-institutional agreement that launched the Canadian National Site Licensing Project (CNSLP), a three-year pilot project aimed at bolstering the research and innovation capacity of the country's universities. CNSLP tests the feasibility of licensing, on a national scale, electronic versions of scholarly publications; in its initial phases the project is focused on full-text electronic journals and research databases in science, engineering, health and environmental disciplines. This article provides an overview of the CNSLP initiative, summarizes organizational and licensing accomplishments to date, and offers preliminary observations on challenges and opportunities for subsequent phases of the project","tok_text":"the canadian nation site licens project \n in januari 2000 , a consortium of 64 univers in canada sign a histor inter-institut agreement that launch the canadian nation site licens project ( cnslp ) , a three-year pilot project aim at bolster the research and innov capac of the countri 's univers . cnslp test the feasibl of licens , on a nation scale , electron version of scholarli public ; in it initi phase the project is focus on full-text electron journal and research databas in scienc , engin , health and environment disciplin . thi articl provid an overview of the cnslp initi , summar organiz and licens accomplish to date , and offer preliminari observ on challeng and opportun for subsequ phase of the project","ordered_present_kp":[4,111,246,190,435,466],"keyphrases":["Canadian National Site Licensing Project","inter-institutional agreement","CNSLP","research and innovation","full-text electronic journals","research databases","academic libraries","information resources","electronic scholarly publications"],"prmu":["P","P","P","P","P","P","U","U","R"]}
{"id":"276","title":"Assessment of the macrocyclic effect for the complexation of crown-ethers with alkali cations using the substructural molecular fragments method","abstract":"The Substructural Molecular Fragments method (Solov'ev, V. P.; Varnek, A. A.; Wipff, G. J. Chem. Inf. Comput. Sci. 2000, 40, 847-858) was applied to assess stability constants (logK) of the complexes of crown-ethers, polyethers, and glymes with Na\/sup +\/, K\/sup +\/, and Cs\/sup +\/ in methanol. One hundred forty-seven computational models including different fragment sets coupled with linear or nonlinear fitting equations were applied for the data sets containing 69 (Na\/sup +\/), 123 (K\/sup +\/), and 31 (Cs\/sup +\/) compounds. To account for the \"macrocyclic effect\" for crown-ethers, an additional \"cyclicity\" descriptor was used. \"Predicted\" stability constants both for macrocyclic compounds and for their open-chain analogues are in good agreement with the experimental data reported earlier and with those studied experimentally in this work. The macrocyclic effect as a function of cation and ligand is quantitatively estimated for all studied crown-ethers","tok_text":"assess of the macrocycl effect for the complex of crown-eth with alkali cation use the substructur molecular fragment method \n the substructur molecular fragment method ( solov'ev , v. p. ; varnek , a. a. ; wipff , g. j. chem . inf . comput . sci . 2000 , 40 , 847 - 858 ) wa appli to assess stabil constant ( logk ) of the complex of crown-eth , polyeth , and glyme with na \/ sup + \/ , k \/ sup + \/ , and cs \/ sup + \/ in methanol . one hundr forty-seven comput model includ differ fragment set coupl with linear or nonlinear fit equat were appli for the data set contain 69 ( na \/ sup + \/ ) , 123 ( k \/ sup + \/ ) , and 31 ( cs \/ sup + \/ ) compound . to account for the \" macrocycl effect \" for crown-eth , an addit \" cyclic \" descriptor wa use . \" predict \" stabil constant both for macrocycl compound and for their open-chain analogu are in good agreement with the experiment data report earlier and with those studi experiment in thi work . the macrocycl effect as a function of cation and ligand is quantit estim for all studi crown-eth","ordered_present_kp":[87,292,39,50,65,14,454,474,515,816],"keyphrases":["macrocyclic effect","complexation","crown-ethers","alkali cations","substructural molecular fragments method","stability constants","computational models","different fragment sets","nonlinear fitting equations","open-chain analogues","linear fitting equations","cyclicity descriptor","data mining","structure-property tool","molecular graph decomposition","quantitative structure-properties relationship","augmented atom","TRAIL program","statistical parameters","thermodynamic parameters"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R","M","U","M","M","U","U","U","U"]}
{"id":"25","title":"Identification of evolving fuzzy rule-based models","abstract":"An approach to identification of evolving fuzzy rule-based (eR) models is proposed. eR models implement a method for the noniterative update of both the rule-base structure and parameters by incremental unsupervised learning. The rule-base evolves by adding more informative rules than those that previously formed the model. In addition, existing rules can be replaced with new rules based on ranking using the informative potential of the data. In this way, the rule-base structure is inherited and updated when new informative data become available, rather than being completely retrained. The adaptive nature of these evolving rule-based models, in combination with the highly transparent and compact form of fuzzy rules, makes them a promising candidate for modeling and control of complex processes, competitive to neural networks. The approach has been tested on a benchmark problem and on an air-conditioning component modeling application using data from an installation serving a real building. The results illustrate the viability and efficiency of the approach","tok_text":"identif of evolv fuzzi rule-bas model \n an approach to identif of evolv fuzzi rule-bas ( er ) model is propos . er model implement a method for the nonit updat of both the rule-bas structur and paramet by increment unsupervis learn . the rule-bas evolv by ad more inform rule than those that previous form the model . in addit , exist rule can be replac with new rule base on rank use the inform potenti of the data . in thi way , the rule-bas structur is inherit and updat when new inform data becom avail , rather than be complet retrain . the adapt natur of these evolv rule-bas model , in combin with the highli transpar and compact form of fuzzi rule , make them a promis candid for model and control of complex process , competit to neural network . the approach ha been test on a benchmark problem and on an air-condit compon model applic use data from an instal serv a real build . the result illustr the viabil and effici of the approach","ordered_present_kp":[11,0,148,172,205,376,389,17,709,815],"keyphrases":["identification","evolving fuzzy rule-based models","fuzzy rules","noniterative update","rule-base structure","incremental unsupervised learning","ranking","informative potential","complex processes","air-conditioning component modeling","adaptive nonlinear control","fault detection","fault diagnostics","performance analysis","forecasting","knowledge extraction","robotics","behavior modeling"],"prmu":["P","P","P","P","P","P","P","P","P","P","M","U","U","U","U","U","U","M"]}
{"id":"2179","title":"Guidelines, the Internet, and personal health: insights from the Canadian HEALNet experience","abstract":"The objectives are to summarize the insights gained in collaborative research in a Canadian Network of Centres of Excellence, devoted to the promotion of evidence-based practice, and to relate this experience to Internet support of health promotion and consumer health informatics. A subjective review of insights is undertaken. Work directed the development of systems incorporating guidelines, care maps, etc., for use by professionals met with limited acceptance. Evidence-based tools for health care consumers are a desirable complement but require radically different content and delivery modes. In addition to evidence-based material offered by professionals, a wide array of Internet-based products and services provided by consumers for consumers emerged and proved a beneficial complement. The consumer-driven products and services provided via the Internet are a potentially important and beneficial complement of traditional health services. They affect the health consumer-provider roles and require changes in healthcare practices","tok_text":"guidelin , the internet , and person health : insight from the canadian healnet experi \n the object are to summar the insight gain in collabor research in a canadian network of centr of excel , devot to the promot of evidence-bas practic , and to relat thi experi to internet support of health promot and consum health informat . a subject review of insight is undertaken . work direct the develop of system incorpor guidelin , care map , etc . , for use by profession met with limit accept . evidence-bas tool for health care consum are a desir complement but requir radic differ content and deliveri mode . in addit to evidence-bas materi offer by profession , a wide array of internet-bas product and servic provid by consum for consum emerg and prove a benefici complement . the consumer-driven product and servic provid via the internet are a potenti import and benefici complement of tradit health servic . they affect the health consumer-provid role and requir chang in healthcar practic","ordered_present_kp":[134,157,217,267,287,305,30,929],"keyphrases":["personal health","collaborative research","Canadian Network of Centres of Excellence","evidence-based practice","Internet support","health promotion","consumer health informatics","health consumer-provider roles"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"2144","title":"The perils of privacy","abstract":"The recent string of failures among dotcom companies has heightened fears of privacy abuse. What should happen to the names and addresses on a customer list if these details were obtained under a privacy policy which specified no disclosure to any third party? Should the personal data in the list be deemed to be an asset of a failing company which can be transferred to any future (third party) purchaser for its purposes? Or should the privacy policy take precedence over the commercial concerns of the purchaser?","tok_text":"the peril of privaci \n the recent string of failur among dotcom compani ha heighten fear of privaci abus . what should happen to the name and address on a custom list if these detail were obtain under a privaci polici which specifi no disclosur to ani third parti ? should the person data in the list be deem to be an asset of a fail compani which can be transfer to ani futur ( third parti ) purchas for it purpos ? or should the privaci polici take preced over the commerci concern of the purchas ?","ordered_present_kp":[92,155,203,235],"keyphrases":["privacy abuse","customer list","privacy policy","disclosure"],"prmu":["P","P","P","P"]}
{"id":"2101","title":"All-optical XOR gate using semiconductor optical amplifiers without additional input beam","abstract":"The novel design of an all-optical XOR gate by using cross-gain modulation of semiconductor optical amplifiers has been suggested and demonstrated successfully at 10 Gb\/s. Boolean AB and AB of the two input signals A and B have been obtained and combined to achieve the all-optical XOR gate. No additional input beam such as a clock signal or continuous wave light is used in this new design, which is required in other all-optical XOR gates","tok_text":"all-opt xor gate use semiconductor optic amplifi without addit input beam \n the novel design of an all-opt xor gate by use cross-gain modul of semiconductor optic amplifi ha been suggest and demonstr success at 10 gb \/ s. boolean ab and ab of the two input signal a and b have been obtain and combin to achiev the all-opt xor gate . no addit input beam such as a clock signal or continu wave light is use in thi new design , which is requir in other all-opt xor gate","ordered_present_kp":[21,86,123],"keyphrases":["semiconductor optical amplifiers","design","cross-gain modulation","all-optical-XOR gate","Boolean logic","10 Gbit\/s"],"prmu":["P","P","P","M","M","M"]}
{"id":"1970","title":"eMarketing: restaurant Web sites that click","abstract":"A number of global companies have adopted electronic commerce as a means of reducing transaction related expenditures, connecting with current and potential customers, and enhancing revenues and profitability. If a restaurant is to have an Internet presence, what aspects of the business should be highlighted? Food service companies that have successfully ventured onto the web have employed assorted web-based technologies to create a powerful marketing tool of unparalleled strength. Historically, it has been difficult to create a set of criteria against which to evaluate website effectiveness. As practitioners consider additional resources for website development, the effectiveness of e-marketing investment becomes increasingly important. Care must be exercised to ensure that the quality of the site adheres to high standards and incorporates evolving technology, as appropriate. Developing a coherent website strategy, including an effective website design, are proving critical to an effective web presence","tok_text":"emarket : restaur web site that click \n a number of global compani have adopt electron commerc as a mean of reduc transact relat expenditur , connect with current and potenti custom , and enhanc revenu and profit . if a restaur is to have an internet presenc , what aspect of the busi should be highlight ? food servic compani that have success ventur onto the web have employ assort web-bas technolog to creat a power market tool of unparallel strength . histor , it ha been difficult to creat a set of criteria against which to evalu websit effect . as practition consid addit resourc for websit develop , the effect of e-market invest becom increasingli import . care must be exercis to ensur that the qualiti of the site adher to high standard and incorpor evolv technolog , as appropri . develop a coher websit strategi , includ an effect websit design , are prove critic to an effect web presenc","ordered_present_kp":[622,10,78,242,307,195,206],"keyphrases":["restaurant Web sites","electronic commerce","revenues","profitability","Internet presence","food service companies","e-marketing"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"1935","title":"Identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information","abstract":"The problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. Some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system","tok_text":"identif of state of complex system with estim of admiss measur error on the basi of fuzzi inform \n the problem of identif of state of complex system on the basi of fuzzi valu of inform attribut is consid . some estim of a maxim admiss degre of measur error are obtain that make it possibl , use the apparatu of fuzzi set theori , to correctli identifi the current state of a system","ordered_present_kp":[49,84,178,56,311],"keyphrases":["admissible measurement errors","measurement error","fuzzy information","informative attributes","fuzzy set theory","complex systems states identification"],"prmu":["P","P","P","P","P","R"]}
{"id":"2200","title":"Labscape: a smart environment for the cell biology laboratory","abstract":"Labscape is a smart environment that we designed to improve the experience of people who work in a cell biology laboratory. Our goal in creating it was to simplify, laboratory work by making information available where it is needed and by collecting and organizing data where and when it is created into a formal representation that others can understand and process. By helping biologists produce a more complete record of their work with less effort, Labscape is designed to foster improved collaboration in conjunction with increased individual efficiency and satisfaction. A user-driven system, although technologically conservative, embraces a central goal of ubiquitous computing: to enhance the ability to perform domain tasks through fluid interaction with computational resources. Smart environments could soon replace the pen and paper commonly used in the laboratory setting","tok_text":"labscap : a smart environ for the cell biolog laboratori \n labscap is a smart environ that we design to improv the experi of peopl who work in a cell biolog laboratori . our goal in creat it wa to simplifi , laboratori work by make inform avail where it is need and by collect and organ data where and when it is creat into a formal represent that other can understand and process . by help biologist produc a more complet record of their work with less effort , labscap is design to foster improv collabor in conjunct with increas individu effici and satisfact . a user-driven system , although technolog conserv , embrac a central goal of ubiquit comput : to enhanc the abil to perform domain task through fluid interact with comput resourc . smart environ could soon replac the pen and paper commonli use in the laboratori set","ordered_present_kp":[34,0,208,641,12],"keyphrases":["Labscape","smart environment","cell biology","laboratory work","ubiquitous computing","experimental technologies","biochemical procedure"],"prmu":["P","P","P","P","P","M","U"]}
{"id":"356","title":"Operational phase-space probability distribution in quantum communication theory","abstract":"Operational phase-space probability distributions are useful tools for describing quantum mechanical systems, including quantum communication and quantum information processing systems. It is shown that quantum communication channels with Gaussian noise and quantum teleportation of continuous variables are described by operational phase-space probability distributions. The relation of operational phase-space probability distribution to the extended phase-space formalism proposed by Chountasis and Vourdas (1998) is discussed","tok_text":"oper phase-spac probabl distribut in quantum commun theori \n oper phase-spac probabl distribut are use tool for describ quantum mechan system , includ quantum commun and quantum inform process system . it is shown that quantum commun channel with gaussian nois and quantum teleport of continu variabl are describ by oper phase-spac probabl distribut . the relat of oper phase-spac probabl distribut to the extend phase-spac formal propos by chountasi and vourda ( 1998 ) is discuss","ordered_present_kp":[0,37,120,170,247,265,285,406],"keyphrases":["operational phase-space probability distribution","quantum communication theory","quantum mechanical systems","quantum information processing systems","Gaussian noise","quantum teleportation","continuous variables","extended phase-space formalism"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"313","title":"Information architecture: notes toward a new curriculum","abstract":"There are signs that information architecture is coalescing into a field of professional practice. However, if it is to become a profession, it must develop a means of educating new information architects. Lessons from other fields suggest that professional education typically evolves along a predictable path, from apprenticeships to trade schools to college- and university-level education. Information architecture education may develop more quickly to meet the growing demands of the information society. Several pedagogical approaches employed in other fields may be adopted for information architecture education, as long as the resulting curricula provide an interdisciplinary approach and balance instruction in technical and design skills with consideration of theoretical concepts. Key content areas are information organization, graphic. design, computer science, user and usability studies, and communication. Certain logistics must be worked out, including where information architecture studies should be housed and what kinds of degrees should be offered and at what levels. The successful information architecture curriculum will be flexible and adaptable in order to meet the changing needs of students and the marketplace","tok_text":"inform architectur : note toward a new curriculum \n there are sign that inform architectur is coalesc into a field of profession practic . howev , if it is to becom a profess , it must develop a mean of educ new inform architect . lesson from other field suggest that profession educ typic evolv along a predict path , from apprenticeship to trade school to college- and university-level educ . inform architectur educ may develop more quickli to meet the grow demand of the inform societi . sever pedagog approach employ in other field may be adopt for inform architectur educ , as long as the result curricula provid an interdisciplinari approach and balanc instruct in technic and design skill with consider of theoret concept . key content area are inform organ , graphic . design , comput scienc , user and usabl studi , and commun . certain logist must be work out , includ where inform architectur studi should be hous and what kind of degre should be offer and at what level . the success inform architectur curriculum will be flexibl and adapt in order to meet the chang need of student and the marketplac","ordered_present_kp":[118,753,787,812,0,268,498,395],"keyphrases":["information architects","professional practice","professional education","information architecture education","pedagogical approaches","information organization","computer science","usability studies","graphic design"],"prmu":["P","P","P","P","P","P","P","P","R"]}
{"id":"2059","title":"Acquiring materials in the history of science, technology, and medicine","abstract":"This article provides detailed advice on acquiring new, out-of-print, and rare materials in the history of science, technology, and medicine for the beginner in these fields. The focus is on the policy formation, basic reference tools, and methods of collection development and acquisitions that are the necessary basis for success in this endeavor","tok_text":"acquir materi in the histori of scienc , technolog , and medicin \n thi articl provid detail advic on acquir new , out-of-print , and rare materi in the histori of scienc , technolog , and medicin for the beginn in these field . the focu is on the polici format , basic refer tool , and method of collect develop and acquisit that are the necessari basi for success in thi endeavor","ordered_present_kp":[32,41,57,133,247,263,296],"keyphrases":["science","technology","medicine","rare materials","policy formation","basic reference tools","collection development","out-of-print books","special collections","library acquisitions"],"prmu":["P","P","P","P","P","P","P","M","M","M"]}
{"id":"1954","title":"Estimating long-range dependence: finite sample properties and confidence intervals","abstract":"A major issue in financial economics is the behavior of asset returns over long horizons. Various estimators of long-range dependence have been proposed. Even though some have known asymptotic properties, it is important to test their accuracy by using simulated series of different lengths. We test R\/S analysis, detrended fluctuation analysis and periodogram regression methods on samples drawn from Gaussian white noise. The DFA statistics turns out to be the unanimous winner. Unfortunately, no asymptotic distribution theory has been derived for this statistics so far. We were able, however, to construct empirical (i.e. approximate) confidence intervals for all three methods. The obtained values differ largely from heuristic values proposed by some authors for the R\/S statistics and are very close to asymptotic values for the periodogram regression method","tok_text":"estim long-rang depend : finit sampl properti and confid interv \n a major issu in financi econom is the behavior of asset return over long horizon . variou estim of long-rang depend have been propos . even though some have known asymptot properti , it is import to test their accuraci by use simul seri of differ length . we test r \/ s analysi , detrend fluctuat analysi and periodogram regress method on sampl drawn from gaussian white nois . the dfa statist turn out to be the unanim winner . unfortun , no asymptot distribut theori ha been deriv for thi statist so far . we were abl , howev , to construct empir ( i.e. approxim ) confid interv for all three method . the obtain valu differ larg from heurist valu propos by some author for the r \/ s statist and are veri close to asymptot valu for the periodogram regress method","ordered_present_kp":[6,25,50,82,116,134,229,346,375,422,703],"keyphrases":["long-range dependence","finite sample properties","confidence intervals","financial economics","asset returns","long horizons","asymptotic properties","detrended fluctuation analysis","periodogram regression methods","Gaussian white noise","heuristic values"],"prmu":["P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"193","title":"Twenty years of the literature on acquiring out-of-print materials","abstract":"This article reviews the last two-and-a-half decades of literature on acquiring out-of-print materials to assess recurring issues and identify changing practices. The out-of-print literature is uniform in its assertion that libraries need to acquire o.p. materials to replace worn or damaged copies, to replace missing copies, to duplicate copies of heavily used materials, to fill gaps in collections, to strengthen weak collections, to continue to develop strong collections, and to provide materials for new courses, new programs, and even entire new libraries","tok_text":"twenti year of the literatur on acquir out-of-print materi \n thi articl review the last two-and-a-half decad of literatur on acquir out-of-print materi to assess recur issu and identifi chang practic . the out-of-print literatur is uniform in it assert that librari need to acquir o.p . materi to replac worn or damag copi , to replac miss copi , to duplic copi of heavili use materi , to fill gap in collect , to strengthen weak collect , to continu to develop strong collect , and to provid materi for new cours , new program , and even entir new librari","ordered_present_kp":[39,162,186],"keyphrases":["out-of-print materials","recurring issues","changing practices","out-of-print books","library materials","acquisition"],"prmu":["P","P","P","M","R","U"]}
{"id":"2160","title":"Challenges and trends in discrete manufacturing","abstract":"Over 50 years ago, the 100,000 workers at Ford's Rouge automobile factory turned out 1200 cars per day. Nowadays, Ford's plant on that same site still produces 800 cars each day but with just 3000 workers. Similar stories abound in the manufacturing industries; technology revolution and evolution; a shift from vertical integration, better business and production practices and improved industrial relations-all have changed manufacturing beyond recognition. So what are the current challenges and trends in manufacturing? Certainly, the relentless advance of technology will continue, as will user pressure for more customized design or improved environmental friendliness. Some trends are already with us and more, as yet indiscernible, will come. But one major, fundamental shift now resounding throughout industry is the way in which information involving every single aspect of the manufacturing process is being integrated into one seamless system","tok_text":"challeng and trend in discret manufactur \n over 50 year ago , the 100,000 worker at ford 's roug automobil factori turn out 1200 car per day . nowaday , ford 's plant on that same site still produc 800 car each day but with just 3000 worker . similar stori abound in the manufactur industri ; technolog revolut and evolut ; a shift from vertic integr , better busi and product practic and improv industri relations-al have chang manufactur beyond recognit . so what are the current challeng and trend in manufactur ? certainli , the relentless advanc of technolog will continu , as will user pressur for more custom design or improv environment friendli . some trend are alreadi with us and more , as yet indiscern , will come . but one major , fundament shift now resound throughout industri is the way in which inform involv everi singl aspect of the manufactur process is be integr into one seamless system","ordered_present_kp":[22,0,97,293,369,13],"keyphrases":["challenges","trends","discrete manufacturing","automobile factory","technology revolution","production practices","technology evolution","business practices","industrial relations","seamless manufacturing process"],"prmu":["P","P","P","P","P","P","R","R","M","R"]}
{"id":"2125","title":"Stochastic systems with a random jump in phase trajectory: stability of their motions","abstract":"The probabilistic stability of the perturbed motion of a system with parameters under the action of a general Markov process is studied. The phase vector is assumed to experience random jumps when the structure the system suffers random jumps. Such a situation is encountered, for example, in the motion of a solid with random jumps in its mass. The mean-square stability of random-structure linear systems and stability. of nonlinear systems in the first approximation are studied. The applied approach is helpful in studying the asymptotic probabilistic stability and mean-square exponential stability of stochastic systems through the stability of the respective deterministic systems","tok_text":"stochast system with a random jump in phase trajectori : stabil of their motion \n the probabilist stabil of the perturb motion of a system with paramet under the action of a gener markov process is studi . the phase vector is assum to experi random jump when the structur the system suffer random jump . such a situat is encount , for exampl , in the motion of a solid with random jump in it mass . the mean-squar stabil of random-structur linear system and stabil . of nonlinear system in the first approxim are studi . the appli approach is help in studi the asymptot probabilist stabil and mean-squar exponenti stabil of stochast system through the stabil of the respect determinist system","ordered_present_kp":[0,23,38,174,561,593],"keyphrases":["stochastic systems","random jump","phase trajectory","general Markov process","asymptotic probabilistic stability","mean-square exponential stability"],"prmu":["P","P","P","P","P","P"]}
{"id":"292","title":"Novel active noise-reducing headset using earshell vibration control","abstract":"Active noise-reducing (ANR) headsets are available commercially in applications varying from aviation communication to consumer audio. Current ANR systems use passive attenuation at high frequencies and loudspeaker-based active noise control at low frequencies to achieve broadband noise reduction. This paper presents a novel ANR headset in which the external noise transmitted to the user's ear via earshell vibration is reduced by controlling the vibration of the earshell using force actuators acting against an inertial mass or the earshell headband. Model-based theoretical analysis using velocity feedback control showed that current piezoelectric actuators provide sufficient force but require lower stiffness for improved low-frequency performance. Control simulations based on experimental data from a laboratory headset showed that good performance can potentially be achieved in practice by a robust feedback controller, while a single-frequency real-time control experiment verified that noise reduction can be achieved using earshell vibration control","tok_text":"novel activ noise-reduc headset use earshel vibrat control \n activ noise-reduc ( anr ) headset are avail commerci in applic vari from aviat commun to consum audio . current anr system use passiv attenu at high frequenc and loudspeaker-bas activ nois control at low frequenc to achiev broadband nois reduct . thi paper present a novel anr headset in which the extern nois transmit to the user 's ear via earshel vibrat is reduc by control the vibrat of the earshel use forc actuat act against an inerti mass or the earshel headband . model-bas theoret analysi use veloc feedback control show that current piezoelectr actuat provid suffici forc but requir lower stiff for improv low-frequ perform . control simul base on experiment data from a laboratori headset show that good perform can potenti be achiev in practic by a robust feedback control , while a single-frequ real-tim control experi verifi that nois reduct can be achiev use earshel vibrat control","ordered_present_kp":[6,36,134,150,188,284,468,495,563,604,660,822,856],"keyphrases":["active noise-reducing headset","earshell vibration control","aviation communication","consumer audio","passive attenuation","broadband noise reduction","force actuators","inertial mass","velocity feedback control","piezoelectric actuators","stiffness","robust feedback controller","single-frequency real-time control","external noise transmission"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"2038","title":"Choice from a three-element set: some lessons of the 2000 presidential campaign in the United States","abstract":"We consider the behavior of four choice rules - plurality voting, approval voting, Borda count, and self-consistent choice - when applied to choose the best option from a three-element set. It is assumed that the two main options are preferred by a large majority of the voters, while the third option gets a very small number of votes and influences the election outcome only when the two main options receive a close number of votes. When used to rate the main options, Borda count and self-consistent choice contain terms that allow both for the \"strength of preferences\" of the voters and the rating of the main candidates by voters who vote for the third option. In this way, it becomes possible to determine more reliably the winner when plurality voting or approval voting produce close results","tok_text":"choic from a three-el set : some lesson of the 2000 presidenti campaign in the unit state \n we consid the behavior of four choic rule - plural vote , approv vote , borda count , and self-consist choic - when appli to choos the best option from a three-el set . it is assum that the two main option are prefer by a larg major of the voter , while the third option get a veri small number of vote and influenc the elect outcom onli when the two main option receiv a close number of vote . when use to rate the main option , borda count and self-consist choic contain term that allow both for the \" strength of prefer \" of the voter and the rate of the main candid by voter who vote for the third option . in thi way , it becom possibl to determin more reliabl the winner when plural vote or approv vote produc close result","ordered_present_kp":[13,47,136,150,164,182],"keyphrases":["three-element set","2000 presidential campaign","plurality voting","approval voting","Borda count","self-consistent choice"],"prmu":["P","P","P","P","P","P"]}
{"id":"2005","title":"State-of-the-art in orthopaedic surgical navigation with a focus on medical image modalities","abstract":"This paper presents a review of surgical navigation systems in orthopaedics and categorizes these systems according to the image modalities that are used for the visualization of surgical action. Medical images used to be an essential part of surgical education and documentation as well as diagnosis and operation planning over many years. With the recent introduction of navigation techniques in orthopaedic surgery, a new field of application has been opened. Today surgical navigation systems - also known as image-guided surgery systems - are available for various applications in orthopaedic surgery. They visualize the position and orientation of surgical instruments as graphical overlays onto a medical image of the operated anatomy on a computer monitor. Preoperative image data such as computed tomography scans or intra operatively generated images (for example, ultrasonic, endoscopic or fluoroscopic images) are suitable for this purpose. A new category of medical images termed 'surgeon-defined anatomy' has been developed that exclusively relies upon the usage of navigation technology. Points on the anatomy are digitized interactively by the surgeon and are used to build up an abstract geometrical model of the bony structures to be operated on. This technique may be used when no other image data is available or appropriate for a given application","tok_text":"state-of-the-art in orthopaed surgic navig with a focu on medic imag modal \n thi paper present a review of surgic navig system in orthopaed and categor these system accord to the imag modal that are use for the visual of surgic action . medic imag use to be an essenti part of surgic educ and document as well as diagnosi and oper plan over mani year . with the recent introduct of navig techniqu in orthopaed surgeri , a new field of applic ha been open . today surgic navig system - also known as image-guid surgeri system - are avail for variou applic in orthopaed surgeri . they visual the posit and orient of surgic instrument as graphic overlay onto a medic imag of the oper anatomi on a comput monitor . preoper imag data such as comput tomographi scan or intra oper gener imag ( for exampl , ultrason , endoscop or fluoroscop imag ) are suitabl for thi purpos . a new categori of medic imag term ' surgeon-defin anatomi ' ha been develop that exclus reli upon the usag of navig technolog . point on the anatomi are digit interact by the surgeon and are use to build up an abstract geometr model of the boni structur to be oper on . thi techniqu may be use when no other imag data is avail or appropri for a given applic","ordered_present_kp":[20,58,277,499,614,635,694,737,763,906,1080,1110],"keyphrases":["orthopaedic surgical navigation","medical image modalities","surgical education","image-guided surgery systems","surgical instruments","graphical overlays","computer monitor","computed tomography scans","intra operatively generated images","surgeon-defined anatomy","abstract geometrical model","bony structures","surgical action visualization","medical image processing","image registration"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","R","M","M"]}
{"id":"2040","title":"Inverse problems for a mathematical model of ion exchange in a compressible ion exchanger","abstract":"A mathematical model of ion exchange is considered, allowing for ion exchanger compression in the process of ion exchange. Two inverse problems are investigated for this model, unique solvability is proved, and numerical solution methods are proposed. The efficiency of the proposed methods is demonstrated by a numerical experiment","tok_text":"invers problem for a mathemat model of ion exchang in a compress ion exchang \n a mathemat model of ion exchang is consid , allow for ion exchang compress in the process of ion exchang . two invers problem are investig for thi model , uniqu solvabl is prove , and numer solut method are propos . the effici of the propos method is demonstr by a numer experi","ordered_present_kp":[0,21,39,56,133,234,263],"keyphrases":["inverse problems","mathematical model","ion exchange","compressible ion exchanger","ion exchanger compression","unique solvability","numerical solution methods"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"1994","title":"A comparison of computational color constancy Algorithms. II. Experiments with image data","abstract":"For pt.I see ibid., vol. 11, no.9, p.972-84 (2002). We test a number of the leading computational color constancy algorithms using a comprehensive set of images. These were of 33 different scenes under 11 different sources representative of common illumination conditions. The algorithms studied include two gray world methods, a version of the Retinex method, several variants of Forsyth's (1990) gamut-mapping method, Cardei et al.'s (2000) neural net method, and Finlayson et al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel and Finlayson 2000). We discuss a number of issues in applying color constancy ideas to image data, and study in depth the effect of different preprocessing strategies. We compare the performance of the algorithms on image data with their performance on synthesized data. All data used for this study are available online at http:\/\/www.cs.sfu.ca\/~color\/data, and implementations for most of the algorithms are also available (http:\/\/www.cs.sfu.ca\/~color\/code). Experiments with synthesized data (part one of this paper) suggested that the methods which emphasize the use of the input data statistics, specifically color by correlation and the neural net algorithm, are potentially the most effective at estimating the chromaticity of the scene illuminant. Unfortunately, we were unable to realize comparable performance on real images. Here exploiting pixel intensity proved to be more beneficial than exploiting the details of image chromaticity statistics, and the three-dimensional (3-D) gamut-mapping algorithms gave the best performance","tok_text":"a comparison of comput color constanc algorithm . ii . experi with imag data \n for pt . i see ibid . , vol . 11 , no.9 , p.972 - 84 ( 2002 ) . we test a number of the lead comput color constanc algorithm use a comprehens set of imag . these were of 33 differ scene under 11 differ sourc repres of common illumin condit . the algorithm studi includ two gray world method , a version of the retinex method , sever variant of forsyth 's ( 1990 ) gamut-map method , cardei et al . 's ( 2000 ) neural net method , and finlayson et al . 's color by correl method ( finlayson et al . 1997 , 2001 ; hubel and finlayson 2000 ) . we discuss a number of issu in appli color constanc idea to imag data , and studi in depth the effect of differ preprocess strategi . we compar the perform of the algorithm on imag data with their perform on synthes data . all data use for thi studi are avail onlin at http:\/\/www.cs.sfu.ca\/~color\/data , and implement for most of the algorithm are also avail ( http:\/\/www.cs.sfu.ca\/~color\/cod ) . experi with synthes data ( part one of thi paper ) suggest that the method which emphas the use of the input data statist , specif color by correl and the neural net algorithm , are potenti the most effect at estim the chromat of the scene illumin . unfortun , we were unabl to realiz compar perform on real imag . here exploit pixel intens prove to be more benefici than exploit the detail of imag chromat statist , and the three-dimension ( 3-d ) gamut-map algorithm gave the best perform","ordered_present_kp":[16,67,67,304,352,389,443,489,534,732,828,1120,1236,1251,1345],"keyphrases":["computational color constancy algorithms","images","image data","illumination conditions","gray world methods","Retinex method","gamut-mapping method","neural net method","color by correlation method","preprocessing strategies","synthesized data","input data statistics","chromaticity","scene illuminant","pixel intensity"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2118","title":"Solutions for cooperative games","abstract":"A new concept of the characteristic function is defined. It matches cooperative games far better than the classical characteristic function and is useful in reducing the number of decisions that can be used as the unique solution of a game","tok_text":"solut for cooper game \n a new concept of the characterist function is defin . it match cooper game far better than the classic characterist function and is use in reduc the number of decis that can be use as the uniqu solut of a game","ordered_present_kp":[10,45,183,212],"keyphrases":["cooperative games","characteristic function","decisions","unique solution","transferrable utility"],"prmu":["P","P","P","P","U"]}
{"id":"217","title":"Vendor qualifications for IT staff and networking","abstract":"In some cases, vendor-run accreditation schemes can offer an objective measure of a job applicant's skills, but they do not always indicate the true extent of practical abilities","tok_text":"vendor qualif for it staff and network \n in some case , vendor-run accredit scheme can offer an object measur of a job applic 's skill , but they do not alway indic the true extent of practic abil","ordered_present_kp":[56,115,18,184],"keyphrases":["IT staff","vendor-run accreditation schemes","job applicant","practical abilities","network administrators"],"prmu":["P","P","P","P","M"]}
{"id":"252","title":"Reaching for five nines: ActiveWatch and SiteSeer","abstract":"Every Web admin's dream is achieving the fabled five nines-99.999 percent uptime. To attain such availability, your Web site must be down no more than about five minutes per year. Technologies like RAID, clustering, and load balancing make this easier, but to actually track uptime, maintain auditable records, and discover patterns in failures to prevent downtime in the future, you'll need to set up external monitoring. Because your Internet connection is a key factor in measuring uptime, you must monitor your site from the Internet itself, beyond your firewall. You could monitor with custom software on remote hosts, or you could use one of the two reasonably priced services available: Mercury Interactive's ActiveWatch and Freshwater Software's SiteSeer. (Freshwater Software has been a subsidiary of Mercury Interactive for about a year now.) The two services offer a slightly different mix of features and target different markets. Both services offer availability and performance monitoring from several remote locations, alerts to email or pager, and periodic reports. They differ in what's most easily monitored, and in the way you interact with the services","tok_text":"reach for five nine : activewatch and sites \n everi web admin 's dream is achiev the fabl five nines-99.999 percent uptim . to attain such avail , your web site must be down no more than about five minut per year . technolog like raid , cluster , and load balanc make thi easier , but to actual track uptim , maintain audit record , and discov pattern in failur to prevent downtim in the futur , you 'll need to set up extern monitor . becaus your internet connect is a key factor in measur uptim , you must monitor your site from the internet itself , beyond your firewal . you could monitor with custom softwar on remot host , or you could use one of the two reason price servic avail : mercuri interact 's activewatch and freshwat softwar 's sites . ( freshwat softwar ha been a subsidiari of mercuri interact for about a year now . ) the two servic offer a slightli differ mix of featur and target differ market . both servic offer avail and perform monitor from sever remot locat , alert to email or pager , and period report . they differ in what 's most easili monitor , and in the way you interact with the servic","ordered_present_kp":[152,318,373,419,448,946,973,1017],"keyphrases":["Web site","auditable records","downtime","external monitoring","Internet connection","performance monitoring","remote locations","periodic reports","uptime tracking","failure pattern discovery","Mercury Interactive ActiveWatch","Freshwater Software SiteSeer","availability monitoring","email alerts","pager alerts"],"prmu":["P","P","P","P","P","P","P","P","R","M","R","R","R","R","R"]}
{"id":"1969","title":"Modeling self-consistent multi-class dynamic traffic flow","abstract":"In this study, we present a systematic self-consistent multiclass multilane traffic model derived from the vehicular Boltzmann equation and the traffic dispersion model. The multilane domain is considered as a two-dimensional space and the interaction among vehicles in the domain is described by a dispersion model. The reason we consider a multilane domain as a two-dimensional space is that the driving behavior of road users may not be restricted by lanes, especially motorcyclists. The dispersion model, which is a nonlinear Poisson equation, is derived from the car-following theory and the equilibrium assumption. Under the concept that all kinds of users share the finite section, the density is distributed on a road by the dispersion model. In addition, the dynamic evolution of the traffic flow is determined by the systematic gas-kinetic model derived from the Boltzmann equation. Multiplying Boltzmann equation by the zeroth, first- and second-order moment functions, integrating both side of the equation and using chain rules, we can derive continuity, motion and variance equation, respectively. However, the second-order moment function, which is the square of the individual velocity, is employed by previous researches does not have physical meaning in traffic flow","tok_text":"model self-consist multi-class dynam traffic flow \n in thi studi , we present a systemat self-consist multiclass multilan traffic model deriv from the vehicular boltzmann equat and the traffic dispers model . the multilan domain is consid as a two-dimension space and the interact among vehicl in the domain is describ by a dispers model . the reason we consid a multilan domain as a two-dimension space is that the drive behavior of road user may not be restrict by lane , especi motorcyclist . the dispers model , which is a nonlinear poisson equat , is deriv from the car-follow theori and the equilibrium assumpt . under the concept that all kind of user share the finit section , the densiti is distribut on a road by the dispers model . in addit , the dynam evolut of the traffic flow is determin by the systemat gas-kinet model deriv from the boltzmann equat . multipli boltzmann equat by the zeroth , first- and second-ord moment function , integr both side of the equat and use chain rule , we can deriv continu , motion and varianc equat , respect . howev , the second-ord moment function , which is the squar of the individu veloc , is employ by previou research doe not have physic mean in traffic flow","ordered_present_kp":[113,151,185,434,527,571,758,1034,537],"keyphrases":["multilane traffic model","vehicular Boltzmann equation","traffic dispersion model","road users","nonlinear Poisson equation","Poisson equation","car-following theory","dynamic evolution","variance equation","self-consistent multiclass dynamic traffic flow modeling","motion equation"],"prmu":["P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"376","title":"Recursive state estimation for multiple switching models with unknown transition probabilities","abstract":"This work considers hybrid systems with continuous-valued target states and discrete-valued regime variable. The changes (switches) of the regime variable are modeled by a finite state Markov chain with unknown and random transition probabilities following Dirichlet distributions. Our work analytically derives the marginal posterior distribution of the states and regime variables, the transition probabilities being integrated out. This leads to a variety of recursive hybrid state estimation schemes which are an appealing intuitive and straightforward extension of standard algorithms. Their performance is illustrated by a maneuvering target tracking example","tok_text":"recurs state estim for multipl switch model with unknown transit probabl \n thi work consid hybrid system with continuous-valu target state and discrete-valu regim variabl . the chang ( switch ) of the regim variabl are model by a finit state markov chain with unknown and random transit probabl follow dirichlet distribut . our work analyt deriv the margin posterior distribut of the state and regim variabl , the transit probabl be integr out . thi lead to a varieti of recurs hybrid state estim scheme which are an appeal intuit and straightforward extens of standard algorithm . their perform is illustr by a maneuv target track exampl","ordered_present_kp":[0,23,49,91,110,143,230,272,302,350,612],"keyphrases":["recursive state estimation","multiple switching models","unknown transition probabilities","hybrid systems","continuous-valued target states","discrete-valued regime variable","finite state Markov chain","random transition probabilities","Dirichlet distributions","marginal posterior distribution","maneuvering target tracking"],"prmu":["P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2084","title":"Evaluation of combined dispatching and routeing strategies for a flexible manufacturing system","abstract":"This paper deals with the evaluation of combined dispatching and routeing strategies on the performance of a flexible manufacturing system. Three routeing policies - no alternative routings, alternative routeing dynamics and alternative routeing plans - are considered with four dispatching rules with finite buffer capacity. In addition, the effect of changing part mix ratios is also discussed. The performance measures considered are makespan, average machine utilization, average flow time and average delay at local input buffers. Simulation results indicate that the alternative routings dynamic policy gives the best results in three performance measures except for average delay at local input buffers. Further, the effect of changing part mix ratios is not significant","tok_text":"evalu of combin dispatch and rout strategi for a flexibl manufactur system \n thi paper deal with the evalu of combin dispatch and rout strategi on the perform of a flexibl manufactur system . three rout polici - no altern rout , altern rout dynam and altern rout plan - are consid with four dispatch rule with finit buffer capac . in addit , the effect of chang part mix ratio is also discuss . the perform measur consid are makespan , averag machin util , averag flow time and averag delay at local input buffer . simul result indic that the altern rout dynam polici give the best result in three perform measur except for averag delay at local input buffer . further , the effect of chang part mix ratio is not signific","ordered_present_kp":[215,49,291,310,362,457],"keyphrases":["flexible manufacturing system","alternative routings","dispatching rules","finite buffer capacity","part mix ratios","average flow time","FMS"],"prmu":["P","P","P","P","P","P","U"]}
{"id":"333","title":"Teaching psychology as a laboratory science in the age of the Internet","abstract":"For over 30 years, psychologists have relied on computers to teach experimental psychology. With the advent of experiment generators, students can create well-designed experiments and can test sophisticated hypotheses from the start of their undergraduate training. Characteristics of new Net-based experiment generators are discussed and compared with traditional stand-alone generators. A call is made to formally evaluate the instructional effectiveness of the wide range of experiment generators now available. Specifically, software should be evaluated in terms of known learning outcomes, using appropriate control groups. The many inherent differences between any two software programs should be made clear. The teacher's instructional method should be fully described and held constant between comparisons. Finally, the often complex interaction between the teacher's instructional method and the pedagogical details of the software must be considered","tok_text":"teach psycholog as a laboratori scienc in the age of the internet \n for over 30 year , psychologist have reli on comput to teach experiment psycholog . with the advent of experi gener , student can creat well-design experi and can test sophist hypothes from the start of their undergradu train . characterist of new net-bas experi gener are discuss and compar with tradit stand-alon gener . a call is made to formal evalu the instruct effect of the wide rang of experi gener now avail . specif , softwar should be evalu in term of known learn outcom , use appropri control group . the mani inher differ between ani two softwar program should be made clear . the teacher 's instruct method should be fulli describ and held constant between comparison . final , the often complex interact between the teacher 's instruct method and the pedagog detail of the softwar must be consid","ordered_present_kp":[21,57,113,204,277,316,372,426,496,531,565,834],"keyphrases":["laboratory science","Internet","computers","well-designed experiments","undergraduate training","Net-based experiment generators","stand-alone generators","instructional effectiveness","software","known learning outcomes","control groups","pedagogical details","experimental psychology teaching","hypothesis testing","teacher instructional method"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","R","M","R"]}
{"id":"2164","title":"Electronic signatures - much ado?","abstract":"Whilst the market may be having a crisis of confidence regarding the prospects for e-commerce, the EU and the Government continue apace to develop the legal framework. Most recently, this has resulted in the Electronic Signatures Regulations 2002. These Regulations were made on 13 February 2002 and came into force on 8 March 2002. The Regulations implement the European Electronic Signatures Directive (1999\/93\/EC). Critics may say that the Regulations were implemented too late (they were due to have been implemented by 19 July 2001), with too short a consultation period (25 January 2002 to 12 February 2002) and with an unconvincing case as to what they add to English law (as to which, read on). The author explains the latest development on e-signatures and the significance of Certification Service Providers (CSPs)","tok_text":"electron signatur - much ado ? \n whilst the market may be have a crisi of confid regard the prospect for e-commerc , the eu and the govern continu apac to develop the legal framework . most recent , thi ha result in the electron signatur regul 2002 . these regul were made on 13 februari 2002 and came into forc on 8 march 2002 . the regul implement the european electron signatur direct ( 1999\/93 \/ ec ) . critic may say that the regul were implement too late ( they were due to have been implement by 19 juli 2001 ) , with too short a consult period ( 25 januari 2002 to 12 februari 2002 ) and with an unconvinc case as to what they add to english law ( as to which , read on ) . the author explain the latest develop on e-signatur and the signific of certif servic provid ( csp )","ordered_present_kp":[105,167,220,354],"keyphrases":["e-commerce","legal framework","Electronic Signatures Regulations 2002","European Electronic Signatures Directive"],"prmu":["P","P","P","P"]}
{"id":"2121","title":"Flexibility analysis of complex technical systems under uncertainty","abstract":"An important problem in designing technical systems under partial uncertainty of the initial physical, chemical, and technological data is the determination of a design in which the technical system is flexible, i.e., its control system is capable of guaranteeing that the constraints hold even under changes in external and internal factors and application of fuzzy mathematical models in its design. Three flexibility problems, viz., the flexibility of a technical system of given structure, structural flexibility of a technical system, and the optimal design guaranteeing the flexibility of a technical system, are studied. Two approaches to these problems are elaborated. Results of a computation experiment are given","tok_text":"flexibl analysi of complex technic system under uncertainti \n an import problem in design technic system under partial uncertainti of the initi physic , chemic , and technolog data is the determin of a design in which the technic system is flexibl , i.e. , it control system is capabl of guarante that the constraint hold even under chang in extern and intern factor and applic of fuzzi mathemat model in it design . three flexibl problem , viz . , the flexibl of a technic system of given structur , structur flexibl of a technic system , and the optim design guarante the flexibl of a technic system , are studi . two approach to these problem are elabor . result of a comput experi are given","ordered_present_kp":[0,19,111,260,381,501,548],"keyphrases":["flexibility analysis","complex technical systems","partial uncertainty","control system","fuzzy mathematical models","structural flexibility","optimal design"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"296","title":"Using the Web to answer legal reference questions","abstract":"In an effort to help non-law librarians with basic legal reference questions, the author highlights three basic legal Web sites and outlines useful subject-specific Web sites that focus on statutes and regulations, case law and attorney directories","tok_text":"use the web to answer legal refer question \n in an effort to help non-law librarian with basic legal refer question , the author highlight three basic legal web site and outlin use subject-specif web site that focu on statut and regul , case law and attorney directori","ordered_present_kp":[22,237,250],"keyphrases":["legal reference questions","case law","attorney directories","World Wide Web","nonlaw librarians"],"prmu":["P","P","P","M","M"]}
{"id":"38","title":"A PID standard: What, why, how?","abstract":"The paper is written for all who develop and use P&IDs. It will aid in solving the long existing and continuing problem of confusing information on P&IDs. The acronym P&ID is widely understood to mean the principal document used to define the details of how a process works and how it is controlled. The ISA Dictionary definition for P&ID tells what they do, \"show the interconnection of process equipment and the instrumentation used to control the process. In the process industry a standard set of symbols is used to prepare drawings of processes. The instrument symbols used in these drawings are generally based on ISA-S5.1.\" In the paper the ISA standard is referred to as ISA-5.1. The article develops the concept of the \"standard\" and poses some of the questions that the \"standard\" can answer","tok_text":"a pid standard : what , whi , how ? \n the paper is written for all who develop and use p&id . it will aid in solv the long exist and continu problem of confus inform on p&id . the acronym p&id is wide understood to mean the princip document use to defin the detail of how a process work and how it is control . the isa dictionari definit for p&id tell what they do , \" show the interconnect of process equip and the instrument use to control the process . in the process industri a standard set of symbol is use to prepar draw of process . the instrument symbol use in these draw are gener base on isa-s5.1 . \" in the paper the isa standard is refer to as isa-5.1 . the articl develop the concept of the \" standard \" and pose some of the question that the \" standard \" can answer","ordered_present_kp":[224,656,628],"keyphrases":["principal document","ISA standard","ISA-5.1","P&ID standard","process controlled"],"prmu":["P","P","P","R","R"]}
{"id":"2199","title":"Activity and location recognition using wearable sensors","abstract":"Using measured acceleration and angular velocity data gathered through inexpensive, wearable sensors, this dead-reckoning method can determine a user's location, detect transitions between preselected locations, and recognize and classify sitting, standing, and walking behaviors. Experiments demonstrate the proposed method's effectiveness","tok_text":"activ and locat recognit use wearabl sensor \n use measur acceler and angular veloc data gather through inexpens , wearabl sensor , thi dead-reckon method can determin a user 's locat , detect transit between preselect locat , and recogn and classifi sit , stand , and walk behavior . experi demonstr the propos method 's effect","ordered_present_kp":[50,69,29,135,169,208,192,196,256,268],"keyphrases":["wearable sensors","measured acceleration","angular velocity","dead-reckoning method","user's location","transitions","sitting","preselected locations","standing","walking"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"1950","title":"General solution of a density functionally gradient piezoelectric cantilever and its applications","abstract":"We have used the plane strain theory of transversely isotropic bodies to study a piezoelectric cantilever. In order to find the general solution of a density functionally gradient piezoelectric cantilever, we have used the inverse method (i.e. the Airy stress function method). We have obtained the stress and induction functions in the form of polynomials as well as the general solution of the beam. Based on this general solution, we have deduced the solutions of the cantilever under different loading conditions. Furthermore, as applications of this general solution in engineering, we have studied the tip deflection and blocking force of a piezoelectric cantilever actuator. Finally, we have addressed a method to determine the density distribution profile for a given piezoelectric material","tok_text":"gener solut of a densiti function gradient piezoelectr cantilev and it applic \n we have use the plane strain theori of transvers isotrop bodi to studi a piezoelectr cantilev . in order to find the gener solut of a densiti function gradient piezoelectr cantilev , we have use the invers method ( i.e. the airi stress function method ) . we have obtain the stress and induct function in the form of polynomi as well as the gener solut of the beam . base on thi gener solut , we have deduc the solut of the cantilev under differ load condit . furthermor , as applic of thi gener solut in engin , we have studi the tip deflect and block forc of a piezoelectr cantilev actuat . final , we have address a method to determin the densiti distribut profil for a given piezoelectr materi","ordered_present_kp":[96,119,279,304,397,526,643,722,759],"keyphrases":["plane strain theory","transversely isotropic bodies","inverse method","Airy stress function","polynomials","loading conditions","piezoelectric cantilever actuator","density distribution profile","piezoelectric material"],"prmu":["P","P","P","P","P","P","P","P","P"]}
{"id":"197","title":"Mixture of experts classification using a hierarchical mixture model","abstract":"A three-level hierarchical mixture model for classification is presented that models the following data generation process: (1) the data are generated by a finite number of sources (clusters), and (2) the generation mechanism of each source assumes the existence of individual internal class-labeled sources (subclusters of the external cluster). The model estimates the posterior probability of class membership similar to a mixture of experts classifier. In order to learn the parameters of the model, we have developed a general training approach based on maximum likelihood that results in two efficient training algorithms. Compared to other classification mixture models, the proposed hierarchical model exhibits several advantages and provides improved classification performance as indicated by the experimental results","tok_text":"mixtur of expert classif use a hierarch mixtur model \n a three-level hierarch mixtur model for classif is present that model the follow data gener process : ( 1 ) the data are gener by a finit number of sourc ( cluster ) , and ( 2 ) the gener mechan of each sourc assum the exist of individu intern class-label sourc ( subclust of the extern cluster ) . the model estim the posterior probabl of class membership similar to a mixtur of expert classifi . in order to learn the paramet of the model , we have develop a gener train approach base on maximum likelihood that result in two effici train algorithm . compar to other classif mixtur model , the propos hierarch model exhibit sever advantag and provid improv classif perform as indic by the experiment result","ordered_present_kp":[31,17,136,435,374],"keyphrases":["classification","hierarchical mixture model","data generation process","posterior probability of class membership","experts classifier","Bayes classifier"],"prmu":["P","P","P","P","P","M"]}
{"id":"213","title":"An application of fuzzy linear regression to the information technology in Turkey","abstract":"Fuzzy set theory deals with the vagueness of human thought. A major contribution of fuzzy set theory is its capability of representing vague knowledge. Fuzzy set theory is very practical when sufficient and reliable data isn't available. Information technology (IT) is the acquisition, processing, storage and dissemination of information in all its forms (auditory, pictorial, textual and numerical) through a combination of computers, telecommunication, networks and electronic devices. IT includes matters concerned with the furtherance of computer science and technology, design, development, installation and implementation of information systems and applications. In the paper, assuming that there are n independent variables and the regression function is linear, the possible levels of information technology (the sale levels of computer equipment) in Turkey are forecasted by using fuzzy linear regression. The independent variables assumed are the import level and the export level of computer equipment","tok_text":"an applic of fuzzi linear regress to the inform technolog in turkey \n fuzzi set theori deal with the vagu of human thought . a major contribut of fuzzi set theori is it capabl of repres vagu knowledg . fuzzi set theori is veri practic when suffici and reliabl data is n't avail . inform technolog ( it ) is the acquisit , process , storag and dissemin of inform in all it form ( auditori , pictori , textual and numer ) through a combin of comput , telecommun , network and electron devic . it includ matter concern with the further of comput scienc and technolog , design , develop , instal and implement of inform system and applic . in the paper , assum that there are n independ variabl and the regress function is linear , the possibl level of inform technolog ( the sale level of comput equip ) in turkey are forecast by use fuzzi linear regress . the independ variabl assum are the import level and the export level of comput equip","ordered_present_kp":[13,41,61,93,440,449,474,536,609,699],"keyphrases":["fuzzy linear regression","information technology","Turkey","IT","computers","telecommunication","electronic devices","computer science","information systems","regression function","vague knowledge representation","computer technology","computer equipment export level"],"prmu":["P","P","P","P","P","P","P","P","P","P","M","R","R"]}
{"id":"256","title":"Debugging Web applications","abstract":"The author considers how one can save time tracking down bugs in Web-based applications by arming yourself with the right tools and programming practices. A wide variety of debugging tools have been written with Web developers in mind","tok_text":"debug web applic \n the author consid how one can save time track down bug in web-bas applic by arm yourself with the right tool and program practic . a wide varieti of debug tool have been written with web develop in mind","ordered_present_kp":[132],"keyphrases":["programming","Web application debugging tools"],"prmu":["P","R"]}
{"id":"1990","title":"Electrical facility construction work for information network structuring by the use of sewage conduits","abstract":"To confront the advent of the advanced information society, there has been a pressing demand for the adjustment of the communications infrastructure and the structuring of the information network by utilizing the sewage conduits. The City of Tokyo is promoting a project by the name of the sewer optical fiber teleway (SOFT) network plan. According to this plan, the total distance of the optical fiber network laid in the sewer conduits is scheduled to reach about 470 km by the end of March 2000. At the final stage, this distance will reach 800 km as a whole. We completed the construction work for the information control facilities scattered in 11 places inclusive of the Treatment Site S, with the intention to adjust and extend the information transmission network laid through the above-mentioned optical fiber network, to be used exclusively by the Bureau of Sewerage. This construction work is described in the paper","tok_text":"electr facil construct work for inform network structur by the use of sewag conduit \n to confront the advent of the advanc inform societi , there ha been a press demand for the adjust of the commun infrastructur and the structur of the inform network by util the sewag conduit . the citi of tokyo is promot a project by the name of the sewer optic fiber teleway ( soft ) network plan . accord to thi plan , the total distanc of the optic fiber network laid in the sewer conduit is schedul to reach about 470 km by the end of march 2000 . at the final stage , thi distanc will reach 800 km as a whole . we complet the construct work for the inform control facil scatter in 11 place inclus of the treatment site s , with the intent to adjust and extend the inform transmiss network laid through the above-ment optic fiber network , to be use exclus by the bureau of sewerag . thi construct work is describ in the paper","ordered_present_kp":[0,32,70,191,291,640,695,755,854],"keyphrases":["electrical facility construction work","information network structuring","sewage conduits","communications infrastructure","Tokyo","information control facilities","Treatment Site S","information transmission network","Bureau of Sewerage","sewer optical fiber teleway network plan","asynchronous transmission mode switches","ATM switches"],"prmu":["P","P","P","P","P","P","P","P","P","R","M","U"]}
{"id":"40","title":"New tuning method for PID controller","abstract":"In this paper, a tuning method for proportional-integral-derivative (PID) controller and the performance assessment formulas for this method are proposed. This tuning method is based on a genetic algorithm based PID controller design method. For deriving the tuning formula, the genetic algorithm based design method is applied to design PID controllers for a variety of processes. The relationship between the controller parameters and the parameters that characterize the process dynamics are determined and the tuning formula is then derived. Using simulation studies, the rules for assessing the performance of a PID controller tuned by the proposed method are also given. This makes it possible to incorporate the capability to determine if the PID controller is well tuned or not into an autotuner. An autotuner based on this new tuning method and the corresponding performance assessment rules is also established. Simulations and real-time experimental results are given to demonstrate the effectiveness and usefulness of these formulas","tok_text":"new tune method for pid control \n in thi paper , a tune method for proportional-integral-deriv ( pid ) control and the perform assess formula for thi method are propos . thi tune method is base on a genet algorithm base pid control design method . for deriv the tune formula , the genet algorithm base design method is appli to design pid control for a varieti of process . the relationship between the control paramet and the paramet that character the process dynam are determin and the tune formula is then deriv . use simul studi , the rule for assess the perform of a pid control tune by the propos method are also given . thi make it possibl to incorpor the capabl to determin if the pid control is well tune or not into an autotun . an autotun base on thi new tune method and the correspond perform assess rule is also establish . simul and real-tim experiment result are given to demonstr the effect and use of these formula","ordered_present_kp":[4,20,199,224,454,730],"keyphrases":["tuning method","PID controller","genetic algorithm","controller design method","process dynamics","autotuner","proportional-integral-derivative controller"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"2159","title":"Real-time enterprise solutions for discrete manufacturing and consumer goods","abstract":"Customer satisfaction and a focus on core competencies have dominated the thinking of a whole host of industries in recent years. However, one outcome, the outsourcing of noncore activities, has made the production of goods-from order entry to final delivery-more and more complex. Suppliers, subsuppliers, producers and customers are therefore busy adopting a new, more collaborative approach. This is mainly taking the form of order-driven planning and scheduling of production, but it is also being steered by a need to reduce inventories and working capital as well as a desire to increase throughput and optimize production","tok_text":"real-tim enterpris solut for discret manufactur and consum good \n custom satisfact and a focu on core compet have domin the think of a whole host of industri in recent year . howev , one outcom , the outsourc of noncor activ , ha made the product of goods-from order entri to final delivery-mor and more complex . supplier , subsuppli , produc and custom are therefor busi adopt a new , more collabor approach . thi is mainli take the form of order-driven plan and schedul of product , but it is also be steer by a need to reduc inventori and work capit as well as a desir to increas throughput and optim product","ordered_present_kp":[0,29,52,66,97,443],"keyphrases":["real-time enterprise solutions","discrete manufacturing","consumer goods","customer satisfaction","core competencies","order-driven planning","production scheduling","working capital reduction","inventories reduction"],"prmu":["P","P","P","P","P","P","R","M","M"]}
{"id":"2001","title":"A simple graphic approach for observer decomposition","abstract":"Based upon the proposition that the roles of inputs and outputs in a physical system and those in the corresponding output-injection observer do not really have to be consistent, a systematic procedure is developed in this work to properly divide a set of sparse system models and measurement models into a number of independent subsets with the help of a visual aid. Several smaller sub-observers can then be constructed accordingly to replace the original one. The size of each sub-observer may be further reduced by strategically selecting one or more appended states. These techniques are shown to be quite effective in relieving on-line computation load of the output-injection observers and also in identifying detectable sub-systems","tok_text":"a simpl graphic approach for observ decomposit \n base upon the proposit that the role of input and output in a physic system and those in the correspond output-inject observ do not realli have to be consist , a systemat procedur is develop in thi work to properli divid a set of spars system model and measur model into a number of independ subset with the help of a visual aid . sever smaller sub-observ can then be construct accordingli to replac the origin one . the size of each sub-observ may be further reduc by strateg select one or more append state . these techniqu are shown to be quit effect in reliev on-lin comput load of the output-inject observ and also in identifi detect sub-system","ordered_present_kp":[8,29,153,279,302,332,394],"keyphrases":["graphic approach","observer decomposition","output-injection observer","sparse system models","measurement models","independent subsets","sub-observers","online computation load","detectable subsystems"],"prmu":["P","P","P","P","P","P","P","M","M"]}
{"id":"2044","title":"Three-dimensional geometrical optics code for indoor propagation","abstract":"This paper presents a program, GO 3D, for computing the fields of a transmitter in an indoor environment using geometrical optics. The program uses an \"image tree\" data structure to construct the images needed to compute all the rays carrying fields above a preset \"threshold\" value, no matter how many reflections are needed. The paper briefly describes the input file required to define wall construction, the floor plan, the transmitter, and the receiver locations. A case study consisting of a long corridor with a small room on one side is used to demonstrate the features of the GO 3D program","tok_text":"three-dimension geometr optic code for indoor propag \n thi paper present a program , go 3d , for comput the field of a transmitt in an indoor environ use geometr optic . the program use an \" imag tree \" data structur to construct the imag need to comput all the ray carri field abov a preset \" threshold \" valu , no matter how mani reflect are need . the paper briefli describ the input file requir to defin wall construct , the floor plan , the transmitt , and the receiv locat . a case studi consist of a long corridor with a small room on one side is use to demonstr the featur of the go 3d program","ordered_present_kp":[0,39,408,429,119,466],"keyphrases":["three-dimensional geometrical optics","indoor propagation","transmitter","wall construction","floor plan","receiver locations","3D geometrical optics code","image tree data structure","image construction","ray tracing","data visualisation"],"prmu":["P","P","P","P","P","P","R","R","R","M","M"]}
{"id":"21","title":"Discrete output feedback sliding mode control of second order systems - a moving switching line approach","abstract":"The sliding mode control systems (SMCS) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase. For second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line. In this paper, we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control. The main contributions of this work are such that we do not require to use system states as it makes use of only the output samples for designing the controller. and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase. Simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback","tok_text":"discret output feedback slide mode control of second order system - a move switch line approach \n the slide mode control system ( smc ) for which the switch variabl is design independ of the initi condit are known to be sensit to paramet variat and extran disturb dure the reach phase . for second order system thi drawback is elimin by use the move switch line techniqu where the switch line is initi design to pass the initi condit and is subsequ move toward a predetermin switch line . in thi paper , we make use of the abov idea of move switch line togeth with the reach law approach to design a discret output feedback slide mode control . the main contribut of thi work are such that we do not requir to use system state as it make use of onli the output sampl for design the control . and by use the move switch line a low sensit system is obtain through shorten the reach phase . simul result show that the fast output sampl feedback guarante slide motion similar to that obtain use state feedback","ordered_present_kp":[24,150,230,70,0,915,991],"keyphrases":["discrete output feedback","sliding mode control","moving switching line","switching variable","parameter variations","fast output sampling feedback","state feedback"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"2138","title":"Reachability sets of a class of multistep control processes: their design","abstract":"An upper estimate and an iterative \"restriction\" algorithm for the reachability set for determining the optimal control for a class of multistep control processes are designed","tok_text":"reachabl set of a class of multistep control process : their design \n an upper estim and an iter \" restrict \" algorithm for the reachabl set for determin the optim control for a class of multistep control process are design","ordered_present_kp":[0,27,73,158],"keyphrases":["reachability sets","multistep control processes","upper estimate","optimal control","discrete systems","iterative restriction algorithm"],"prmu":["P","P","P","P","U","R"]}
{"id":"1949","title":"A new graphical user interface for fast construction of computation phantoms and MCNP calculations: application to calibration of in vivo measurement systems","abstract":"Reports on a new utility for development of computational phantoms for Monte Carlo calculations and data analysis for in vivo measurements of radionuclides deposited in tissues. The individual properties of each worker can be acquired for a rather precise geometric representation of his (her) anatomy, which is particularly important for low energy gamma ray emitting sources such as thorium, uranium, plutonium and other actinides. The software enables automatic creation of an MCNP input data file based on scanning data. The utility includes segmentation of images obtained with either computed tomography or magnetic resonance imaging by distinguishing tissues according to their signal (brightness) and specification of the source and detector. In addition, a coupling of individual voxels within the tissue is used to reduce the memory demand and to increase the calculational speed. The utility was tested for low energy emitters in plastic and biological tissues as well as for computed tomography and magnetic resonance imaging scanning information","tok_text":"a new graphic user interfac for fast construct of comput phantom and mcnp calcul : applic to calibr of in vivo measur system \n report on a new util for develop of comput phantom for mont carlo calcul and data analysi for in vivo measur of radionuclid deposit in tissu . the individu properti of each worker can be acquir for a rather precis geometr represent of hi ( her ) anatomi , which is particularli import for low energi gamma ray emit sourc such as thorium , uranium , plutonium and other actinid . the softwar enabl automat creation of an mcnp input data file base on scan data . the util includ segment of imag obtain with either comput tomographi or magnet reson imag by distinguish tissu accord to their signal ( bright ) and specif of the sourc and detector . in addit , a coupl of individu voxel within the tissu is use to reduc the memori demand and to increas the calcul speed . the util wa test for low energi emitt in plastic and biolog tissu as well as for comput tomographi and magnet reson imag scan inform","ordered_present_kp":[50,182,103,239,262,300,334,547,576,639,724,6,50,93,103,715,761,794,846,879,935,947,997,373,416,496,510,524],"keyphrases":["graphical user interface","computational phantoms","computational phantoms","calibration","in vivo measurements","in vivo measurement systems","Monte Carlo calculations","radionuclides","tissues","worker","precise geometric representation","anatomy","low energy gamma ray emitting sources","actinides","software","automatic creation","MCNP input data file","scanning data","computed tomography","signal","brightness","detector","individual voxels","memory demand","calculational speed","plastic","biological tissues","magnetic resonance imaging scanning information","computation phantoms","Th","U","Pu"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","U","U","U"]}
{"id":"2180","title":"Standard protocol for exchange of health-checkup data based on SGML: the Health-checkup Data Markup Language (HDML)","abstract":"The objectives are to develop a health\/medical data interchange model for efficient electronic exchange of data among health-checkup facilities. A Health-checkup Data Markup Language (HDML) was developed on the basis of the Standard Generalized Markup Language (SGML), and a feasibility study carried out, involving data exchange between two health checkup facilities. The structure of HDML is described. The transfer of numerical lab data, summary findings and health status assessment was successful. HDML is an improvement to laboratory data exchange. Further work has to address the exchange of qualitative and textual data","tok_text":"standard protocol for exchang of health-checkup data base on sgml : the health-checkup data markup languag ( hdml ) \n the object are to develop a health \/ medic data interchang model for effici electron exchang of data among health-checkup facil . a health-checkup data markup languag ( hdml ) wa develop on the basi of the standard gener markup languag ( sgml ) , and a feasibl studi carri out , involv data exchang between two health checkup facil . the structur of hdml is describ . the transfer of numer lab data , summari find and health statu assess wa success . hdml is an improv to laboratori data exchang . further work ha to address the exchang of qualit and textual data","ordered_present_kp":[61,72,161,502,519,536],"keyphrases":["SGML","Health-checkup Data Markup Language","data interchange model","numerical lab data","summary findings","health status assessment","health checkup data exchange"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"237","title":"International library consortia: positive starts, promising futures","abstract":"Library consortia have grown substantially over the past ten years, both within North America and globally. As this resurgent consortial movement has begun to mature, and as publishers and vendors have begun to adapt to consortial purchasing models, consortia have expanded their agendas for action. The movement to globalize consortia is traced (including the development and current work of the International Coalition of Library Consortia-ICOLC). A methodology is explored to classify library consortia by articulating the key factors that affect and distinguish consortia as organizations within three major areas: strategic, tactical, and practical (or managerial) concerns. Common consortial values are examined, and a list of known international library consortia is presented","tok_text":"intern librari consortia : posit start , promis futur \n librari consortia have grown substanti over the past ten year , both within north america and global . as thi resurg consorti movement ha begun to matur , and as publish and vendor have begun to adapt to consorti purchas model , consortia have expand their agenda for action . the movement to global consortia is trace ( includ the develop and current work of the intern coalit of librari consortia-icolc ) . a methodolog is explor to classifi librari consortia by articul the key factor that affect and distinguish consortia as organ within three major area : strateg , tactic , and practic ( or manageri ) concern . common consorti valu are examin , and a list of known intern librari consortia is present","ordered_present_kp":[260,0],"keyphrases":["international library consortia","consortial purchasing models"],"prmu":["P","P"]}
{"id":"272","title":"Median partitioning: a novel method for the selection of representative subsets from large compound pools","abstract":"A method termed median partitioning (MP) has been developed to select diverse sets of molecules from large compound pools. Unlike many other methods for subset selection, the MP approach does not depend on pairwise comparison of molecules and can therefore be applied to very large compound collections. The only time limiting step is the calculation of molecular descriptors for database compounds. MP employs arrays of property descriptors with little correlation to divide large compound pools into partitions from which representative molecules can be selected. In each of n subsequent steps, a population of molecules is divided into subpopulations above and below the median value of a property descriptor until a desired number of 2\/sup n\/ partitions are obtained. For descriptor evaluation and selection, an entropy formulation was embedded in a genetic algorithm. MP has been applied to generate a subset of the Available Chemicals Directory, and the results have been compared with cell-based partitioning","tok_text":"median partit : a novel method for the select of repres subset from larg compound pool \n a method term median partit ( mp ) ha been develop to select divers set of molecul from larg compound pool . unlik mani other method for subset select , the mp approach doe not depend on pairwis comparison of molecul and can therefor be appli to veri larg compound collect . the onli time limit step is the calcul of molecular descriptor for databas compound . mp employ array of properti descriptor with littl correl to divid larg compound pool into partit from which repres molecul can be select . in each of n subsequ step , a popul of molecul is divid into subpopul abov and below the median valu of a properti descriptor until a desir number of 2 \/ sup n\/ partit are obtain . for descriptor evalu and select , an entropi formul wa embed in a genet algorithm . mp ha been appli to gener a subset of the avail chemic directori , and the result have been compar with cell-bas partit","ordered_present_kp":[0,68,164,373,406,431,807,836,896,958],"keyphrases":["median partitioning","large compound pools","molecules","time limiting step","molecular descriptors","database compounds","entropy formulation","genetic algorithm","Available Chemicals Directory","cell-based partitioning","representative subset selection","property descriptor array"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"407","title":"Generating code at run time with Reflection.Emit","abstract":"The .NET framework SDK includes several tools that convert source code into executable code-the C# and VB.NET compilers get most of the attention, but there are others. The Regex class (in the System.Text.RegularExpressions namespace) has the ability to compile favorite regular expressions into a .NET assembly. In fact, the NET Common Language Runtime (CLR) contains a whole namespace full of classes to help us build assemblies, define types, and emit their implementations, all at run time. These classes, which comprise the System.Reflection.Emit namespace, are known collectively as Reflection. Emit","tok_text":"gener code at run time with reflect . emit \n the .net framework sdk includ sever tool that convert sourc code into execut code-th c # and vb.net compil get most of the attent , but there are other . the regex class ( in the system . text . regularexpress namespac ) ha the abil to compil favorit regular express into a .net assembl . in fact , the net common languag runtim ( clr ) contain a whole namespac full of class to help us build assembl , defin type , and emit their implement , all at run time . these class , which compris the system . reflect . emit namespac , are known collect as reflect . emit","ordered_present_kp":[49,203,324,454,538,28],"keyphrases":["Reflection.Emit",".NET framework SDK","Regex class","assemblies","types","System.Reflection.Emit namespace","runtime code generation",".NET Common Language Runtime"],"prmu":["P","P","P","P","P","P","R","R"]}
{"id":"2025","title":"Multispectral color image capture using a liquid crystal tunable filter","abstract":"We describe the experimental setup of a multispectral color image acquisition system consisting of a professional monochrome CCD camera and a tunable filter in which the spectral transmittance can be controlled electronically. We perform a spectral characterization of the acquisition system taking into account the acquisition noise. To convert the camera output signals to device-independent color data, two main approaches are proposed and evaluated. One consists in applying regression methods to convert from the K camera outputs to a device-independent color space such as CIEXYZ or CIELAB. Another method is based on a spectral model of the acquisition system. By inverting the model using a principal eigenvector approach, we estimate the spectral reflectance of each pixel of the imaged surface","tok_text":"multispectr color imag captur use a liquid crystal tunabl filter \n we describ the experiment setup of a multispectr color imag acquisit system consist of a profession monochrom ccd camera and a tunabl filter in which the spectral transmitt can be control electron . we perform a spectral character of the acquisit system take into account the acquisit nois . to convert the camera output signal to device-independ color data , two main approach are propos and evalu . one consist in appli regress method to convert from the k camera output to a device-independ color space such as ciexyz or cielab . anoth method is base on a spectral model of the acquisit system . by invert the model use a princip eigenvector approach , we estim the spectral reflect of each pixel of the imag surfac","ordered_present_kp":[0,36,104,167,51,221,279,127,343,374,398,489,374,581,591,626,692,736,774,761],"keyphrases":["multispectral color image capture","liquid crystal tunable filter","tunable filter","multispectral color image acquisition system","acquisition system","monochrome CCD camera","spectral transmittance","spectral characterization","acquisition noise","camera output signals","camera outputs","device-independent color data","regression methods","CIEXYZ","CIELAB","spectral model","principal eigenvector approach","spectral reflectance","pixel","imaged surface","independent color space"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"392","title":"Time-varying properties of renal autoregulatory mechanisms","abstract":"In order to assess the possible time-varying properties of renal autoregulation, time-frequency and time-scaling methods were applied to renal blood flow under broad-band forced arterial blood pressure fluctuations and single-nephron renal blood flow with spontaneous oscillations obtained from normotensive (Sprague-Dawley, Wistar, and Long-Evans) rats, and spontaneously hypertensive rats. Time-frequency analyses of normotensive and hypertensive blood flow data obtained from either the whole kidney or the single-nephron show that indeed both the myogenic and tubuloglomerular feedback (TGF) mechanisms have time-varying characteristics. Furthermore, we utilized the Renyi entropy to measure the complexity of blood-flow dynamics in the time-frequency plane in an effort to discern differences between normotensive and hypertensive recordings. We found a clear difference in Renyi entropy between normotensive and hypertensive blood flow recordings at the whole kidney level for both forced (p < 0.037) and spontaneous arterial pressure fluctuations (p < 0.033), and at the single-nephron level (p < 0.008). Especially at the single-nephron level, the mean Renyi entropy is significantly larger for hypertensive than normotensive rats, suggesting more complex dynamics in the hypertensive condition. To further evaluate whether or not the separation of dynamics between normotensive and hypertensive rats is found in the prescribed frequency ranges of the myogenic and TGF mechanisms, we employed multiresolution wavelet transform. Our analysis revealed that exclusively over scale ranges corresponding to the frequency intervals of the myogenic and TGF mechanisms, the widths of the blood flow wavelet coefficients fall into disjoint sets for normotensive and hypertensive rats. The separation of the scales at the myogenic and TGF frequency ranges is distinct and obtained with 100% accuracy. However, this observation remains valid only for the whole kidney blood pressure\/flow data. The results suggest that understanding of the time-varying properties of the two mechanisms is required for a complete description of renal autoregulation","tok_text":"time-vari properti of renal autoregulatori mechan \n in order to assess the possibl time-vari properti of renal autoregul , time-frequ and time-sc method were appli to renal blood flow under broad-band forc arteri blood pressur fluctuat and single-nephron renal blood flow with spontan oscil obtain from normotens ( sprague-dawley , wistar , and long-evan ) rat , and spontan hypertens rat . time-frequ analys of normotens and hypertens blood flow data obtain from either the whole kidney or the single-nephron show that inde both the myogen and tubuloglomerular feedback ( tgf ) mechan have time-vari characterist . furthermor , we util the renyi entropi to measur the complex of blood-flow dynam in the time-frequ plane in an effort to discern differ between normotens and hypertens record . we found a clear differ in renyi entropi between normotens and hypertens blood flow record at the whole kidney level for both forc ( p < 0.037 ) and spontan arteri pressur fluctuat ( p < 0.033 ) , and at the single-nephron level ( p < 0.008 ) . especi at the single-nephron level , the mean renyi entropi is significantli larger for hypertens than normotens rat , suggest more complex dynam in the hypertens condit . to further evalu whether or not the separ of dynam between normotens and hypertens rat is found in the prescrib frequenc rang of the myogen and tgf mechan , we employ multiresolut wavelet transform . our analysi reveal that exclus over scale rang correspond to the frequenc interv of the myogen and tgf mechan , the width of the blood flow wavelet coeffici fall into disjoint set for normotens and hypertens rat . the separ of the scale at the myogen and tgf frequenc rang is distinct and obtain with 100 % accuraci . howev , thi observ remain valid onli for the whole kidney blood pressur \/ flow data . the result suggest that understand of the time-vari properti of the two mechan is requir for a complet descript of renal autoregul","ordered_present_kp":[0,475,240,641,942,375,1141,22,190,240,277],"keyphrases":["time-varying properties","renal autoregulatory mechanisms","broad-band forced arterial blood pressure fluctuations","single-nephron","single-nephron renal blood flow","spontaneous oscillations","hypertensive rats","whole kidney","Renyi entropy","spontaneous arterial pressure fluctuations","normotensive rats","Sprague-Dawley rats","Wistar rats","Long-Evans rats"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"2060","title":"Decisions, decisions, decisions: a tale of special collections in the small academic library","abstract":"A case study of a special collections department in a small academic library and how its collections have been acquired and developed over the years is described. It looks at the changes that have occurred in the academic environment and what effect, if any, these changes may have had on the department and how it has adapted to them. It raises questions about development and acquisitions policies and procedures","tok_text":"decis , decis , decis : a tale of special collect in the small academ librari \n a case studi of a special collect depart in a small academ librari and how it collect have been acquir and develop over the year is describ . it look at the chang that have occur in the academ environ and what effect , if ani , these chang may have had on the depart and how it ha adapt to them . it rais question about develop and acquisit polici and procedur","ordered_present_kp":[34,57,82,412],"keyphrases":["special collections","small academic library","case study","acquisitions policies","out-of-print books","University library"],"prmu":["P","P","P","P","U","M"]}
{"id":"2018","title":"Design and implementation of a 3-D mapping system for highly irregular shaped objects with application to semiconductor manufacturing","abstract":"The basic technology for a robotic system is developed to automate the packing of polycrystalline silicon nuggets into fragile fused silica crucible in Czochralski (melt pulling) semiconductor wafer production. The highly irregular shapes of the nuggets and the packing constraints make this a difficult and challenging task. It requires the delicate manipulation and packing of highly irregular polycrystalline silicon nuggets into a fragile fused silica crucible. For this application, a dual optical 3-D surface mapping system that uses active laser triangulation has been developed and successfully tested. One part of the system measures the geometry profile of a nugget being packed and the other the profile of the nuggets already in the crucible. A resolution of 1 mm with 15-KHz sampling frequency is achieved. Data from the system are used by the packing algorithm, which determines optimal nugget placement. The key contribution is to describe the design and implementation of an efficient and robust 3-D imaging system to map highly irregular shaped objects using conventional components in context of real commercial manufacturing processes","tok_text":"design and implement of a 3-d map system for highli irregular shape object with applic to semiconductor manufactur \n the basic technolog for a robot system is develop to autom the pack of polycrystallin silicon nugget into fragil fuse silica crucibl in czochralski ( melt pull ) semiconductor wafer product . the highli irregular shape of the nugget and the pack constraint make thi a difficult and challeng task . it requir the delic manipul and pack of highli irregular polycrystallin silicon nugget into a fragil fuse silica crucibl . for thi applic , a dual optic 3-d surfac map system that use activ laser triangul ha been develop and success test . one part of the system measur the geometri profil of a nugget be pack and the other the profil of the nugget alreadi in the crucibl . a resolut of 1 mm with 15-khz sampl frequenc is achiev . data from the system are use by the pack algorithm , which determin optim nugget placement . the key contribut is to describ the design and implement of an effici and robust 3-d imag system to map highli irregular shape object use convent compon in context of real commerci manufactur process","ordered_present_kp":[45,90,143,188,223,819,882,1013,52,1111,455,599],"keyphrases":["highly irregular shaped objects","irregular shaped objects","semiconductor manufacturing","robotic system","polycrystalline silicon nuggets","fragile fused silica crucible","highly irregular polycrystalline silicon nuggets","active laser triangulation","sampling frequency","packing algorithm","robust 3-D imaging system","commercial manufacturing processes","3D mapping system","optical nugget placement","Czochralski semiconductor wafer production","dual optical 3D surface mapping system"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","M","R","R","M"]}
{"id":"2","title":"Waiting for the wave to crest [wavelength services]","abstract":"Wavelength services have been hyped ad nauseam for years. But despite their quick turn-up time and impressive margins, such services have yet to live up to the industry's expectations. The reasons for this lukewarm reception are many, not the least of which is the confusion that still surrounds the technology, but most industry observers are still convinced that wavelength services with ultimately flourish","tok_text":"wait for the wave to crest [ wavelength servic ] \n wavelength servic have been hype ad nauseam for year . but despit their quick turn-up time and impress margin , such servic have yet to live up to the industri 's expect . the reason for thi lukewarm recept are mani , not the least of which is the confus that still surround the technolog , but most industri observ are still convinc that wavelength servic with ultim flourish","ordered_present_kp":[29],"keyphrases":["wavelength services","fiber optic networks","Looking Glass Networks","PointEast Research"],"prmu":["P","U","U","U"]}
{"id":"352","title":"An efficient retrieval selection algorithm for video servers with random duplicated assignment storage technique","abstract":"Random duplicated assignment (RDA) is an approach in which video data is stored by assigning a number of copies of each data block to different, randomly chosen disks. It has been shown that this approach results in smaller response times and lower disk and RAM costs compared to the well-known disk stripping techniques. Based on this storage approach, one has to determine, for each given batch of data blocks, from which disk each of the data blocks is to be retrieved. This is to be done in such a way that the maximum load of the disks is minimized. The problem is called the retrieval selection problem (RSP). In this paper, we propose a new efficient algorithm for RSP. This algorithm is based on the breadth-first search approach and is able to guarantee optimal solutions for RSP in O(n\/sup 2\/+mn), where m and n correspond to the number of data blocks and the number of disks, respectively. We show that our proposed algorithm has a lower time complexity than an existing algorithm, called the MFS algorithm","tok_text":"an effici retriev select algorithm for video server with random duplic assign storag techniqu \n random duplic assign ( rda ) is an approach in which video data is store by assign a number of copi of each data block to differ , randomli chosen disk . it ha been shown that thi approach result in smaller respons time and lower disk and ram cost compar to the well-known disk strip techniqu . base on thi storag approach , one ha to determin , for each given batch of data block , from which disk each of the data block is to be retriev . thi is to be done in such a way that the maximum load of the disk is minim . the problem is call the retriev select problem ( rsp ) . in thi paper , we propos a new effici algorithm for rsp . thi algorithm is base on the breadth-first search approach and is abl to guarante optim solut for rsp in o(n \/ sup 2\/+mn ) , where m and n correspond to the number of data block and the number of disk , respect . we show that our propos algorithm ha a lower time complex than an exist algorithm , call the mf algorithm","ordered_present_kp":[3,39,57,191,204,227,303,335,578,758,811,987],"keyphrases":["efficient retrieval selection algorithm","video servers","random duplicated assignment storage technique","copies","data block","randomly chosen disks","response times","RAM costs","maximum load","breadth-first search","optimal solutions","time complexity","disk costs"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"317","title":"Relevance of Web documents: ghosts consensus method","abstract":"The dominant method currently used to improve the quality of Internet search systems is often called \"digital democracy.\" Such an approach implies the utilization of the majority opinion of Internet users to determine the most relevant documents: for example, citation index usage for sorting of search results (google.com) or an enrichment of a query with terms that are asked frequently in relation with the query's theme. \"Digital democracy\" is an effective instrument in many cases, but it has an unavoidable shortcoming, which is a matter of principle: the average intellectual and cultural level of Internet users is very low; everyone knows what kind of information is dominant in Internet query statistics. Therefore, when one searches the Internet by means of \"digital democracy\" systems, one gets answers that reflect an underlying assumption that the user's mind potential is very low, and that his cultural interests are not demanding. Thus, it is more correct to use the term \"digital ochlocracy\" to refer to Internet search systems with \"digital democracy.\" Based on the well-known mathematical mechanism of linear programming, we propose a method to solve the indicated problem","tok_text":"relev of web document : ghost consensu method \n the domin method current use to improv the qualiti of internet search system is often call \" digit democraci . \" such an approach impli the util of the major opinion of internet user to determin the most relev document : for exampl , citat index usag for sort of search result ( google.com ) or an enrich of a queri with term that are ask frequent in relat with the queri 's theme . \" digit democraci \" is an effect instrument in mani case , but it ha an unavoid shortcom , which is a matter of principl : the averag intellectu and cultur level of internet user is veri low ; everyon know what kind of inform is domin in internet queri statist . therefor , when one search the internet by mean of \" digit democraci \" system , one get answer that reflect an underli assumpt that the user 's mind potenti is veri low , and that hi cultur interest are not demand . thu , it is more correct to use the term \" digit ochlocraci \" to refer to internet search system with \" digit democraci . \" base on the well-known mathemat mechan of linear program , we propos a method to solv the indic problem","ordered_present_kp":[102,141,200,282,311,669,953,1076,24],"keyphrases":["ghosts consensus method","Internet search systems","digital democracy","majority opinion","citation index usage","search results","Internet query statistics","digital ochlocracy","linear programming","World Wide Web"],"prmu":["P","P","P","P","P","P","P","P","P","M"]}
{"id":"1974","title":"Real-time implementation of a new low-memory SPIHT image coding algorithm using DSP chip","abstract":"Among all algorithms based on wavelet transform and zerotree quantization, Said and Pearlman's (1996) set partitioning in hierarchical trees (SPIHT) algorithm is well-known for its simplicity and efficiency. This paper deals with the real-time implementation of SPIHT algorithm using DSP chip. In order to facilitate the implementation and improve the codec's performance, some relative issues are thoroughly discussed, such as the optimization of program structure to speed up the wavelet decomposition. SPIHT's high memory requirement is a major drawback for hardware implementation. In this paper, we modify the original SPIHT algorithm by presenting two new concepts-number of error bits and absolute zerotree. Consequently, the memory cost is significantly reduced. We also introduce a new method to control the coding process by number of error bits. Our experimental results show that the implementation meets common requirement of real-time video coding and is proven to be a practical and efficient DSP solution","tok_text":"real-tim implement of a new low-memori spiht imag code algorithm use dsp chip \n among all algorithm base on wavelet transform and zerotre quantiz , said and pearlman 's ( 1996 ) set partit in hierarch tree ( spiht ) algorithm is well-known for it simplic and effici . thi paper deal with the real-tim implement of spiht algorithm use dsp chip . in order to facilit the implement and improv the codec 's perform , some rel issu are thoroughli discuss , such as the optim of program structur to speed up the wavelet decomposit . spiht 's high memori requir is a major drawback for hardwar implement . in thi paper , we modifi the origin spiht algorithm by present two new concepts-numb of error bit and absolut zerotre . consequ , the memori cost is significantli reduc . we also introduc a new method to control the code process by number of error bit . our experiment result show that the implement meet common requir of real-tim video code and is proven to be a practic and effici dsp solut","ordered_present_kp":[314,0,108,130,394,506,831,701,69,178,930],"keyphrases":["real-time implementation","DSP chip","wavelet transform","zerotree quantization","set partitioning in hierarchical trees","SPIHT algorithm","codec","wavelet decomposition","absolute zerotree","number of error bits","video coding","memory cost reduction"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"1931","title":"Mathematical fundamentals of constructing fuzzy Bayesian inference techniques","abstract":"Problems and an associated technique for developing a Bayesian approach to decision-making in the case of fuzzy data are presented. The concept of fuzzy and pseudofuzzy quantities is introduced and main operations with pseudofuzzy quantities are considered. The basic relationships and the principal concepts of the Bayesian decision procedure based on the modus-ponens rule are proposed. Some problems concerned with the practical realization of the fuzzy Bayesian method are considered","tok_text":"mathemat fundament of construct fuzzi bayesian infer techniqu \n problem and an associ techniqu for develop a bayesian approach to decision-mak in the case of fuzzi data are present . the concept of fuzzi and pseudofuzzi quantiti is introduc and main oper with pseudofuzzi quantiti are consid . the basic relationship and the princip concept of the bayesian decis procedur base on the modus-ponen rule are propos . some problem concern with the practic realiz of the fuzzi bayesian method are consid","ordered_present_kp":[0,32,208,384],"keyphrases":["mathematical fundamentals","fuzzy Bayesian inference techniques","pseudofuzzy quantities","modus-ponens rule","decision making"],"prmu":["P","P","P","P","M"]}
{"id":"2140","title":"Strong active solution in non-cooperative games","abstract":"For the non-cooperative games and the problems of accepting or rejecting a proposal, a new notion of equilibrium was proposed, its place among the known basic equilibria was established, and its application to the static and dynamic game problems was demonstrated","tok_text":"strong activ solut in non-coop game \n for the non-coop game and the problem of accept or reject a propos , a new notion of equilibrium wa propos , it place among the known basic equilibria wa establish , and it applic to the static and dynam game problem wa demonstr","ordered_present_kp":[0,236],"keyphrases":["strong active solution","dynamic game problems","noncooperative games","static game problems"],"prmu":["P","P","M","R"]}
{"id":"2105","title":"Collective action in the age of the Internet: mass communication and online mobilization","abstract":"This article examines how the Internet transforms collective action. Current practices on the Web bear witness to thriving collective action ranging from persuasive to confrontational, individual to collective, undertakings. Even more influential than direct calls for action is the indirect mobilizing influence of the Internet's powers of mass communication, which is boosted by an antiauthoritarian ideology on the Web. Theoretically, collective action through the otherwise socially isolating computer is possible because people rely on internalized group memberships and social identities to achieve social involvement. Empirical evidence from an online survey among environmental activists and nonactivists confirms that online action is considered an equivalent alternative to offline action by activists and nonactivists alike. However, the Internet may slightly alter the motives underlying collective action and thereby alter the nature of collective action and social movements. Perhaps more fundamental is the reverse influence that successful collective action will have on the nature and function of the Internet","tok_text":"collect action in the age of the internet : mass commun and onlin mobil \n thi articl examin how the internet transform collect action . current practic on the web bear wit to thrive collect action rang from persuas to confront , individu to collect , undertak . even more influenti than direct call for action is the indirect mobil influenc of the internet 's power of mass commun , which is boost by an antiauthoritarian ideolog on the web . theoret , collect action through the otherwis social isol comput is possibl becaus peopl reli on intern group membership and social ident to achiev social involv . empir evid from an onlin survey among environment activist and nonactivist confirm that onlin action is consid an equival altern to offlin action by activist and nonactivist alik . howev , the internet may slightli alter the motiv underli collect action and therebi alter the natur of collect action and social movement . perhap more fundament is the revers influenc that success collect action will have on the natur and function of the internet","ordered_present_kp":[33,44,60,0,404,547,568,626],"keyphrases":["collective action","Internet","mass communication","online mobilization","antiauthoritarian ideology","group memberships","social identities","online survey","World Wide Web","anonymity","politics"],"prmu":["P","P","P","P","P","P","P","P","M","U","U"]}
{"id":"1989","title":"Managing system risk","abstract":"Companies are increasingly required to provide assurance that their systems are secure and conform to commercial security standards. Senior business managers are ultimately responsible for the security of their corporate systems and for the implications in the event of a failure. Businesses will be exposed to unquantified security risks unless they have a formal risk management framework in place to enable risks to be identified, evaluated and managed. Failure to assess and manage risks can lead to a business suffering serious financial impacts, commercial embarrassment and fines or sanctions from regulators. This is both a key responsibility and opportunity for Management Services Practitioners","tok_text":"manag system risk \n compani are increasingli requir to provid assur that their system are secur and conform to commerci secur standard . senior busi manag are ultim respons for the secur of their corpor system and for the implic in the event of a failur . busi will be expos to unquantifi secur risk unless they have a formal risk manag framework in place to enabl risk to be identifi , evalu and manag . failur to assess and manag risk can lead to a busi suffer seriou financi impact , commerci embarrass and fine or sanction from regul . thi is both a key respons and opportun for manag servic practition","ordered_present_kp":[326,111],"keyphrases":["commercial security standards","risk management framework","IT projects"],"prmu":["P","P","U"]}
{"id":"350","title":"There is no optimal routing policy for the torus","abstract":"A routing policy is the method used to select a specific output channel for a message from among a number of acceptable output channels. An optimal routing policy is a policy that maximizes the probability of a message reaching its destination without delays. Optimal routing policies have been proposed for several regular networks, including the mesh and the hypercube. An open problem in interconnection network research has been the identification of an optimal routing policy for the torus. In this paper, we show that there is no optimal routing policy for the torus. Our result is demonstrated by presenting a detailed example in which the best choice of output channel is dependent on the probability of each channel being available. This result settles, in the negative, a conjecture by J. Wu (1996) concerning an optimal routing policy for the torus","tok_text":"there is no optim rout polici for the toru \n a rout polici is the method use to select a specif output channel for a messag from among a number of accept output channel . an optim rout polici is a polici that maxim the probabl of a messag reach it destin without delay . optim rout polici have been propos for sever regular network , includ the mesh and the hypercub . an open problem in interconnect network research ha been the identif of an optim rout polici for the toru . in thi paper , we show that there is no optim rout polici for the toru . our result is demonstr by present a detail exampl in which the best choic of output channel is depend on the probabl of each channel be avail . thi result settl , in the neg , a conjectur by j. wu ( 1996 ) concern an optim rout polici for the toru","ordered_present_kp":[12,38,358],"keyphrases":["optimal routing policy","torus","hypercube"],"prmu":["P","P","P"]}
{"id":"315","title":"The impact of the Internet on public library use: an analysis of the current consumer market for library and Internet services","abstract":"The potential impact of the Internet on the public's demand for the services and resources of public libraries is an issue of critical importance. The research reported in this article provides baseline data concerning the evolving relationship between the public's use of the library and its use of the Internet. The authors developed a consumer model of the American adult market for information services and resources, segmented by use (or nonuse) of the public library and by access (or lack of access) to, and use (or nonuse) of, the Internet. A national Random Digit Dialing telephone survey collected data to estimate the size of each of six market segments, and to describe their usage choices between the public library and the Internet. The analyses presented in this article provide estimates of the size and demographics of each of the market segments; describe why people are currently using the public library and the Internet; identify the decision criteria people use in their choices of which provider to use; identify areas in which libraries and the Internet appear to be competing and areas in which they appear to be complementary; and identify reasons why people choose not to use the public library and\/or the Internet. The data suggest that some differentiation between the library and the Internet is taking place, which may very well have an impact on consumer choices between the two. Longitudinal research is necessary to fully reveal trends in these usage choices, which have implications for all types of libraries in planning and policy development","tok_text":"the impact of the internet on public librari use : an analysi of the current consum market for librari and internet servic \n the potenti impact of the internet on the public 's demand for the servic and resourc of public librari is an issu of critic import . the research report in thi articl provid baselin data concern the evolv relationship between the public 's use of the librari and it use of the internet . the author develop a consum model of the american adult market for inform servic and resourc , segment by use ( or nonus ) of the public librari and by access ( or lack of access ) to , and use ( or nonus ) of , the internet . a nation random digit dial telephon survey collect data to estim the size of each of six market segment , and to describ their usag choic between the public librari and the internet . the analys present in thi articl provid estim of the size and demograph of each of the market segment ; describ whi peopl are current use the public librari and the internet ; identifi the decis criteria peopl use in their choic of which provid to use ; identifi area in which librari and the internet appear to be compet and area in which they appear to be complementari ; and identifi reason whi peopl choos not to use the public librari and\/or the internet . the data suggest that some differenti between the librari and the internet is take place , which may veri well have an impact on consum choic between the two . longitudin research is necessari to fulli reveal trend in these usag choic , which have implic for all type of librari in plan and polici develop","ordered_present_kp":[18,30,300,435,455,643,1014,30,1447],"keyphrases":["Internet","public libraries","public libraries","baseline data","consumer model","American adult market","national Random Digit Dialing telephone survey","decision criteria","longitudinal research","public library"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"2142","title":"Spectral characteristics of the linear systems over a bounded time interval","abstract":"Consideration was given to the spectral characteristics of the linear dynamic systems over a bounded time interval. Singular characteristics of standard dynamic blocks, transcendental characteristic equations, and partial spectra of the singular functions were studied. Relationship between the spectra under study and the classical frequency characteristic was demonstrated","tok_text":"spectral characterist of the linear system over a bound time interv \n consider wa given to the spectral characterist of the linear dynam system over a bound time interv . singular characterist of standard dynam block , transcendent characterist equat , and partial spectra of the singular function were studi . relationship between the spectra under studi and the classic frequenc characterist wa demonstr","ordered_present_kp":[0,50,124,171,196,219,257,280,372],"keyphrases":["spectral characteristics","bounded time interval","linear dynamic systems","singular characteristics","standard dynamic blocks","transcendental characteristic equations","partial spectra","singular functions","frequency characteristic"],"prmu":["P","P","P","P","P","P","P","P","P"]}
{"id":"2107","title":"The effects of asynchronous computer-mediated group interaction on group processes","abstract":"This article reports a study undertaken to investigate some of the social psychological processes underlying computer-supported group discussion in natural computer-mediated contexts. Based on the concept of deindividuation, it was hypothesized that personal identifiability and group identity would be important factors that affect the perceptions and behavior of members of computer-mediated groups. The degree of personal identifiability and the strength of group identity were manipulated across groups of geographically dispersed computer users who took part in e-mail discussions during a 2-week period. The results do not support the association between deindividuation and uninhibited behavior cited in much previous research. Instead, the data provide some support for a social identity perspective of computer-mediated communication, which explains the higher levels uninhibited in identifiable computer-mediated groups. However, predictions based on social identity theory regarding group polarization and group cohesion were not supported. Possible explanations for this are discussed and further research is suggested to resolve these discrepancies","tok_text":"the effect of asynchron computer-medi group interact on group process \n thi articl report a studi undertaken to investig some of the social psycholog process underli computer-support group discuss in natur computer-medi context . base on the concept of deindividu , it wa hypothes that person identifi and group ident would be import factor that affect the percept and behavior of member of computer-medi group . the degre of person identifi and the strength of group ident were manipul across group of geograph dispers comput user who took part in e-mail discuss dure a 2-week period . the result do not support the associ between deindividu and uninhibit behavior cite in much previou research . instead , the data provid some support for a social ident perspect of computer-medi commun , which explain the higher level uninhibit in identifi computer-medi group . howev , predict base on social ident theori regard group polar and group cohes were not support . possibl explan for thi are discuss and further research is suggest to resolv these discrep","ordered_present_kp":[14,56,140,253,286,306,503,549,890,917,933],"keyphrases":["asynchronous computer-mediated group interaction","group processes","psychology","deindividuation","personal identifiability","group identity","geographically dispersed computer users","e-mail discussions","social identity theory","group polarization","group cohesion","social issues","Internet"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","M","U"]}
{"id":"208","title":"Designing a new urban Internet","abstract":"The parallel between designing a Web site and the construction of a building is a familiar one, but how often do we think of the Internet as having parks and streets? It would be absurd to say that the Internet could ever take the place of real, livable communities; however, it is safe to say that the context for using the Internet is on a path of change. As the Internet evolves beyond a simple linkage of disparate Web sites and applications, the challenge for Information Architects is establishing a process by which to structure, organize, and design networked environments. The principles that guide New Urbanism can offer much insight into networked electronic environment design. At the core of every New Urbanism principle is the idea of \"wholeness\"-of making sure that neighborhoods and communities are knit together in a way that supports civic activities, economic development, efficient ecosystems, aesthetic beauty, and human interaction","tok_text":"design a new urban internet \n the parallel between design a web site and the construct of a build is a familiar one , but how often do we think of the internet as have park and street ? it would be absurd to say that the internet could ever take the place of real , livabl commun ; howev , it is safe to say that the context for use the internet is on a path of chang . as the internet evolv beyond a simpl linkag of dispar web site and applic , the challeng for inform architect is establish a process by which to structur , organ , and design network environ . the principl that guid new urban can offer much insight into network electron environ design . at the core of everi new urban principl is the idea of \" wholeness\"-of make sure that neighborhood and commun are knit togeth in a way that support civic activ , econom develop , effici ecosystem , aesthet beauti , and human interact","ordered_present_kp":[60,19,463,545,624,273],"keyphrases":["Internet","Web site","communities","information architects","networked environments","networked electronic environment design","private-public sector cooperation","global information networks"],"prmu":["P","P","P","P","P","P","U","M"]}
{"id":"1976","title":"Adaptive image denoising using scale and space consistency","abstract":"This paper proposes a new method for image denoising with edge preservation, based on image multiresolution decomposition by a redundant wavelet transform. In our approach, edges are implicitly located and preserved in the wavelet domain, whilst image noise is filtered out. At each resolution level, the image edges are estimated by gradient magnitudes (obtained from the wavelet coefficients), which are modeled probabilistically, and a shrinkage function is assembled based on the model obtained. Joint use of space and scale consistency is applied for better preservation of edges. The shrinkage functions are combined to preserve edges that appear simultaneously at several resolutions, and geometric constraints are applied to preserve edges that are not isolated. The proposed technique produces a filtered version of the original image, where homogeneous regions appear separated by well-defined edges. Possible applications include image presegmentation, and image denoising","tok_text":"adapt imag denois use scale and space consist \n thi paper propos a new method for imag denois with edg preserv , base on imag multiresolut decomposit by a redund wavelet transform . in our approach , edg are implicitli locat and preserv in the wavelet domain , whilst imag nois is filter out . at each resolut level , the imag edg are estim by gradient magnitud ( obtain from the wavelet coeffici ) , which are model probabilist , and a shrinkag function is assembl base on the model obtain . joint use of space and scale consist is appli for better preserv of edg . the shrinkag function are combin to preserv edg that appear simultan at sever resolut , and geometr constraint are appli to preserv edg that are not isol . the propos techniqu produc a filter version of the origin imag , where homogen region appear separ by well-defin edg . possibl applic includ imag presegment , and imag denois","ordered_present_kp":[0,516,32,99,121,155,322,344,437,659],"keyphrases":["adaptive image denoising","space consistency","edge preservation","image multiresolution decomposition","redundant wavelet transform","image edges","gradient magnitudes","shrinkage function","scale consistency","geometric constraints","edge enhancement"],"prmu":["P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"1933","title":"Accelerated simulation of the steady-state availability of non-Markovian systems","abstract":"A general accelerated simulation method for evaluation of the steady-state availability of non-Markovian systems is proposed. It is applied to the investigation of a class of systems with repair. Numerical examples are given","tok_text":"acceler simul of the steady-st avail of non-markovian system \n a gener acceler simul method for evalu of the steady-st avail of non-markovian system is propos . it is appli to the investig of a class of system with repair . numer exampl are given","ordered_present_kp":[0,21,40,65,224],"keyphrases":["accelerated simulation","steady-state availability","non-Markovian systems","general accelerated simulation method","numerical examples"],"prmu":["P","P","P","P","P"]}
{"id":"2182","title":"Organization design: The continuing influence of information technology","abstract":"Drawing from an information processing perspective, this paper examines how information technology (IT) has been a catalyst in the development of new forms of organizational structures. The article draws a historical linkage between the relative stability of an organization's task environment starting after the Second World War to the present environmental instability that now characterizes many industries. Specifically, the authors suggest that advances in IT have enabled managers to adapt existing forms and create new models for organizational design that better fit requirements of an unstable environment. Time has seemingly borne out this hypothesis as the bureaucratic structure evolved to the matrix to the network and now to the emerging shadow structure. IT has gone from a support mechanism to a substitute for organizational structures in the form of the shadow structure. The article suggests that the evolving and expanding role of IT will continue for organizations that face unstable environments","tok_text":"organ design : the continu influenc of inform technolog \n draw from an inform process perspect , thi paper examin how inform technolog ( it ) ha been a catalyst in the develop of new form of organiz structur . the articl draw a histor linkag between the rel stabil of an organ 's task environ start after the second world war to the present environment instabl that now character mani industri . specif , the author suggest that advanc in it have enabl manag to adapt exist form and creat new model for organiz design that better fit requir of an unstabl environ . time ha seemingli born out thi hypothesi as the bureaucrat structur evolv to the matrix to the network and now to the emerg shadow structur . it ha gone from a support mechan to a substitut for organiz structur in the form of the shadow structur . the articl suggest that the evolv and expand role of it will continu for organ that face unstabl environ","ordered_present_kp":[0,71,191,341,39],"keyphrases":["organization design","information technology","information processing perspective","organizational structures","environmental instability","organization task environment"],"prmu":["P","P","P","P","P","R"]}
{"id":"235","title":"The role of CAUL (Council of Australian Libraries) in consortial purchasing","abstract":"The Council of Australian University Librarians, constituted in 1965 for the purposes of cooperative action and the sharing of information, assumed the role of consortial purchasing agent in 1996 on behalf of its members and associate organisations in Australia and New Zealand. This role continues to grow in tandem with the burgeoning of electronic publication and the acceptance of publishers of the advantages of dealing with consortia. The needs of the Australian university community overlap significantly with consortia in North America and Europe, but important differences are highlighted","tok_text":"the role of caul ( council of australian librari ) in consorti purchas \n the council of australian univers librarian , constitut in 1965 for the purpos of cooper action and the share of inform , assum the role of consorti purchas agent in 1996 on behalf of it member and associ organis in australia and new zealand . thi role continu to grow in tandem with the burgeon of electron public and the accept of publish of the advantag of deal with consortia . the need of the australian univers commun overlap significantli with consortia in north america and europ , but import differ are highlight","ordered_present_kp":[77,155,54,30,303,372,537,555],"keyphrases":["Australia","consortial purchasing","Council of Australian University Librarians","cooperative action","New Zealand","electronic publication","North America","Europe","information sharing"],"prmu":["P","P","P","P","P","P","P","P","R"]}
{"id":"270","title":"Using molecular equivalence numbers to visually explore structural features that distinguish chemical libraries","abstract":"A molecular equivalence number (meqnum) classifies a molecule with respect to a class of structural features or topological shapes such as its cyclic system or its set of functional groups. Meqnums can be used to organize molecular structures into nonoverlapping, yet highly relatable classes. We illustrate the construction of some different types of meqnums and present via examples some methods of comparing diverse chemical libraries based on meqnums. In the examples we compare a library which is a random sample from the MDL Drug Data Report (MDDR) with a library which is a random sample from the Available Chemical Directory (ACD). In our analyses, we discover some interesting features of the topological shape of a molecule and its set of functional groups that are strongly linked with compounds occurring in the MDDR but not in the ACD. We also illustrate the utility of molecular equivalence indices in delineating the structural domain over which an SAR conclusion is valid","tok_text":"use molecular equival number to visual explor structur featur that distinguish chemic librari \n a molecular equival number ( meqnum ) classifi a molecul with respect to a class of structur featur or topolog shape such as it cyclic system or it set of function group . meqnum can be use to organ molecular structur into nonoverlap , yet highli relat class . we illustr the construct of some differ type of meqnum and present via exampl some method of compar divers chemic librari base on meqnum . in the exampl we compar a librari which is a random sampl from the mdl drug data report ( mddr ) with a librari which is a random sampl from the avail chemic directori ( acd ) . in our analys , we discov some interest featur of the topolog shape of a molecul and it set of function group that are strongli link with compound occur in the mddr but not in the acd . we also illustr the util of molecular equival indic in delin the structur domain over which an sar conclus is valid","ordered_present_kp":[4,46,199,224,251,79,563,641,888],"keyphrases":["molecular equivalence number","structural features","chemical libraries","topological shapes","cyclic system","functional groups","MDL Drug Data Report","Available Chemical Directory","molecular equivalence indices","molecule classification","nonoverlapping relatable classes"],"prmu":["P","P","P","P","P","P","P","P","P","M","R"]}
{"id":"23","title":"Absorption of long waves by nonresonant parametric microstructures","abstract":"Using simple acoustical and mechanical models, we consider the conceptual possibility of designing an active absorbing (nonreflecting) coating in the form of a thin layer with small-scale stratification and fast time modulation of parameters. Algorithms for space-time modulation of the controlled-layer structure are studied in detail for a one-dimensional boundary-value problem. These algorithms do not require wave-field measurements, which eliminates the self-excitation problem that is characteristic of active systems. The majority of the considered algorithms of parametric control transform the low-frequency incident wave to high-frequency waves of the technological band for which the waveguiding medium inside the layer is assumed to be opaque (absorbing). The efficient use conditions are found for all the algorithms. It is shown that the absorbing layer can be as thin as desired with respect to the minimum spatial scale of the incident wave and ensures efficient absorption in a wide frequency interval (starting from zero frequency) that is bounded from above only by a finite space-time resolution of the parameter-control operations. The structure of a three-dimensional parametric \"'black\" coating whose efficiency is independent of the angle of incidence of an incoming wave is developed on the basis of the studied one-dimensional problems. The general solution of the problem of diffraction of incident waves from such a coating is obtained. This solution is analyzed in detail for the case of a disk-shaped element","tok_text":"absorpt of long wave by nonreson parametr microstructur \n use simpl acoust and mechan model , we consid the conceptu possibl of design an activ absorb ( nonreflect ) coat in the form of a thin layer with small-scal stratif and fast time modul of paramet . algorithm for space-tim modul of the controlled-lay structur are studi in detail for a one-dimension boundary-valu problem . these algorithm do not requir wave-field measur , which elimin the self-excit problem that is characterist of activ system . the major of the consid algorithm of parametr control transform the low-frequ incid wave to high-frequ wave of the technolog band for which the waveguid medium insid the layer is assum to be opaqu ( absorb ) . the effici use condit are found for all the algorithm . it is shown that the absorb layer can be as thin as desir with respect to the minimum spatial scale of the incid wave and ensur effici absorpt in a wide frequenc interv ( start from zero frequenc ) that is bound from abov onli by a finit space-tim resolut of the parameter-control oper . the structur of a three-dimension parametr \" ' black \" coat whose effici is independ of the angl of incid of an incom wave is develop on the basi of the studi one-dimension problem . the gener solut of the problem of diffract of incid wave from such a coat is obtain . thi solut is analyz in detail for the case of a disk-shap element","ordered_present_kp":[79,188,204,227,270,293,343,543,574,598,650,793,1152,1219,1277,1377],"keyphrases":["mechanical models","thin layer","small-scale stratification","fast time modulation","space-time modulation","controlled-layer structure","one-dimensional boundary-value problem","parametric control","low-frequency incident wave","high-frequency waves","waveguiding medium","absorbing layer","angle of incidence","one-dimensional problems","diffraction","disk-shaped element","acoustical models","active absorbing coating","nonreflecting coating"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"2027","title":"Motion estimation using modified dynamic programming","abstract":"A new method for computing precise estimates of the motion vector field of moving objects in a sequence of images is proposed. Correspondence vector-field computation is formulated as a matching optimization problem for multiple dynamic images. The proposed method is a heuristic modification of dynamic programming applied to the 2-D optimization problem. Motion-vector-field estimates using real movie images demonstrate good performance of the algorithm in terms of dynamic motion analysis","tok_text":"motion estim use modifi dynam program \n a new method for comput precis estim of the motion vector field of move object in a sequenc of imag is propos . correspond vector-field comput is formul as a match optim problem for multipl dynam imag . the propos method is a heurist modif of dynam program appli to the 2-d optim problem . motion-vector-field estim use real movi imag demonstr good perform of the algorithm in term of dynam motion analysi","ordered_present_kp":[17,0,64,84,107,163,198,222,266,24,310,360,404,425],"keyphrases":["motion estimation","modified dynamic programming","dynamic programming","precise estimates","motion vector field","moving objects","vector-field computation","matching optimization problem","multiple dynamic images","heuristic modification","2-D optimization problem","real movie images","algorithm","dynamic motion analysis","image sequence","motion vector field estimates"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"390","title":"Automated breath detection on long-duration signals using feedforward backpropagation artificial neural networks","abstract":"A new breath-detection algorithm is presented, intended to automate the analysis of respiratory data acquired during sleep. The algorithm is based on two independent artificial neural networks (ANN\/sub insp\/ and ANN\/sub expi\/) that recognize, in the original signal, windows of interest where the onset of inspiration and expiration occurs. Postprocessing consists in finding inside each of these windows of interest minimum and maximum corresponding to each inspiration and expiration. The ANN\/sub insp\/ and ANN\/sub expi\/ correctly determine respectively 98.0% and 98.7% of the desired windows, when compared with 29 820 inspirations and 29 819 expirations detected by a human expert, obtained from three entire-night recordings. Postprocessing allowed determination of inspiration and expiration onsets with a mean difference with respect to the same human expert of (mean +or- SD) 34 +or- 71 ms for inspiration and 5 +or- 46 ms for expiration. The method proved to be effective in detecting the onset of inspiration and expiration in full night continuous recordings. A comparison of five human experts performing the same classification task yielded that the automated algorithm was undifferentiable from these human experts, failing within the distribution of human expert results. Besides being applicable to adult respiratory volume data, the presented algorithm was also successfully applied to infant sleep data, consisting of uncalibrated rib cage and abdominal movement recordings. A comparison with two previously published algorithms for breath detection in respiratory volume signal shows that the presented algorithm has a higher specificity, while presenting similar or higher positive predictive values","tok_text":"autom breath detect on long-dur signal use feedforward backpropag artifici neural network \n a new breath-detect algorithm is present , intend to autom the analysi of respiratori data acquir dure sleep . the algorithm is base on two independ artifici neural network ( ann \/ sub insp\/ and ann \/ sub expi\/ ) that recogn , in the origin signal , window of interest where the onset of inspir and expir occur . postprocess consist in find insid each of these window of interest minimum and maximum correspond to each inspir and expir . the ann \/ sub insp\/ and ann \/ sub expi\/ correctli determin respect 98.0 % and 98.7 % of the desir window , when compar with 29 820 inspir and 29 819 expir detect by a human expert , obtain from three entire-night record . postprocess allow determin of inspir and expir onset with a mean differ with respect to the same human expert of ( mean + or- sd ) 34 + or- 71 ms for inspir and 5 + or- 46 ms for expir . the method prove to be effect in detect the onset of inspir and expir in full night continu record . a comparison of five human expert perform the same classif task yield that the autom algorithm wa undifferenti from these human expert , fail within the distribut of human expert result . besid be applic to adult respiratori volum data , the present algorithm wa also success appli to infant sleep data , consist of uncalibr rib cage and abdomin movement record . a comparison with two previous publish algorithm for breath detect in respiratori volum signal show that the present algorithm ha a higher specif , while present similar or higher posit predict valu","ordered_present_kp":[0,405,380,391,1119,697,730,1356,1378,1325,1247,23,43],"keyphrases":["automated breath detection","long-duration signals","feedforward backpropagation artificial neural networks","inspiration","expiration","postprocessing","human experts","entire-night recordings","automated algorithm","adult respiratory volume data","infant sleep data","uncalibrated rib cage","abdominal movement recordings","respiratory movements","34 ms","5 ms"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"2062","title":"Underground poetry, collecting poetry, and the librarian","abstract":"A powerful encounter with underground poetry and its important role in poetry, literature, and culture is discussed. The acquisitions difficulties encountered in the unique publishing world of underground poetry are introduced. Strategies for acquiring underground poetry for library collections are proposed, including total immersion and local focus, with accompanying action","tok_text":"underground poetri , collect poetri , and the librarian \n a power encount with underground poetri and it import role in poetri , literatur , and cultur is discuss . the acquisit difficulti encount in the uniqu publish world of underground poetri are introduc . strategi for acquir underground poetri for librari collect are propos , includ total immers and local focu , with accompani action","ordered_present_kp":[46,0,210,304,129,145],"keyphrases":["underground poetry","librarian","literature","culture","publishing","library collections","out-of-print books","special collections"],"prmu":["P","P","P","P","P","P","U","M"]}
{"id":"328","title":"Personality research on the Internet: a comparison of Web-based and traditional instruments in take-home and in-class settings","abstract":"Students, faculty, and researchers have become increasingly comfortable with the Internet, and many of them are interested in using the Web to collect data. Few published studies have investigated the differences between Web-based data and data collected with more traditional methods. In order to investigate these potential differences, two important factors were crossed in this study: whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent's choosing. The Visions of Morality scale (Shelton and McAdams, 1990) was used, and the participants were assigned to one of four conditions: in-class Web survey, in-class paper-and-pencil survey; take-home Web survey, and take-home paper-and-pencil survey. No significant differences in scores were found for any condition; however, response rates were affected by the type of survey administered, with the take-home Web-based instrument having the lowest response rate. Therefore, researchers need to be aware that different modes of administration may affect subject attrition and may, therefore, confound investigations of other independent variables","tok_text":"person research on the internet : a comparison of web-bas and tradit instrument in take-hom and in-class set \n student , faculti , and research have becom increasingli comfort with the internet , and mani of them are interest in use the web to collect data . few publish studi have investig the differ between web-bas data and data collect with more tradit method . in order to investig these potenti differ , two import factor were cross in thi studi : whether the data were collect on line or not and whether the data were collect in a group set at a fix time or individu at a time of the respond 's choos . the vision of moral scale ( shelton and mcadam , 1990 ) wa use , and the particip were assign to one of four condit : in-class web survey , in-class paper-and-pencil survey ; take-hom web survey , and take-hom paper-and-pencil survey . no signific differ in score were found for ani condit ; howev , respons rate were affect by the type of survey administ , with the take-hom web-bas instrument have the lowest respons rate . therefor , research need to be awar that differ mode of administr may affect subject attrit and may , therefor , confound investig of other independ variabl","ordered_present_kp":[0,23,986,327,614,728,750,785,811,910,1092,1113],"keyphrases":["personality research","Internet","data collection","Visions of Morality scale","in-class Web survey","in-class paper-and-pencil survey","take-home Web survey","take-home paper-and-pencil survey","response rates","Web-based instruments","administration","subject attrition"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"405","title":"Learning spatial relations using an inductive logic programming system","abstract":"The ability to learn spatial relations is a prerequisite for performing many relevant tasks such as those associated with motion, orientation, navigation, etc. This paper reports on using an Inductive Logic Programming (ILP) system for learning function-free Horn-clause descriptions of spatial knowledge. Its main contribution, however, is to show that an existing relation between two reference systems-the speaker-relative and the absolute-can be automatically learned by an ILP system, given the proper background knowledge and positive examples","tok_text":"learn spatial relat use an induct logic program system \n the abil to learn spatial relat is a prerequisit for perform mani relev task such as those associ with motion , orient , navig , etc . thi paper report on use an induct logic program ( ilp ) system for learn function-fre horn-claus descript of spatial knowledg . it main contribut , howev , is to show that an exist relat between two refer systems-th speaker-rel and the absolute-can be automat learn by an ilp system , given the proper background knowledg and posit exampl","ordered_present_kp":[27,6,265],"keyphrases":["spatial relations","inductive logic programming system","function-free Horn-clause descriptions","spatial relations learning"],"prmu":["P","P","P","R"]}
{"id":"1992","title":"Geometrically invariant watermarking using feature points","abstract":"This paper presents a new approach for watermarking of digital images providing robustness to geometrical distortions. The weaknesses of classical watermarking methods to geometrical distortions are outlined first. Geometrical distortions can be decomposed into two classes: global transformations such as rotations and translations and local transformations such as the StirMark attack. An overview of existing self-synchronizing schemes is then presented. Theses schemes can use periodical properties of the mark, invariant properties of transforms, template insertion, or information provided by the original image to counter geometrical distortions. Thereafter, a new class of watermarking schemes using the image content is presented. We propose an embedding and detection scheme where the mark is bound with a content descriptor defined by salient points. Three different types of feature points are studied and their robustness to geometrical transformations is evaluated to develop an enhanced detector. The embedding of the signature is done by extracting feature points of the image and performing a Delaunay tessellation on the set of points. The mark is embedded using a classical additive scheme inside each triangle of the tessellation. The detection is done using correlation properties on the different triangles. The performance of the presented scheme is evaluated after JPEG compression, geometrical attack and transformations. Results show that the fact that the scheme is robust to these different manipulations. Finally, in our concluding remarks, we analyze the different perspectives of such content-based watermarking scheme","tok_text":"geometr invari watermark use featur point \n thi paper present a new approach for watermark of digit imag provid robust to geometr distort . the weak of classic watermark method to geometr distort are outlin first . geometr distort can be decompos into two class : global transform such as rotat and translat and local transform such as the stirmark attack . an overview of exist self-synchron scheme is then present . these s scheme can use period properti of the mark , invari properti of transform , templat insert , or inform provid by the origin imag to counter geometr distort . thereaft , a new class of watermark scheme use the imag content is present . we propos an embed and detect scheme where the mark is bound with a content descriptor defin by salient point . three differ type of featur point are studi and their robust to geometr transform is evalu to develop an enhanc detector . the embed of the signatur is done by extract featur point of the imag and perform a delaunay tessel on the set of point . the mark is embed use a classic addit scheme insid each triangl of the tessel . the detect is done use correl properti on the differ triangl . the perform of the present scheme is evalu after jpeg compress , geometr attack and transform . result show that the fact that the scheme is robust to these differ manipul . final , in our conclud remark , we analyz the differ perspect of such content-bas watermark scheme","ordered_present_kp":[0,29,94,122,264,289,299,312,340,379,441,471,271,502,635,674,684,729,980,1050,1121,1210,1226],"keyphrases":["geometrically invariant watermarking","feature points","digital images","geometrical distortions","global transformations","transforms","rotations","translations","local transformations","StirMark attack","self-synchronizing schemes","periodical properties","invariant properties","template insertion","image content","embedding","detection scheme","content descriptor","Delaunay tessellation","additive scheme","correlation properties","JPEG compression","geometrical attack","feature extraction"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"211","title":"Pervasive computing goes to work: interfacing to the enterprise","abstract":"The paperless office is an idea whose time has come, and come, and come again. To see how pervasive computing applications might bring some substance to this dream, the author spoke recently with key managers and technologists at McKesson Corporation (San Francisco), a healthcare supplier, service, and technology company with US$50 billion in sales last year, and also at AvantGo (Hayward, Calif.), a provider of mobile infrastructure software and services. For the past several years, McKesson has used mobility middleware developed by AvantGo to deploy major supply chain applications with thousands of pervasive clients and multiple servers that replace existing paper-based tracking systems. According to McKesson's managers, their system greatly reduced errors and associated costs caused by redelivery or loss of valuable products, giving McKesson a solid return on its investment","tok_text":"pervas comput goe to work : interfac to the enterpris \n the paperless offic is an idea whose time ha come , and come , and come again . to see how pervas comput applic might bring some substanc to thi dream , the author spoke recent with key manag and technologist at mckesson corpor ( san francisco ) , a healthcar supplier , servic , and technolog compani with us$ 50 billion in sale last year , and also at avantgo ( hayward , calif. ) , a provid of mobil infrastructur softwar and servic . for the past sever year , mckesson ha use mobil middlewar develop by avantgo to deploy major suppli chain applic with thousand of pervas client and multipl server that replac exist paper-bas track system . accord to mckesson 's manag , their system greatli reduc error and associ cost caus by redeliveri or loss of valuabl product , give mckesson a solid return on it invest","ordered_present_kp":[60,624,642],"keyphrases":["paperless office","pervasive clients","multiple servers","mobile workers","enterprise resource planning","data warehousing"],"prmu":["P","P","P","M","M","U"]}
{"id":"254","title":"What you get is what you see [Web performance monitoring]","abstract":"To get the best possible performance from your Web infrastructure, you'll need a complete view. Don't neglect the big picture because you're too busy concentrating on details. The increasing complexity of Web sites and the content they provide has consequently increased the complexity of the infrastructure that supports them. But with some knowledge of networking, a handful of useful tools, and the insight that those tools provide, designing and operating for optimal performance and reliability is within your grasp","tok_text":"what you get is what you see [ web perform monitor ] \n to get the best possibl perform from your web infrastructur , you 'll need a complet view . do n't neglect the big pictur becaus you 're too busi concentr on detail . the increas complex of web site and the content they provid ha consequ increas the complex of the infrastructur that support them . but with some knowledg of network , a hand of use tool , and the insight that those tool provid , design and oper for optim perform and reliabl is within your grasp","ordered_present_kp":[31,245,97,380,490],"keyphrases":["Web performance","Web infrastructure","Web sites","networking","reliability"],"prmu":["P","P","P","P","P"]}
{"id":"349","title":"A self-adjusting quality of service control scheme","abstract":"We propose and analyze a self-adjusting Quality of Service (QoS) control scheme with the goal of optimizing the system reward as a result of servicing different priority clients with varying workload, QoS and reward\/penalty requirements. Our scheme is based on resource partitioning and designated \"degrade QoS areas\" such that system resources are partitioned into priority areas each of which is reserved specifically to serve only clients in a corresponding class with no QoS degradation, plus one \"degraded QoS area\" into which all clients can be admitted with QoS adjustment being applied only to the lowest priority clients. We show that the best partition is dictated by the workload and the reward\/penalty characteristics of clients in difference priority classes. The analysis results can be used by a QoS manager to optimize the system total reward dynamically in response to changing workloads at run time. We demonstrate the validity of our scheme by means of simulation and comparing the proposed QoS self-adjusting scheme with those that do not use resource partitioning or designated degraded QoS areas","tok_text":"a self-adjust qualiti of servic control scheme \n we propos and analyz a self-adjust qualiti of servic ( qo ) control scheme with the goal of optim the system reward as a result of servic differ prioriti client with vari workload , qo and reward \/ penalti requir . our scheme is base on resourc partit and design \" degrad qo area \" such that system resourc are partit into prioriti area each of which is reserv specif to serv onli client in a correspond class with no qo degrad , plu one \" degrad qo area \" into which all client can be admit with qo adjust be appli onli to the lowest prioriti client . we show that the best partit is dictat by the workload and the reward \/ penalti characterist of client in differ prioriti class . the analysi result can be use by a qo manag to optim the system total reward dynam in respons to chang workload at run time . we demonstr the valid of our scheme by mean of simul and compar the propos qo self-adjust scheme with those that do not use resourc partit or design degrad qo area","ordered_present_kp":[2,194,286,905],"keyphrases":["self-adjusting quality of service control scheme","priority clients","resource partitioning","simulation","multimedia systems","performance evaluation","resource reservation"],"prmu":["P","P","P","P","M","U","R"]}
{"id":"2003","title":"Nonlinear modeling and adaptive fuzzy control of MCFC stack","abstract":"To improve availability and performance of fuel cells, the operating temperature of the molten carbonate fuel cells (MCFC) stack should be controlled within a specified range. However, most existing models of MCFC are not ready to be applied in synthesis. In the paper, a radial basis function neural networks identification model of a MCFC stack is developed based on the input-output sampled data. An adaptive fuzzy control procedure for the temperature of the MCFC stack is also developed. The parameters of the fuzzy control system are regulated by back-propagation algorithm, and the rule database of the fuzzy system is also adaptively adjusted by the nearest-neighbor-clustering algorithm. Finally using the neural networks model of MCFC stack, the simulation results of the control algorithm are presented. The results show the effectiveness of the proposed modeling and design procedures for the MCFC stack based on neural networks identification and the novel adaptive fuzzy control","tok_text":"nonlinear model and adapt fuzzi control of mcfc stack \n to improv avail and perform of fuel cell , the oper temperatur of the molten carbon fuel cell ( mcfc ) stack should be control within a specifi rang . howev , most exist model of mcfc are not readi to be appli in synthesi . in the paper , a radial basi function neural network identif model of a mcfc stack is develop base on the input-output sampl data . an adapt fuzzi control procedur for the temperatur of the mcfc stack is also develop . the paramet of the fuzzi control system are regul by back-propag algorithm , and the rule databas of the fuzzi system is also adapt adjust by the nearest-neighbor-clust algorithm . final use the neural network model of mcfc stack , the simul result of the control algorithm are present . the result show the effect of the propos model and design procedur for the mcfc stack base on neural network identif and the novel adapt fuzzi control","ordered_present_kp":[0,20,43,87,297,386,584,645],"keyphrases":["nonlinear modeling","adaptive fuzzy control","MCFC stack","fuel cells","radial basis function neural networks identification model","input-output sampled data","rule database","nearest-neighbor-clustering algorithm","molten carbonate fuel cells stack","backpropagation algorithm"],"prmu":["P","P","P","P","P","P","P","P","R","M"]}
{"id":"2046","title":"Designing and delivering a university course - a process (or operations) management perspective","abstract":"With over 30 years of academic experience in both engineering and management faculties, involving trial and error experimentation in teaching as well as reading relevant literature and observing other instructors in action, the author has accumulated a number of ideas, regarding the preparation and delivery of a university course, that should be of interest to other instructors. This should be particularly the case for those individuals who have had little or no teaching experience (e.g. those whose graduate education was recently completed at research-oriented institutions providing little guidance with respect to teaching). A particular perspective is used to convey the ideas, namely one of viewing the preparation and delivery of a course as two major processes that should provide outputs or outcomes that are of value to a number of customers, in particular, students","tok_text":"design and deliv a univers cours - a process ( or oper ) manag perspect \n with over 30 year of academ experi in both engin and manag faculti , involv trial and error experiment in teach as well as read relev literatur and observ other instructor in action , the author ha accumul a number of idea , regard the prepar and deliveri of a univers cours , that should be of interest to other instructor . thi should be particularli the case for those individu who have had littl or no teach experi ( e.g. those whose graduat educ wa recent complet at research-ori institut provid littl guidanc with respect to teach ) . a particular perspect is use to convey the idea , name one of view the prepar and deliveri of a cours as two major process that should provid output or outcom that are of valu to a number of custom , in particular , student","ordered_present_kp":[57,95,127],"keyphrases":["management perspective","academic experience","management faculties","university course delivery","engineering faculties","search-oriented institutions"],"prmu":["P","P","P","R","R","M"]}
{"id":"389","title":"Bayesian nonstationary autoregressive models for biomedical signal analysis","abstract":"We describe a variational Bayesian algorithm for the estimation of a multivariate autoregressive model with time-varying coefficients that adapt according to a linear dynamical system. The algorithm allows for time and frequency domain characterization of nonstationary multivariate signals and is especially suited to the analysis of event-related data. Results are presented on synthetic data and real electroencephalogram data recorded in event-related desynchronization and photic synchronization scenarios","tok_text":"bayesian nonstationari autoregress model for biomed signal analysi \n we describ a variat bayesian algorithm for the estim of a multivari autoregress model with time-vari coeffici that adapt accord to a linear dynam system . the algorithm allow for time and frequenc domain character of nonstationari multivari signal and is especi suit to the analysi of event-rel data . result are present on synthet data and real electroencephalogram data record in event-rel desynchron and photic synchron scenario","ordered_present_kp":[476,451,257,45,0,202,82,160],"keyphrases":["Bayesian nonstationary autoregressive models","biomedical signal analysis","variational Bayesian algorithm","time-varying coefficients","linear dynamical system","frequency domain characterization","event-related desynchronization","photic synchronization scenarios","time domain characterization","Kalman smoother","EEG analysis"],"prmu":["P","P","P","P","P","P","P","P","R","U","M"]}
{"id":"374","title":"Horizontal waypoint guidance design using optimal control","abstract":"A horizontal waypoint guidance algorithm is proposed by applying line-following guidance to waypoint line segments in sequence. The line-following guidance is designed using an LQR (linear quadratic regulator). Then, the optimal waypoint changing points are derived by minimizing the accelerations required for changing the waypoint line segments. Also derived is a sufficient condition for the stability bound of ground speed changes based on the Lyapunov stability theorem. Simulation results show that the proposed algorithm can effectively guide a vehicle along the sequence of waypoint line segments","tok_text":"horizont waypoint guidanc design use optim control \n a horizont waypoint guidanc algorithm is propos by appli line-follow guidanc to waypoint line segment in sequenc . the line-follow guidanc is design use an lqr ( linear quadrat regul ) . then , the optim waypoint chang point are deriv by minim the acceler requir for chang the waypoint line segment . also deriv is a suffici condit for the stabil bound of ground speed chang base on the lyapunov stabil theorem . simul result show that the propos algorithm can effect guid a vehicl along the sequenc of waypoint line segment","ordered_present_kp":[55,110,133,209,215,251,393,409,440],"keyphrases":["horizontal waypoint guidance algorithm","line-following guidance","waypoint line segments","LQR","linear quadratic regulator","optimal waypoint changing points","stability bound","ground speed changes","Lyapunov stability theorem","unmanned flying vehicle","threat avoidance","terrain masking","attack directions","target location arrival time"],"prmu":["P","P","P","P","P","P","P","P","P","M","U","U","U","U"]}
{"id":"2086","title":"A design to cost system for innovative product development","abstract":"Presents a prototype object-oriented and rule-based system for product cost modelling and design for automation at an early design stage. The developed system comprises a computer aided design (CAD) solid modelling system, a material selection module, a knowledge-based system (KBS), a process optimization module, a design for assembly module, a cost estimation module and a user interface. Two manufacturing processes, namely machining and injection moulding processes, were considered in the developed system. The main function of the system, besides estimating the product cost, is to generate initial process planning, including the generation and selection of machining processes, their sequence and their machining parameters, and to recommend the most economical assembly technique for a product and provide design improvement suggestions based on a design feasibility technique. In addition, a feature-by-feature cost estimation report is generated using the proposed system to highlight the features of high manufacturing cost. Two case studies were used to validate the developed system","tok_text":"a design to cost system for innov product develop \n present a prototyp object-ori and rule-bas system for product cost model and design for autom at an earli design stage . the develop system compris a comput aid design ( cad ) solid model system , a materi select modul , a knowledge-bas system ( kb ) , a process optim modul , a design for assembl modul , a cost estim modul and a user interfac . two manufactur process , name machin and inject mould process , were consid in the develop system . the main function of the system , besid estim the product cost , is to gener initi process plan , includ the gener and select of machin process , their sequenc and their machin paramet , and to recommend the most econom assembl techniqu for a product and provid design improv suggest base on a design feasibl techniqu . in addit , a feature-by-featur cost estim report is gener use the propos system to highlight the featur of high manufactur cost . two case studi were use to valid the develop system","ordered_present_kp":[2,28,106,129,251,275,307,331,360,383,429,440,582,832],"keyphrases":["design to cost system","innovative product development","product cost modelling","design for automation","material selection module","knowledge-based system","process optimization module","design for assembly module","cost estimation module","user interface","machining","injection moulding","process planning","feature-by-feature cost estimation report","object-oriented rule-based system","computer aided design solid modelling system","fuzzy logic","object-oriented programming","concurrent engineering"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","U","M","U"]}
{"id":"331","title":"Multidimensional data visualization","abstract":"Historically, data visualization has been limited primarily to two dimensions (e.g., histograms or scatter plots). Available software packages (e.g., Data Desk 6.1, MatLab 6.1, SAS-JMP 4.04, SPSS 10.0) are capable of producing three-dimensional scatter plots with (varying degrees of) user interactivity. We constructed our own data visualization application with the Visualization Toolkit (Schroeder et al., 1998) and Tcl\/Tk to display multivariate data through the application of glyphs (Ware, 2000). A glyph is a visual object onto which many data parameters may be mapped, each with a different visual attribute (e.g., size or color). We used our multi-dimensional data viewer to explore data from several psycholinguistic experiments. The graphical interface provides flexibility when users dynamically explore the multidimensional image rendered from raw experimental data. We highlight advantages of multidimensional data visualization and consider some potential limitations","tok_text":"multidimension data visual \n histor , data visual ha been limit primarili to two dimens ( e.g. , histogram or scatter plot ) . avail softwar packag ( e.g. , data desk 6.1 , matlab 6.1 , sas-jmp 4.04 , spss 10.0 ) are capabl of produc three-dimension scatter plot with ( vari degre of ) user interact . we construct our own data visual applic with the visual toolkit ( schroeder et al . , 1998 ) and tcl \/ tk to display multivari data through the applic of glyph ( ware , 2000 ) . a glyph is a visual object onto which mani data paramet may be map , each with a differ visual attribut ( e.g. , size or color ) . we use our multi-dimension data viewer to explor data from sever psycholinguist experi . the graphic interfac provid flexibl when user dynam explor the multidimension imag render from raw experiment data . we highlight advantag of multidimension data visual and consid some potenti limit","ordered_present_kp":[0,286,351,399,456,493,523,568,622,676,704,763],"keyphrases":["multidimensional data visualization","user interactivity","Visualization Toolkit","Tcl\/Tk","glyphs","visual object","data parameters","visual attribute","multi-dimensional data viewer","psycholinguistic experiments","graphical interface","multidimensional image rendering","3D scatter plots","multivariate data display"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","M","R"]}
{"id":"1952","title":"Comprehensive encoding and decoupling solution to problems of decoherence and design in solid-state quantum computing","abstract":"Proposals for scalable quantum computing devices suffer not only from decoherence due to the interaction with their environment, but also from severe engineering constraints. Here we introduce a practical solution to these major concerns, addressing solid-state proposals in particular. Decoherence is first reduced by encoding a logical qubit into two qubits, then completely eliminated by an efficient set of decoupling pulse sequences. The same encoding removes the need for single-qubit operations, which pose a difficult design constraint. We further show how the dominant decoherence processes can be identified empirically, in order to optimize the decoupling pulses","tok_text":"comprehens encod and decoupl solut to problem of decoher and design in solid-st quantum comput \n propos for scalabl quantum comput devic suffer not onli from decoher due to the interact with their environ , but also from sever engin constraint . here we introduc a practic solut to these major concern , address solid-st propos in particular . decoher is first reduc by encod a logic qubit into two qubit , then complet elimin by an effici set of decoupl puls sequenc . the same encod remov the need for single-qubit oper , which pose a difficult design constraint . we further show how the domin decoher process can be identifi empir , in order to optim the decoupl puls","ordered_present_kp":[71,49,227,108],"keyphrases":["decoherence","solid-state quantum computing","scalable quantum computing devices","engineering constraints","logical qubit encoding","pulse sequence decoupling","decoupling pulse optimization","exchange Hamiltonian"],"prmu":["P","P","P","P","R","R","R","U"]}
{"id":"195","title":"The acquisition of out-of-print music","abstract":"Non-specialist librarians are alerted to factors important in the successful acquisition of out-of-print music, both scholarly editions and performance editions. The appropriate technical music vocabulary, the music publishing industry, specialized publishers and vendors, and methods of acquisition of out-of-print printed music are introduced, and the need for familiarity with them is emphasized","tok_text":"the acquisit of out-of-print music \n non-specialist librarian are alert to factor import in the success acquisit of out-of-print music , both scholarli edit and perform edit . the appropri technic music vocabulari , the music publish industri , special publish and vendor , and method of acquisit of out-of-print print music are introduc , and the need for familiar with them is emphas","ordered_present_kp":[16,142,161,189,220,245,300],"keyphrases":["out-of-print music","scholarly editions","performance editions","technical music vocabulary","music publishing industry","specialized publishers","out-of-print printed music","specialized vendors"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"269","title":"Genetic algorithm guided selection: variable selection and subset selection","abstract":"A novel genetic algorithm guided selection method, GAS, has been described. The method utilizes a simple encoding scheme which can represent both compounds and variables used to construct a QSAR\/QSPR model. A genetic algorithm is then utilized to simultaneously optimize the encoded variables that include both descriptors and compound subsets. The GAS method generates multiple models each applying to a subset of the compounds. Typically the subsets represent clusters with different chemotypes. Also a procedure based on molecular similarity is presented to determine which model should be applied to a given test set compound. The variable selection method implemented in GAS has been tested and compared using the Selwood data set (n = 31 compounds; nu = 53 descriptors). The results showed that the method is comparable to other published methods. The subset selection method implemented in GAS has been first tested using an artificial data set (n = 100 points; nu = 1 descriptor) to examine its ability to subset data points and second applied to analyze the XLOGP data set (n = 1831 compounds; nu = 126 descriptors). The method is able to correctly identify artificial data points belonging to various subsets. The analysis of the XLOGP data set shows that the subset selection method can be useful in improving a QSAR\/QSPR model when the variable selection method fails","tok_text":"genet algorithm guid select : variabl select and subset select \n a novel genet algorithm guid select method , ga , ha been describ . the method util a simpl encod scheme which can repres both compound and variabl use to construct a qsar \/ qspr model . a genet algorithm is then util to simultan optim the encod variabl that includ both descriptor and compound subset . the ga method gener multipl model each appli to a subset of the compound . typic the subset repres cluster with differ chemotyp . also a procedur base on molecular similar is present to determin which model should be appli to a given test set compound . the variabl select method implement in ga ha been test and compar use the selwood data set ( n = 31 compound ; nu = 53 descriptor ) . the result show that the method is compar to other publish method . the subset select method implement in ga ha been first test use an artifici data set ( n = 100 point ; nu = 1 descriptor ) to examin it abil to subset data point and second appli to analyz the xlogp data set ( n = 1831 compound ; nu = 126 descriptor ) . the method is abl to correctli identifi artifici data point belong to variou subset . the analysi of the xlogp data set show that the subset select method can be use in improv a qsar \/ qspr model when the variabl select method fail","ordered_present_kp":[73,157,192,30,30,49,232,295,336,351,389,468,488,523,697,1018,1119],"keyphrases":["variables","variable selection","subset selection","genetic algorithm guided selection method","encoding scheme","compounds","QSAR\/QSPR model","optimization","descriptors","compound subsets","multiple models","clusters","chemotypes","molecular similarity","Selwood data set","XLOGP data set","artificial data points"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2166","title":"Don't always believe what you Reed [optimisation techniques for Web sites and trade mark infringement]","abstract":"On 20 May 2002, Mr Justice Pumfrey gave judgment in the case of (1) Reed Executive Plc (2) Reed Solutions Plc versus (1) Reed Business Information Limited (2) Reed Elsevier (UK) Limited (3) totaljobs.com Limited. The case explored for the first time in any detail the extent to which the use of various optimisation techniques for Web sites could give rise to new forms of trade mark infringement and passing off. The author reports on the case and offers his comments","tok_text":"do n't alway believ what you reed [ optimis techniqu for web site and trade mark infring ] \n on 20 may 2002 , mr justic pumfrey gave judgment in the case of ( 1 ) reed execut plc ( 2 ) reed solut plc versu ( 1 ) reed busi inform limit ( 2 ) reed elsevi ( uk ) limit ( 3 ) totaljobs.com limit . the case explor for the first time in ani detail the extent to which the use of variou optimis techniqu for web site could give rise to new form of trade mark infring and pass off . the author report on the case and offer hi comment","ordered_present_kp":[163,185,212,241,272,36,57,70,465],"keyphrases":["optimisation techniques","Web sites","trade mark infringement","Reed Executive Plc","Reed Solutions Plc","Reed Business Information Limited","Reed Elsevier (UK) Limited","totaljobs.com Limited","passing off"],"prmu":["P","P","P","P","P","P","P","P","P"]}
{"id":"2123","title":"\"Hidden convexity\" of finite-dimensional stationary linear discrete-time systems under conical constraints","abstract":"New properties of finite-dimensional linear discrete-time systems under conical control constraints that are similar to the \"hidden convexity\" of continuous-time systems are studied","tok_text":"\" hidden convex \" of finite-dimension stationari linear discrete-tim system under conic constraint \n new properti of finite-dimension linear discrete-tim system under conic control constraint that are similar to the \" hidden convex \" of continuous-tim system are studi","ordered_present_kp":[2,21,82,173],"keyphrases":["hidden convexity","finite-dimensional stationary linear discrete-time systems","conical constraints","control constraint"],"prmu":["P","P","P","P"]}
{"id":"294","title":"High-density remote storage: the Ohio State University Libraries depository","abstract":"The article describes a high-density off-site book storage facility operated by the Ohio State University Libraries. Opened in 1995, it has the capacity to house nearly 1.5 million items in only 9000 square feet by shelving books by size on 30-foot tall shelving. A sophisticated climate control system extends the life of stored materials up to 12 times. An online catalog record for each item informs patrons that the item is located in a remote location. Regular courier deliveries from the storage facility bring requested materials to patrons with minimal delay","tok_text":"high-dens remot storag : the ohio state univers librari depositori \n the articl describ a high-dens off-sit book storag facil oper by the ohio state univers librari . open in 1995 , it ha the capac to hous nearli 1.5 million item in onli 9000 squar feet by shelv book by size on 30-foot tall shelv . a sophist climat control system extend the life of store materi up to 12 time . an onlin catalog record for each item inform patron that the item is locat in a remot locat . regular courier deliveri from the storag facil bring request materi to patron with minim delay","ordered_present_kp":[0,29,90,257,310,351,425,383,460,482],"keyphrases":["high-density remote storage","Ohio State University Libraries","high-density off-site book storage facility","shelving","climate control system","stored materials","online catalog record","patrons","remote location","courier deliveries","circulation"],"prmu":["P","P","P","P","P","P","P","P","P","P","U"]}
{"id":"312","title":"Information architecture for the Web: The IA matrix approach to designing children's portals","abstract":"The article presents a matrix that can serve as a tool for designing the information architecture of a Web portal in a logical and systematic manner. The information architect begins by inputting the portal's objective, target user, and target content. The matrix then determines the most appropriate information architecture attributes for the portal by filling in the Applied Information Architecture portion of the matrix. The article discusses how the matrix works using the example of a children's Web portal to provide access to museum information","tok_text":"inform architectur for the web : the ia matrix approach to design children 's portal \n the articl present a matrix that can serv as a tool for design the inform architectur of a web portal in a logic and systemat manner . the inform architect begin by input the portal 's object , target user , and target content . the matrix then determin the most appropri inform architectur attribut for the portal by fill in the appli inform architectur portion of the matrix . the articl discuss how the matrix work use the exampl of a children 's web portal to provid access to museum inform","ordered_present_kp":[0,281,299,525,568],"keyphrases":["information architecture","target user","target content","children's Web portal","museum information"],"prmu":["P","P","P","P","P"]}
{"id":"357","title":"Embedding of level-continuous fuzzy sets on Banach spaces","abstract":"In this paper we present an extension of the Minkowski embedding theorem, showing the existence of an isometric embedding between the classF\/sub c\/(X) of compact-convex and level-continuous fuzzy sets on a real separable Banach space X and C([0, 1] * B(X*)), the Banach space of real continuous functions defined on the cartesian product between [0, 1] and the unit ball B(X*) in the dual space X*. Also, by using this embedding, we give some applications to the characterization of relatively compact subsets of F\/sub c\/(X). In particular, an Ascoli-Arzela type theorem is proved and applied to solving the Cauchy problem x(t) = f(t, x(t)), x(t\/sub 0\/) = x\/sub 0\/ on F\/sub c\/(X)","tok_text":"embed of level-continu fuzzi set on banach space \n in thi paper we present an extens of the minkowski embed theorem , show the exist of an isometr embed between the classf \/ sub c\/(x ) of compact-convex and level-continu fuzzi set on a real separ banach space x and c([0 , 1 ] * b(x * ) ) , the banach space of real continu function defin on the cartesian product between [ 0 , 1 ] and the unit ball b(x * ) in the dual space x * . also , by use thi embed , we give some applic to the character of rel compact subset of f \/ sub c\/(x ) . in particular , an ascoli-arzela type theorem is prove and appli to solv the cauchi problem x(t ) = f(t , x(t ) ) , x(t \/ sub 0\/ ) = x \/ sub 0\/ on f \/ sub c\/(x )","ordered_present_kp":[139,9,236,311,346,390,415,556,614],"keyphrases":["level-continuous fuzzy sets","isometric embedding","real separable Banach space","real continuous functions","cartesian product","unit ball","dual space","Ascoli-Arzela type theorem","Cauchy problem","compact-convex fuzzy sets"],"prmu":["P","P","P","P","P","P","P","P","P","R"]}
{"id":"2058","title":"Four factors influencing the fair market value of out-of-print books.1","abstract":"Four factors (edition, condition, dust jacket, and autograph) that are hypothesized to influence the value of books are identified and linked to basic economic principles, which are explained. A sample of fifty-six titles is qualitatively examined to test the hypothesis","tok_text":"four factor influenc the fair market valu of out-of-print books.1 \n four factor ( edit , condit , dust jacket , and autograph ) that are hypothes to influenc the valu of book are identifi and link to basic econom principl , which are explain . a sampl of fifty-six titl is qualit examin to test the hypothesi","ordered_present_kp":[25,206],"keyphrases":["fair market value","economic principles","out-of-print books","pricing"],"prmu":["P","P","R","U"]}
{"id":"2100","title":"Optimization of planning an advertising campaign of goods and services","abstract":"A generalization of the mathematical model and operations research problems formulated on its basis, which were presented by Belenky (2001) in the framework of an approach to planning an advertising campaign of goods and services, is considered, and corresponding nonlinear programming problems with linear constraints are formulated","tok_text":"optim of plan an advertis campaign of good and servic \n a gener of the mathemat model and oper research problem formul on it basi , which were present by belenki ( 2001 ) in the framework of an approach to plan an advertis campaign of good and servic , is consid , and correspond nonlinear program problem with linear constraint are formul","ordered_present_kp":[0,90,280],"keyphrases":["optimization","operations research","nonlinear programming","advertising campaign planning","OR"],"prmu":["P","P","P","R","U"]}
{"id":"2145","title":"Enterprise in focus at NetSec 2002","abstract":"NetSec 2002 took place in San Francisco, amid industry reflection on the balance to be struck between combatting cyber-terrorism and safeguarding civil liberties post-9.11. The author reports on the punditry and the pedagogy at the CSI event, focusing on security in the enterprise","tok_text":"enterpris in focu at netsec 2002 \n netsec 2002 took place in san francisco , amid industri reflect on the balanc to be struck between combat cyber-terror and safeguard civil liberti post-9.11 . the author report on the punditri and the pedagogi at the csi event , focus on secur in the enterpris","ordered_present_kp":[21,252],"keyphrases":["NetSec 2002","CSI","enterprise security"],"prmu":["P","P","R"]}
{"id":"1934","title":"Computational finite-element schemes for optimal control of an elliptic system with conjugation conditions","abstract":"New optimal control problems are considered for distributed systems described by elliptic equations with conjugate conditions and a quadratic minimized function. Highly accurate computational discretization schemes are constructed for the case where a feasible control set u\/sub delta \/ coincides with the full Hilbert space u of controls","tok_text":"comput finite-el scheme for optim control of an ellipt system with conjug condit \n new optim control problem are consid for distribut system describ by ellipt equat with conjug condit and a quadrat minim function . highli accur comput discret scheme are construct for the case where a feasibl control set u \/ sub delta \/ coincid with the full hilbert space u of control","ordered_present_kp":[87,124,152,67,190,228],"keyphrases":["conjugate conditions","optimal control problems","distributed systems","elliptic equations","quadratic minimized function","computational discretization schemes"],"prmu":["P","P","P","P","P","P"]}
{"id":"1971","title":"Exploring developments in Web based relationship marketing within the hotel industry","abstract":"This paper provides a content analysis study of the application of World Wide Web marketing by the hotel industry. There is a lack of historical perspective on industry related Web marketing applications and this paper attempts to resolve this with a two-year follow-up case study of the changing use of the Web to develop different types of relationships. Specifically, the aims are: (1) to identify key changes in the way hotels are using the Web; (2) to look for evidence of the adoption of a relationship marketing (RM) model as a strategy for the development of hotel Web sites and the use of new technologies; and, (3) To investigate the use of multimedia in hotel Web sites. The development and strategic exploitation of the Internet has transformed the basis of marketing. Using the evidence from a Web content survey this study reveals the way relationships are being created and managed within the hotel industry by its use of the Web as a marketing tool. The authors have collected evidence by means of a descriptive study on the way hotels build and create relationships with their Web presence delivering multimedia information as well as channel and interactive means of communication. In addition a strategic framework is offered as the means to describe the mechanism and orientation of Web based marketing by hotels. The study utilizes a model by Gilbert (1996) as a means of developing a measurement instrument to allow a content analysis of the current approach by hotels to the development of Web sites. The results indicate hotels are aware of the new uses of Web technology and are promoting hotel products in the global electronic market in new and sophisticated ways","tok_text":"explor develop in web base relationship market within the hotel industri \n thi paper provid a content analysi studi of the applic of world wide web market by the hotel industri . there is a lack of histor perspect on industri relat web market applic and thi paper attempt to resolv thi with a two-year follow-up case studi of the chang use of the web to develop differ type of relationship . specif , the aim are : ( 1 ) to identifi key chang in the way hotel are use the web ; ( 2 ) to look for evid of the adopt of a relationship market ( rm ) model as a strategi for the develop of hotel web site and the use of new technolog ; and , ( 3 ) to investig the use of multimedia in hotel web site . the develop and strateg exploit of the internet ha transform the basi of market . use the evid from a web content survey thi studi reveal the way relationship are be creat and manag within the hotel industri by it use of the web as a market tool . the author have collect evid by mean of a descript studi on the way hotel build and creat relationship with their web presenc deliv multimedia inform as well as channel and interact mean of commun . in addit a strateg framework is offer as the mean to describ the mechan and orient of web base market by hotel . the studi util a model by gilbert ( 1996 ) as a mean of develop a measur instrument to allow a content analysi of the current approach by hotel to the develop of web site . the result indic hotel are awar of the new use of web technolog and are promot hotel product in the global electron market in new and sophist way","ordered_present_kp":[18,58,133,585,666,799,1530],"keyphrases":["Web based relationship marketing","hotel industry","World Wide Web marketing","hotel Web sites","multimedia","Web content survey","global electronic market"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"277","title":"Improving the predicting power of partial order based QSARs through linear extensions","abstract":"Partial order theory (POT) is an attractive and operationally simple method that allows ordering of compounds, based on selected structural and\/or electronic descriptors (modeled order), or based on their end points, e.g., solubility (experimental order). If the modeled order resembles the experimental order, compounds that are not experimentally investigated can be assigned a position in the model that eventually might lead to a prediction of an end-point value. However, in the application of POT in quantitative structure-activity relationship modeling, only the compounds directly comparable to the noninvestigated compounds are applied. To explore the possibilities of improving the methodology, the theory is extended by application of the so-called linear extensions of the model order. The study show that partial ordering combined with linear extensions appears as a promising tool providing probability distribution curves in the range of possible end-point values for compounds not being experimentally investigated","tok_text":"improv the predict power of partial order base qsar through linear extens \n partial order theori ( pot ) is an attract and oper simpl method that allow order of compound , base on select structur and\/or electron descriptor ( model order ) , or base on their end point , e.g. , solubl ( experiment order ) . if the model order resembl the experiment order , compound that are not experiment investig can be assign a posit in the model that eventu might lead to a predict of an end-point valu . howev , in the applic of pot in quantit structure-act relationship model , onli the compound directli compar to the noninvestig compound are appli . to explor the possibl of improv the methodolog , the theori is extend by applic of the so-cal linear extens of the model order . the studi show that partial order combin with linear extens appear as a promis tool provid probabl distribut curv in the rang of possibl end-point valu for compound not be experiment investig","ordered_present_kp":[525,76,60,203,225,258,277],"keyphrases":["linear extensions","partial order theory","electronic descriptors","modeled order","end points","solubilities","quantitative structure-activity relationships","predicting power improvement","structural descriptors","graphical representation","combinatorial rule","most probable linear order","partially ordered set","Hasse diagram","organic compounds"],"prmu":["P","P","P","P","P","P","P","R","R","U","U","M","M","U","M"]}
{"id":"2185","title":"In search of a general enterprise model","abstract":"Many organisations, particularly SMEs, are reluctant to invest time and money in models to support decision making. Such reluctance could be overcome if a model could be used for several purposes rather than using a traditional \"single perspective\" model. This requires the development of a \"general enterprise model\" (GEM), which can be applied to a wide range of problem domains with unlimited scope. Current enterprise modelling frameworks only deal effectively with nondynamic modelling issues whilst dynamic modelling issues have traditionally only been addressed at the operational level. Although the majority of research in this area relates to manufacturing companies, the framework for a GEM must be equally applicable to service and public sector organisations. The paper identifies five key design issues that need to be considered when constructing a GEM. A framework for such a GEM is presented based on a \"plug and play\" methodology and demonstrated by a simple case study","tok_text":"in search of a gener enterpris model \n mani organis , particularli sme , are reluct to invest time and money in model to support decis make . such reluct could be overcom if a model could be use for sever purpos rather than use a tradit \" singl perspect \" model . thi requir the develop of a \" gener enterpris model \" ( gem ) , which can be appli to a wide rang of problem domain with unlimit scope . current enterpris model framework onli deal effect with nondynam model issu whilst dynam model issu have tradit onli been address at the oper level . although the major of research in thi area relat to manufactur compani , the framework for a gem must be equal applic to servic and public sector organis . the paper identifi five key design issu that need to be consid when construct a gem . a framework for such a gem is present base on a \" plug and play \" methodolog and demonstr by a simpl case studi","ordered_present_kp":[15,67,129,320,365,409,538,460,683,894],"keyphrases":["general enterprise model","SMEs","decision making","GEM","problem domains","enterprise modelling frameworks","dynamic modelling issues","operational level","public sector organisations","case study","business process re-engineering","single perspective model","service sector organisations","plug and play methodology"],"prmu":["P","P","P","P","P","P","P","P","P","P","U","R","R","R"]}
{"id":"232","title":"Library services today and tomorrow: lessons from iLumina, a digital library for creating and sharing teaching resources","abstract":"This article is based on the emerging experience associated with a digital library of instructional resources, iLumina, in which the contributors of resources and the users of those resources are the same-an open community of instructors in science, mathematics, engineering, and technology. Moreover, it is not the resources, most of which will be distributed across the Internet, but metadata about the resources that is the focus of the central iLumina repository and its support services for resource contributors and users. The distributed iLumina library is a community-sharing library for repurposing and adding value to potentially useful, mostly non-commercial instructional resources that are typically more granular in nature than commercially developed course materials. The experience of developing iLumina is raising a range of issues that have nothing to do with the place and time characteristics of the instructional context in which iLumina instructional resources are created or used. The issues instead have their locus in the democratization of both the professional roles of librarians and the quality assurance mechanisms associated with traditional peer review","tok_text":"librari servic today and tomorrow : lesson from ilumina , a digit librari for creat and share teach resourc \n thi articl is base on the emerg experi associ with a digit librari of instruct resourc , ilumina , in which the contributor of resourc and the user of those resourc are the same-an open commun of instructor in scienc , mathemat , engin , and technolog . moreov , it is not the resourc , most of which will be distribut across the internet , but metadata about the resourc that is the focu of the central ilumina repositori and it support servic for resourc contributor and user . the distribut ilumina librari is a community-shar librari for repurpos and ad valu to potenti use , mostli non-commerci instruct resourc that are typic more granular in natur than commerci develop cours materi . the experi of develop ilumina is rais a rang of issu that have noth to do with the place and time characterist of the instruct context in which ilumina instruct resourc are creat or use . the issu instead have their locu in the democrat of both the profession role of librarian and the qualiti assur mechan associ with tradit peer review","ordered_present_kp":[48,60,440,455,625,1051,1070,1088,1128],"keyphrases":["iLumina","digital library","Internet","metadata","community-sharing library","professional roles","librarians","quality assurance","peer review","teaching resource sharing","information resources","academic library","library automation","standards","interoperability","reusable software","distributed systems","user issues"],"prmu":["P","P","P","P","P","P","P","P","P","R","M","M","M","U","U","U","M","R"]}
{"id":"2178","title":"Medicine in the 21 st century: global problems, global solutions","abstract":"The objectives are to discuss application areas of information, technology in medicine and health care on the occasion of the opening of the Private Universitat fur Medizinische Informatik and Technik Tirol\/University for Health Informatics and Technology Tyrol (LIMIT) at Innsbruck, Tyrol, Austria. Important application areas of information technology in medicine and health are appropriate individual access to medical knowledge, new engineering developments such as new radiant imaging methods and the implantable pacemaker\/defibrillator devices, mathematical modeling for understanding the workings of the human body, the computer-based patient record, as well as new knowledge in molecular biology, human genetics, and biotechnology. Challenges and responsibilities for medical informatics research include medical data privacy and intellectual property rights inherent in the content of the information systems","tok_text":"medicin in the 21 st centuri : global problem , global solut \n the object are to discuss applic area of inform , technolog in medicin and health care on the occas of the open of the privat universitat fur medizinisch informatik and technik tirol \/ univers for health informat and technolog tyrol ( limit ) at innsbruck , tyrol , austria . import applic area of inform technolog in medicin and health are appropri individu access to medic knowledg , new engin develop such as new radiant imag method and the implant pacemak \/ defibril devic , mathemat model for understand the work of the human bodi , the computer-bas patient record , as well as new knowledg in molecular biolog , human genet , and biotechnolog . challeng and respons for medic informat research includ medic data privaci and intellectu properti right inher in the content of the inform system","ordered_present_kp":[138,0,361,453,479,542,588,605,662,681,699,739,770,793,847],"keyphrases":["medicine","health care","information technology","engineering developments","radiant imaging methods","mathematical modeling","human body","computer-based patient record","molecular biology","human genetics","biotechnology","medical informatics research","medical data privacy","intellectual property rights","information systems","individual medical knowledge access","implantable pacemaker devices","implantable defibrillator devices"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"24","title":"Fuzzy modeling based on generalized conjunction operations","abstract":"An approach to fuzzy modeling based on the tuning of parametric conjunction operations is proposed. First, some methods for the construction of parametric generalized conjunction operations simpler than the known parametric classes of conjunctions are considered and discussed. Second, several examples of function approximation by fuzzy models, based on the tuning of the parameters of the new conjunction operations, are given and their approximation performances are compared with the approaches based on a tuning of membership functions and other approaches proposed in the literature. It is seen that the tuning of the conjunction operations can be used for obtaining fuzzy models with a sufficiently good performance when the tuning of membership functions is not possible or not desirable","tok_text":"fuzzi model base on gener conjunct oper \n an approach to fuzzi model base on the tune of parametr conjunct oper is propos . first , some method for the construct of parametr gener conjunct oper simpler than the known parametr class of conjunct are consid and discuss . second , sever exampl of function approxim by fuzzi model , base on the tune of the paramet of the new conjunct oper , are given and their approxim perform are compar with the approach base on a tune of membership function and other approach propos in the literatur . it is seen that the tune of the conjunct oper can be use for obtain fuzzi model with a suffici good perform when the tune of membership function is not possibl or not desir","ordered_present_kp":[0,20,294,81,408,472],"keyphrases":["fuzzy modeling","generalized conjunction operations","tuning","function approximation","approximation performances","membership functions","t-norm","fuzzy inference systems"],"prmu":["P","P","P","P","P","P","U","M"]}
{"id":"2065","title":"Emotion and self-control","abstract":"A biology-based model of choice is used to examine time-inconsistent preferences and the problem of self-control. Emotion is shown to be the biological substrate of choice, in that emotional systems assign value to 'goods' in the environment and also facilitate the learning of expectations regarding alternative options for acquiring those goods. A third major function of the emotional choice systems is motivation. Self-control is shown to be the result of a problem with the inhibition of the motive force of emotion, where this inhibition is necessary for higher level deliberation","tok_text":"emot and self-control \n a biology-bas model of choic is use to examin time-inconsist prefer and the problem of self-control . emot is shown to be the biolog substrat of choic , in that emot system assign valu to ' good ' in the environ and also facilit the learn of expect regard altern option for acquir those good . a third major function of the emot choic system is motiv . self-control is shown to be the result of a problem with the inhibit of the motiv forc of emot , where thi inhibit is necessari for higher level deliber","ordered_present_kp":[438,257,70,9,348,0],"keyphrases":["emotion","self-control","time-inconsistent preferences","learning","emotional choice systems","inhibition","choice model"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"2020","title":"Restoration of broadband imagery steered with a liquid-crystal optical phased array","abstract":"In many imaging applications, it is highly desirable to replace mechanical beam-steering components (i.e., mirrors and gimbals) with a nonmechanical device. One such device is a nematic liquid crystal optical phased array (LCOPA). An LCOPA can implement a blazed phase grating to steer the incident light. However, when a phase grating is used in a broadband imaging system, two adverse effects can occur. First, dispersion will cause different incident wavelengths arriving at the same angle to be steered to different output angles, causing chromatic aberrations in the image plane. Second, the device will steer energy not only to the first diffraction order, but to others as well. This multiple-order effect results in multiple copies of the scene appearing in the image plane. We describe a digital image restoration technique designed to overcome these degradations. The proposed postprocessing technique is based on a Wiener deconvolution filter. The technique, however, is applicable only to scenes containing objects with approximately constant reflectivities over the spectral region of interest. Experimental results are presented to demonstrate the effectiveness of this technique","tok_text":"restor of broadband imageri steer with a liquid-cryst optic phase array \n in mani imag applic , it is highli desir to replac mechan beam-steer compon ( i.e. , mirror and gimbal ) with a nonmechan devic . one such devic is a nemat liquid crystal optic phase array ( lcopa ) . an lcopa can implement a blaze phase grate to steer the incid light . howev , when a phase grate is use in a broadband imag system , two advers effect can occur . first , dispers will caus differ incid wavelength arriv at the same angl to be steer to differ output angl , caus chromat aberr in the imag plane . second , the devic will steer energi not onli to the first diffract order , but to other as well . thi multiple-ord effect result in multipl copi of the scene appear in the imag plane . we describ a digit imag restor techniqu design to overcom these degrad . the propos postprocess techniqu is base on a wiener deconvolut filter . the techniqu , howev , is applic onli to scene contain object with approxim constant reflect over the spectral region of interest . experiment result are present to demonstr the effect of thi techniqu","ordered_present_kp":[10,82,125,159,170,186,224,300,384,446,471,533,552,573,54,639,689,719,785,856,890,984,1019],"keyphrases":["broadband imagery","optical phased array","imaging applications","mechanical beam-steering components","mirrors","gimbals","nonmechanical device","nematic liquid crystal optical phased array","blazed phase grating","broadband imaging system","dispersion","incident wavelengths","output angles","chromatic aberrations","image plane","first diffraction order","multiple-order effect","multiple copies","digital image restoration technique","postprocessing technique","Wiener deconvolution filter","approximately constant reflectivities","spectral region of interest","liquid-crystal optical phased array steering","incident light steering","halogen lamp"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","U"]}
{"id":"397","title":"Accurate modeling of lossy nonuniform transmission lines by using differential quadrature methods","abstract":"This paper discusses an efficient numerical approximation technique, called the differential quadrature method (DQM), which has been adapted to model lossy uniform and nonuniform transmission lines. The DQM can quickly compute the derivative of a function at any point within its bounded domain by estimating a weighted linear sum of values of the function at a small set of points belonging to the domain. Using the DQM, the frequency-domain Telegrapher's partial differential equations for transmission lines can be discretized into a set of easily solvable algebraic equations. DQM reduces interconnects into multiport models whose port voltages and currents are related by rational formulas in the frequency domain. Although the rationalization process in DQM is comparable with the Pade approximation of asymptotic waveform evaluation (AWE) applied to transmission lines, the derivation mechanisms in these two disparate methods are significantly different. Unlike AWE, which employs a complex moment-matching process to obtain rational approximation, the DQM requires no approximation of transcendental functions, thereby avoiding the process of moment generation and moment matching. Due to global sampling of points in the DQM approximation, it requires far fewer grid points in order to build accurate discrete models than other numerical methods do. The DQM-based time-domain model can be readily integrated in a circuit simulator like SPICE","tok_text":"accur model of lossi nonuniform transmiss line by use differenti quadratur method \n thi paper discuss an effici numer approxim techniqu , call the differenti quadratur method ( dqm ) , which ha been adapt to model lossi uniform and nonuniform transmiss line . the dqm can quickli comput the deriv of a function at ani point within it bound domain by estim a weight linear sum of valu of the function at a small set of point belong to the domain . use the dqm , the frequency-domain telegraph 's partial differenti equat for transmiss line can be discret into a set of easili solvabl algebra equat . dqm reduc interconnect into multiport model whose port voltag and current are relat by ration formula in the frequenc domain . although the ration process in dqm is compar with the pade approxim of asymptot waveform evalu ( awe ) appli to transmiss line , the deriv mechan in these two dispar method are significantli differ . unlik awe , which employ a complex moment-match process to obtain ration approxim , the dqm requir no approxim of transcendent function , therebi avoid the process of moment gener and moment match . due to global sampl of point in the dqm approxim , it requir far fewer grid point in order to build accur discret model than other numer method do . the dqm-base time-domain model can be readili integr in a circuit simul like spice","ordered_present_kp":[15,54,112,495,583,609,627,739,1287],"keyphrases":["lossy nonuniform transmission lines","differential quadrature method","numerical approximation technique","partial differential equations","algebraic equations","interconnects","multiport models","rationalization process","time-domain model","frequency-domain Telegrapher PDE","multiconductor transmission lines"],"prmu":["P","P","P","P","P","P","P","P","P","M","M"]}
{"id":"402","title":"Fast frequency acquisition phase-frequency detectors for Gsamples\/s phase-locked loops","abstract":"This paper describes two techniques for designing phase-frequency detectors (PFDs) with higher operating frequencies [periods of less than 8* the delay of a fan-out-4 inverter (FO-4)] and faster frequency acquisition. Prototypes designed in 0.25- mu m CMOS process exhibit operating frequencies of 1.25 GHz [=1\/(8.FO-4)] and 1.5 GHz [=1\/(6.7.FO-4)] for two techniques, respectively, whereas a conventional PFD operates at <1 GHz [=1\/(10.FO-4)]. The two proposed PFDs achieve a capture range of 1.7* and 1.4* the conventional design, respectively","tok_text":"fast frequenc acquisit phase-frequ detector for gsampl \/ s phase-lock loop \n thi paper describ two techniqu for design phase-frequ detector ( pfd ) with higher oper frequenc [ period of less than 8 * the delay of a fan-out-4 invert ( fo-4 ) ] and faster frequenc acquisit . prototyp design in 0.25- mu m cmo process exhibit oper frequenc of 1.25 ghz [= 1\/(8.fo-4 ) ] and 1.5 ghz [= 1\/(6.7.fo-4 ) ] for two techniqu , respect , wherea a convent pfd oper at < 1 ghz [= 1\/(10.fo-4 ) ] . the two propos pfd achiev a captur rang of 1.7 * and 1.4 * the convent design , respect","ordered_present_kp":[23,0,304,59,341,371],"keyphrases":["fast frequency acquisition","phase-frequency detectors","phase-locked loop","CMOS process","1.25 GHz","1.5 GHz","clock generator","latch-based PFD architecture","GSamples\/s PLL","pass-transistor DFF PFD architecture","0.25 micron"],"prmu":["P","P","P","P","P","P","U","M","M","M","U"]}
{"id":"2098","title":"Nonlinear systems arising from nonisothermal, non-Newtonian Hele-Shaw flows in the presence of body forces and sources","abstract":"In this paper, we first give a formal derivation of several systems of equations for injection moulding. This is done starting from the basic equations for nonisothermal, non-Newtonian flows in a three-dimensional domain. We derive systems for both (T\/sup 0\/, p\/sup 0\/) and (T\/sup 1\/, p\/sup 1\/) in the presence of body forces and sources. We find that body forces and sources have a nonlinear effect on the systems. We also derive a nonlinear \"Darcy law\". Our formulation includes not only the pressure gradient, but also body forces and sources, which play the role of a nonlinearity. Later, we prove the existence of weak solutions to certain boundary value problems and initial-boundary value problems associated with the resulting equations for (T\/sup 0\/, p\/sup 0\/) but in a more general mathematical setting","tok_text":"nonlinear system aris from nonisotherm , non-newtonian hele-shaw flow in the presenc of bodi forc and sourc \n in thi paper , we first give a formal deriv of sever system of equat for inject mould . thi is done start from the basic equat for nonisotherm , non-newtonian flow in a three-dimension domain . we deriv system for both ( t \/ sup 0\/ , p \/ sup 0\/ ) and ( t \/ sup 1\/ , p \/ sup 1\/ ) in the presenc of bodi forc and sourc . we find that bodi forc and sourc have a nonlinear effect on the system . we also deriv a nonlinear \" darci law \" . our formul includ not onli the pressur gradient , but also bodi forc and sourc , which play the role of a nonlinear . later , we prove the exist of weak solut to certain boundari valu problem and initial-boundari valu problem associ with the result equat for ( t \/ sup 0\/ , p \/ sup 0\/ ) but in a more gener mathemat set","ordered_present_kp":[183,88,102,530,0,714,55],"keyphrases":["nonlinear systems","Hele-Shaw flows","body forces","sources","injection moulding","Darcy law","boundary value problems"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"1995","title":"A comparison of computational color constancy algorithms. I: Methodology and experiments with synthesized data","abstract":"We introduce a context for testing computational color constancy, specify our approach to the implementation of a number of the leading algorithms, and report the results of three experiments using synthesized data. Experiments using synthesized data are important because the ground truth is known, possible confounds due to camera characterization and pre-processing are absent, and various factors affecting color constancy can be efficiently investigated because they can be manipulated individually and precisely. The algorithms chosen for close study include two gray world methods, a limiting case of a version of the Retinex method, a number of variants of Forsyth's (1990) gamut-mapping method, Cardei et al.'s (2000) neural net method, and Finlayson et al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel and Finlayson 2000) . We investigate the ability of these algorithms to make estimates of three different color constancy quantities: the chromaticity of the scene illuminant, the overall magnitude of that illuminant, and a corrected, illumination invariant, image. We consider algorithm performance as a function of the number of surfaces in scenes generated from reflectance spectra, the relative effect on the algorithms of added specularities, and the effect of subsequent clipping of the data. All data is available on-line at http:\/\/www.cs.sfu.ca\/~color\/data, and implementations for most of the algorithms are also available (http:\/\/www.cs.sfu.ca\/~color\/code)","tok_text":"a comparison of comput color constanc algorithm . i : methodolog and experi with synthes data \n we introduc a context for test comput color constanc , specifi our approach to the implement of a number of the lead algorithm , and report the result of three experi use synthes data . experi use synthes data are import becaus the ground truth is known , possibl confound due to camera character and pre-process are absent , and variou factor affect color constanc can be effici investig becaus they can be manipul individu and precis . the algorithm chosen for close studi includ two gray world method , a limit case of a version of the retinex method , a number of variant of forsyth 's ( 1990 ) gamut-map method , cardei et al . 's ( 2000 ) neural net method , and finlayson et al . 's color by correl method ( finlayson et al . 1997 , 2001 ; hubel and finlayson 2000 ) . we investig the abil of these algorithm to make estim of three differ color constanc quantiti : the chromat of the scene illumin , the overal magnitud of that illumin , and a correct , illumin invari , imag . we consid algorithm perform as a function of the number of surfac in scene gener from reflect spectra , the rel effect on the algorithm of ad specular , and the effect of subsequ clip of the data . all data is avail on-lin at http:\/\/www.cs.sfu.ca\/~color\/data , and implement for most of the algorithm are also avail ( http:\/\/www.cs.sfu.ca\/~color\/cod )","ordered_present_kp":[16,81,582,635,695,741,786,972,987,1091,1167,1223,1260],"keyphrases":["computational color constancy algorithms","synthesized data","gray world methods","Retinex method","gamut-mapping method","neural net method","color by correlation method","chromaticity","scene illuminant","algorithm performance","reflectance spectra","specularities","clipping","illumination invariant image"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2119","title":"Location of transport nets on a heterogeneous territory","abstract":"The location of transport routes on a heterogeneous territory is studied. The network joins a given set of terminal points and a certain number of additional (branch) points. The problem is formulated, properties of the optimal solution for a. tree-like network, and the number of branch points are studied. A stepwise optimization algorithm for a. network with given adjacency matrix based on an algorithm for constructing minimal-cost routes is designed","tok_text":"locat of transport net on a heterogen territori \n the locat of transport rout on a heterogen territori is studi . the network join a given set of termin point and a certain number of addit ( branch ) point . the problem is formul , properti of the optim solut for a. tree-lik network , and the number of branch point are studi . a stepwis optim algorithm for a. network with given adjac matrix base on an algorithm for construct minimal-cost rout is design","ordered_present_kp":[9,28,63,146,304,267,331,381],"keyphrases":["transport nets","heterogeneous territory","transport routes","terminal points","tree-like network","branch points","stepwise optimization algorithm","adjacency matrix"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"253","title":"Accessible streaming content","abstract":"Make sure your Web site is offering quality service to all your users. The article provides some tips and tactics for making your streaming media accessible. Accessibility of streaming content for people with disabilities is often not part of the spec for multimedia projects, but it certainly affects your quality of service. Most of the resources available on Web accessibility deal with HTML. Fortunately, rich media and streaming content developers have a growing number of experts to turn to for information and assistance. The essentials of providing accessible streaming content are simple: blind and visually impaired people need audio to discern important visual detail and interface elements, while deaf and hard-of-hearing people need text to access sound effects and dialog. Actually implementing these principles is quite a challenge, though. Now due to a relatively new law in the US, known as Section 508, dealing with accessibility issues is becoming an essential part of publishing on the Web","tok_text":"access stream content \n make sure your web site is offer qualiti servic to all your user . the articl provid some tip and tactic for make your stream media access . access of stream content for peopl with disabl is often not part of the spec for multimedia project , but it certainli affect your qualiti of servic . most of the resourc avail on web access deal with html . fortun , rich media and stream content develop have a grow number of expert to turn to for inform and assist . the essenti of provid access stream content are simpl : blind and visual impair peopl need audio to discern import visual detail and interfac element , while deaf and hard-of-hear peopl need text to access sound effect and dialog . actual implement these principl is quit a challeng , though . now due to a rel new law in the us , known as section 508 , deal with access issu is becom an essenti part of publish on the web","ordered_present_kp":[39,57,143,0,246,345,366,397,550,599,617,651,690,824,848],"keyphrases":["accessible streaming content","Web site","quality service","streaming media","multimedia projects","Web accessibility","HTML","streaming content developers","visually impaired people","visual detail","interface elements","hard-of-hearing people","sound effects","Section 508","accessibility issues","content providers","United States","disabled users","blind people","deaf people","Web publishing"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","U","R","R","R","R"]}
{"id":"216","title":"Extinction cross sections of realistic raindrops: data-bank established using T-matrix method and nonlinear fitting technique","abstract":"A new computer program is developed based on the T-matrix method to generate a large number of total (extinction) cross sections (TCS) values of the realistic raindrops that are deformed due to a balance of the forces that act on a drop failing under gravity, and were described in shape by Pruppacher and Pitter (1971). These data for various dimensions of the raindrops (mean effective radius from 0 to 3.25 mm), frequencies (10 to 80 GHz), (horizontal and vertical) polarizations, and temperatures (0, 10 and 20 degrees C) are stored to establish a data bank. Furthermore, a curve fitting technique, i.e., interpolation of order 3, is implemented for the TCS values in the data bank. Therefore, the interpolated TCS results can be obtained readily from the interpolation process with negligible or even null computational time and efforts. Error analysis is carried out to show the high accuracy of the present analysis and applicability of the interpolation. At three operating frequencies of 15, 21.225, and 38 GHz locally used in Singapore, some new TCS values are obtained from the new fast and efficient interpolation with a good accuracy","tok_text":"extinct cross section of realist raindrop : data-bank establish use t-matrix method and nonlinear fit techniqu \n a new comput program is develop base on the t-matrix method to gener a larg number of total ( extinct ) cross section ( tc ) valu of the realist raindrop that are deform due to a balanc of the forc that act on a drop fail under graviti , and were describ in shape by pruppach and pitter ( 1971 ) . these data for variou dimens of the raindrop ( mean effect radiu from 0 to 3.25 mm ) , frequenc ( 10 to 80 ghz ) , ( horizont and vertic ) polar , and temperatur ( 0 , 10 and 20 degre c ) are store to establish a data bank . furthermor , a curv fit techniqu , i.e. , interpol of order 3 , is implement for the tc valu in the data bank . therefor , the interpol tc result can be obtain readili from the interpol process with neglig or even null comput time and effort . error analysi is carri out to show the high accuraci of the present analysi and applic of the interpol . at three oper frequenc of 15 , 21.225 , and 38 ghz local use in singapor , some new tc valu are obtain from the new fast and effici interpol with a good accuraci","ordered_present_kp":[0,25,44,68,562,880,458,341,678,994,1049,119,1029,509,481],"keyphrases":["extinction cross sections","realistic raindrops","data-bank","T-matrix method","computer program","gravity","mean effective radius","0 to 3.25 mm","10 to 80 GHz","temperature","interpolation","error analysis","operating frequencies","38 GHz","Singapore","total cross sections","horizontal polarization","vertical polarization","nonlinear curve fitting technique","SHF","EHF","electromagnetic wave scattering","EM wave scattering","15 GHz","21.225 GHz","0 C","10 C","20 C"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R","R","U","U","U","U","R","R","R","R","R"]}
{"id":"1968","title":"Phase control of higher-order squeezing of a quantum field","abstract":"In a recent experiment [Phys. Rev. Lett. 88 (2002) 023601], phase-dependent photon statistics in a c.w. system has been observed in the mixing of a coherent field with a two-photon source. Their system has the advantage over other atomic transition-based fluorescent systems. In this paper, we examine further the squeezing properties of higher-order quantum fluctuations in one of the quadrature components of the combined field in this system. We demonstrate that efficient and lasting higher-order squeezing effects could be observed with proper choice of the relative phase between the pump and coherent fields. This nonclassical feature is attributed to a constructive two-photon interference. Relationship between the second- and higher-order squeezing of the field is discussed","tok_text":"phase control of higher-ord squeez of a quantum field \n in a recent experi [ phi . rev. lett . 88 ( 2002 ) 023601 ] , phase-depend photon statist in a c.w . system ha been observ in the mix of a coher field with a two-photon sourc . their system ha the advantag over other atom transition-bas fluoresc system . in thi paper , we examin further the squeez properti of higher-ord quantum fluctuat in one of the quadratur compon of the combin field in thi system . we demonstr that effici and last higher-ord squeez effect could be observ with proper choic of the rel phase between the pump and coher field . thi nonclass featur is attribut to a construct two-photon interfer . relationship between the second- and higher-ord squeez of the field is discuss","ordered_present_kp":[0,17,40,118,273,378,653],"keyphrases":["phase control","higher-order squeezing","quantum field","phase-dependent photon statistics","atomic transition-based fluorescent systems","quantum fluctuations","two-photon interference","coherent field mixing"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"2041","title":"Application of multiprocessor systems for computation of jets","abstract":"The article describes the implementation of methods for numerical solution of gas-dynamic problems on a wide class of multiprocessor systems, conventionally characterized as \"cluster\" systems. A standard data-transfer interface - the so-called message passing interface - is used for parallelization of application algorithms among processors. Simulation of jets escaping into a low-pressure region is chosen as a computational example","tok_text":"applic of multiprocessor system for comput of jet \n the articl describ the implement of method for numer solut of gas-dynam problem on a wide class of multiprocessor system , convent character as \" cluster \" system . a standard data-transf interfac - the so-cal messag pass interfac - is use for parallel of applic algorithm among processor . simul of jet escap into a low-pressur region is chosen as a comput exampl","ordered_present_kp":[10,36,114,228,262,369],"keyphrases":["multiprocessor systems","computation of jets","gas-dynamic problems","data-transfer interface","message passing interface","low-pressure region","cluster systems"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"2004","title":"New paradigms for interactive 3D volume segmentation","abstract":"We present a new virtual reality-based interaction metaphor for semi-automatic segmentation of medical 3D volume data. The mouse-based, manual initialization of deformable surfaces in 3D represents a major bottleneck in interactive segmentation. In our multi-modal system we enhance this process with additional sensory feedback. A 3D haptic device is used to extract the centreline of a tubular structure. Based on the obtained path a cylinder with varying diameter is generated, which in turn is used as the initial guess for a deformable surface","tok_text":"new paradigm for interact 3d volum segment \n we present a new virtual reality-bas interact metaphor for semi-automat segment of medic 3d volum data . the mouse-bas , manual initi of deform surfac in 3d repres a major bottleneck in interact segment . in our multi-mod system we enhanc thi process with addit sensori feedback . a 3d haptic devic is use to extract the centrelin of a tubular structur . base on the obtain path a cylind with vari diamet is gener , which in turn is use as the initi guess for a deform surfac","ordered_present_kp":[17,82,182,231,257,307,328,381,182],"keyphrases":["interactive 3D volume segmentation","interaction metaphor","deformable surfaces","deformable surfaces","interactive segmentation","multi-modal system","sensory feedback","3D haptic device","tubular structure","virtual reality","medical image segmentation","mouse","varying diameter cylinder","deformable surface","haptic interaction"],"prmu":["P","P","P","P","P","P","P","P","P","M","M","U","R","P","R"]}
{"id":"2039","title":"An inverse problem for a model of a hierarchical structure","abstract":"We consider the inverse problem for the identification of the coefficient in a parabolic equation. The model is applied to describe the functioning of a hierarchical structure; it is also relevant for heat-conduction theory. Unique solvability of the inverse problem is proved","tok_text":"an invers problem for a model of a hierarch structur \n we consid the invers problem for the identif of the coeffici in a parabol equat . the model is appli to describ the function of a hierarch structur ; it is also relev for heat-conduct theori . uniqu solvabl of the invers problem is prove","ordered_present_kp":[3,35,121,226,248],"keyphrases":["inverse problem","hierarchical structure","parabolic equation","heat-conduction theory","unique solvability"],"prmu":["P","P","P","P","P"]}
{"id":"2081","title":"Three-dimensional optimum design of the cooling lines of injection moulds based on boundary element design sensitivity analysis","abstract":"A three-dimensional numerical simulation using the boundary element method is proposed, which can predict the cavity temperature distributions in the cooling stage of injection moulding. Then, choosing the radii and positions of cooling lines as design variables, the boundary integral sensitivity formulations are deduced. For the optimum design of cooling lines, the squared difference between the objective temperature and temperature of the cavity is taken as the objective function. Based on the optimization techniques with design sensitivity analysis, an iterative algorithm to reach the minimum value of the objective function is introduced, which leads to the optimum design of cooling lines at the same time","tok_text":"three-dimension optimum design of the cool line of inject mould base on boundari element design sensit analysi \n a three-dimension numer simul use the boundari element method is propos , which can predict the caviti temperatur distribut in the cool stage of inject mould . then , choos the radii and posit of cool line as design variabl , the boundari integr sensit formul are deduc . for the optimum design of cool line , the squar differ between the object temperatur and temperatur of the caviti is taken as the object function . base on the optim techniqu with design sensit analysi , an iter algorithm to reach the minimum valu of the object function is introduc , which lead to the optimum design of cool line at the same time","ordered_present_kp":[51,151,209,244,592,515,16],"keyphrases":["optimization","injection moulding","boundary element method","cavity temperature distributions","cooling stage","objective function","iterative algorithm","3D numerical simulation","boundary integral sensitivity analysis","heat conduction"],"prmu":["P","P","P","P","P","P","P","M","R","U"]}
{"id":"373","title":"Putting pen to screen on Tablet PCs","abstract":"With the release of the first Tablet PCs produced to Microsoft Corp.'s general specifications, handheld computers may be about to leap into the ring with today's laptops. They will be about the size of the smaller laptops, will be at least as powerful, and maybe their biggest selling point-will be able to handle handwritten text. The Tablet PCs will be amply configured, general-purpose machines with more than enough power to run the full-blown Windows XP operating system. In particular, they will allow handwritten text to be entered onto a digitizing tablet and recognized, a functionality that's called pen-based computing. The Tablet PC will far outpace the computing power of existing small devices such as PDAs (personal digital assistants), including those variants based on Microsoft's own Pocket PC operating system","tok_text":"put pen to screen on tablet pc \n with the releas of the first tablet pc produc to microsoft corp. 's gener specif , handheld comput may be about to leap into the ring with today 's laptop . they will be about the size of the smaller laptop , will be at least as power , and mayb their biggest sell point-wil be abl to handl handwritten text . the tablet pc will be ampli configur , general-purpos machin with more than enough power to run the full-blown window xp oper system . in particular , they will allow handwritten text to be enter onto a digit tablet and recogn , a function that 's call pen-bas comput . the tablet pc will far outpac the comput power of exist small devic such as pda ( person digit assist ) , includ those variant base on microsoft 's own pocket pc oper system","ordered_present_kp":[21,82,116,324,454,546,596],"keyphrases":["Tablet PC","Microsoft","handheld computers","handwritten text","Windows XP operating system","digitizing tablet","pen-based computing"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"1955","title":"Simulation of evacuation processes using a bionics-inspired cellular automaton model for pedestrian dynamics","abstract":"We present simulations of evacuation processes using a recently introduced cellular automaton model for pedestrian dynamics. This model applies a bionics approach to describe the interaction between the pedestrians using ideas from chemotaxis. Here we study a rather simple situation, namely the evacuation from a large room with one or two doors. It is shown that the variation of the model parameters allows to describe different types of behaviour, from regular to panic. We find a non-monotonic dependence of the evacuation times on the coupling constants. These times depend on the strength of the herding behaviour, with minimal evacuation times for some intermediate values of the couplings, i.e., a proper combination of herding and use of knowledge about the shortest way to the exit","tok_text":"simul of evacu process use a bionics-inspir cellular automaton model for pedestrian dynam \n we present simul of evacu process use a recent introduc cellular automaton model for pedestrian dynam . thi model appli a bionic approach to describ the interact between the pedestrian use idea from chemotaxi . here we studi a rather simpl situat , name the evacu from a larg room with one or two door . it is shown that the variat of the model paramet allow to describ differ type of behaviour , from regular to panic . we find a non-monoton depend of the evacu time on the coupl constant . these time depend on the strength of the herd behaviour , with minim evacu time for some intermedi valu of the coupl , i.e. , a proper combin of herd and use of knowledg about the shortest way to the exit","ordered_present_kp":[291,567,625,29,73],"keyphrases":["bionics-inspired cellular automaton model","pedestrian dynamics","chemotaxis","coupling constants","herding behaviour","evacuation processes simulation","nonmonotonic dependence"],"prmu":["P","P","P","P","P","R","M"]}
{"id":"2124","title":"The set of stable polynomials of linear discrete systems: its geometry","abstract":"The multidimensional stability domain of linear discrete systems is studied. Its configuration is determined from the parameters of its intersection with coordinate axes, coordinate planes, and certain auxiliary planes. Counterexamples for the discrete variant of the Kharitonov theorem are given","tok_text":"the set of stabl polynomi of linear discret system : it geometri \n the multidimension stabil domain of linear discret system is studi . it configur is determin from the paramet of it intersect with coordin axe , coordin plane , and certain auxiliari plane . counterexampl for the discret variant of the kharitonov theorem are given","ordered_present_kp":[11,303,29,56,71],"keyphrases":["stable polynomials","linear discrete systems","geometry","multidimensional stability domain","Kharitonov theorem","characteristic polynomial"],"prmu":["P","P","P","P","P","M"]}
{"id":"293","title":"Theoretical and experimental investigations on coherence of traffic noise transmission through an open window into a rectangular room in high-rise buildings","abstract":"A method for theoretically calculating the coherence between sound pressure inside a rectangular room in a high-rise building and that outside the open window of the room is proposed. The traffic noise transmitted into a room is generally dominated by low-frequency components, to which active noise control (ANC) technology may find an application. However, good coherence between reference and error signals is essential for an effective noise reduction and should be checked first. Based on traffic noise prediction methods, wave theory, and mode coupling theory, the results of this paper enabled one to determine the potentials and limitations of ANC used to reduce such a transmission. Experimental coherence results are shown for two similar, empty rectangular rooms located on the 17th and 30th floors of a 34 floor high-rise building. The calculated results with the proposed method are generally in good agreement with the experimental results and demonstrate the usefulness of the method for predicting the coherence","tok_text":"theoret and experiment investig on coher of traffic nois transmiss through an open window into a rectangular room in high-ris build \n a method for theoret calcul the coher between sound pressur insid a rectangular room in a high-ris build and that outsid the open window of the room is propos . the traffic nois transmit into a room is gener domin by low-frequ compon , to which activ nois control ( anc ) technolog may find an applic . howev , good coher between refer and error signal is essenti for an effect nois reduct and should be check first . base on traffic nois predict method , wave theori , and mode coupl theori , the result of thi paper enabl one to determin the potenti and limit of anc use to reduc such a transmiss . experiment coher result are shown for two similar , empti rectangular room locat on the 17th and 30th floor of a 34 floor high-ris build . the calcul result with the propos method are gener in good agreement with the experiment result and demonstr the use of the method for predict the coher","ordered_present_kp":[44,78,97,117,180,351,560,608,590],"keyphrases":["traffic noise transmission","open window","rectangular room","high-rise buildings","sound pressure","low-frequency components","traffic noise prediction methods","wave theory","mode coupling theory","active noise control technology"],"prmu":["P","P","P","P","P","P","P","P","P","R"]}
{"id":"2161","title":"On the Beth properties of some intuitionistic modal logics","abstract":"Let L be one of the intuitionistic modal logics. As in the classical modal case, we define two different forms of the Beth property for L, which are denoted by B1 and B2; in this paper we study the relation among B1, B2 and the interpolation properties C1 and C2. It turns out that C1 implies B1, but contrary to the boolean case, is not equivalent to B1. It is shown that B2 and C2 are independent, and moreover it comes out that, in contrast to classical case, there exists an extension of the intuitionistic modal logic of S\/sub 4\/-type, that has not the property B2. Finally we give two algebraic properties, that characterize respectively B1 and B2","tok_text":"on the beth properti of some intuitionist modal logic \n let l be one of the intuitionist modal logic . as in the classic modal case , we defin two differ form of the beth properti for l , which are denot by b1 and b2 ; in thi paper we studi the relat among b1 , b2 and the interpol properti c1 and c2 . it turn out that c1 impli b1 , but contrari to the boolean case , is not equival to b1 . it is shown that b2 and c2 are independ , and moreov it come out that , in contrast to classic case , there exist an extens of the intuitionist modal logic of s \/ sub 4\/-type , that ha not the properti b2 . final we give two algebra properti , that character respect b1 and b2","ordered_present_kp":[7,273,29],"keyphrases":["Beth properties","intuitionistic modal logics","interpolation properties"],"prmu":["P","P","P"]}
{"id":"2083","title":"Innovative manufacture of impulse turbine blades for wave energy power conversion","abstract":"An innovative approach to the manufacture of impulse turbine blades using rapid prototyping, fused decomposition modelling (FDM), is presented. These blades were designed and manufactured by the Wave Energy Research Team (WERT) at the University of Limerick for the experimental analysis of a 0.6 m impulse turbine with fixed guide vanes for wave energy power conversion. The computer aided design\/manufacture (CAD\/CAM) package Pro-Engineer 2000i was used for three-dimensional solid modelling of the individual blades. A detailed finite element analysis of the blades under centrifugal loads was performed using Pro-Mechanica. based on this analysis and FDM machine capabilities, blades were redesigned. Finally, Pro-E data were transferred to an FDM machine for the manufacture of turbine blades. The objective of this paper is to present the innovative method used to design, modify and manufacture blades in a time and cost effective manner using a concurrent engineering approach","tok_text":"innov manufactur of impuls turbin blade for wave energi power convers \n an innov approach to the manufactur of impuls turbin blade use rapid prototyp , fuse decomposit model ( fdm ) , is present . these blade were design and manufactur by the wave energi research team ( wert ) at the univers of limerick for the experiment analysi of a 0.6 m impuls turbin with fix guid vane for wave energi power convers . the comput aid design \/ manufactur ( cad \/ cam ) packag pro-engin 2000i wa use for three-dimension solid model of the individu blade . a detail finit element analysi of the blade under centrifug load wa perform use pro-mechanica . base on thi analysi and fdm machin capabl , blade were redesign . final , pro-e data were transfer to an fdm machin for the manufactur of turbin blade . the object of thi paper is to present the innov method use to design , modifi and manufactur blade in a time and cost effect manner use a concurr engin approach","ordered_present_kp":[445,20,44,152,135,6,930,285,507,552],"keyphrases":["manufacturing","impulse turbine blades","wave energy power conversion","rapid prototyping","fused decomposition modelling","University of Limerick","CAD\/CAM","solid modelling","finite element analysis","concurrent engineering"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"334","title":"Capturing niche markets with copper","abstract":"For \"last-mile access\" in niche applications, twisted copper pair may be the cable of best option to gain access and deliver desired services. The article discusses how operators can use network edge devices to serve new customers. Niche market segments represent a significant opportunity for cable TV delivery of television and high-speed Internet signals. But the existing telecommunications infrastructure in those developments frequently presents unique challenges for the service provider to overcome","tok_text":"captur nich market with copper \n for \" last-mil access \" in nich applic , twist copper pair may be the cabl of best option to gain access and deliv desir servic . the articl discuss how oper can use network edg devic to serv new custom . nich market segment repres a signific opportun for cabl tv deliveri of televis and high-spe internet signal . but the exist telecommun infrastructur in those develop frequent present uniqu challeng for the servic provid to overcom","ordered_present_kp":[39,74,199,7],"keyphrases":["niche markets","last-mile access","twisted copper pair","network edge devices","copper cables"],"prmu":["P","P","P","P","R"]}
{"id":"371","title":"A better ballot box?","abstract":"Election officials are examining technologies to address a wide range of voting issues. The problems observed in the November 2000 US election accelerated existing trends to get rid of lever machines, punch-cards, and hand-counted paper ballots and replace them with mark-sense balloting, Internet, and automatic teller machine (ATM) kiosk style computer-based systems. An estimated US $2-$4 billion will be spent in the United States and Canada to update voting systems during the next decade. Voting online might enable citizens to vote even if they are unable to get to the polls. Yet making these methods work right turns out to be considerably more difficult than originally thought. New electronic voting systems pose risks as well as solutions. As it turns out, many of the voting products currently for sale provide less accountability, poorer reliability, and greater opportunity for widespread fraud than those already in use. This paper discusses the technology available and how to ensure accurate ballots","tok_text":"a better ballot box ? \n elect offici are examin technolog to address a wide rang of vote issu . the problem observ in the novemb 2000 us elect acceler exist trend to get rid of lever machin , punch-card , and hand-count paper ballot and replac them with mark-sens ballot , internet , and automat teller machin ( atm ) kiosk style computer-bas system . an estim us $ 2-$4 billion will be spent in the unit state and canada to updat vote system dure the next decad . vote onlin might enabl citizen to vote even if they are unabl to get to the poll . yet make these method work right turn out to be consider more difficult than origin thought . new electron vote system pose risk as well as solut . as it turn out , mani of the vote product current for sale provid less account , poorer reliabl , and greater opportun for widespread fraud than those alreadi in use . thi paper discuss the technolog avail and how to ensur accur ballot","ordered_present_kp":[9,254,646],"keyphrases":["ballot box","mark-sense balloting","electronic voting","automatic teller machine computer-based voting system","ATM kiosk style computer-based voting systems","online voting"],"prmu":["P","P","P","R","R","R"]}
{"id":"2126","title":"A nonlinear time-optimal control problem","abstract":"Sufficient conditions for the existence of an optimal control in a time-optimal control problem with fixed ends for a smooth nonlinear control system are formulated. The properties of this system for characterizing the optimal control switching points are studied","tok_text":"a nonlinear time-optim control problem \n suffici condit for the exist of an optim control in a time-optim control problem with fix end for a smooth nonlinear control system are formul . the properti of thi system for character the optim control switch point are studi","ordered_present_kp":[2,141,231],"keyphrases":["nonlinear time-optimal control problem","smooth nonlinear control system","optimal control switching points","sufficient existence conditions"],"prmu":["P","P","P","R"]}
{"id":"291","title":"Nuclear magnetic resonance molecular photography","abstract":"A procedure is described for storing a two-dimensional (2D) pattern consisting of 32*32=1024 bits in a spin state of a molecular system and then retrieving the stored information as a stack of nuclear magnetic resonance spectra. The system used is a nematic liquid crystal, the protons of which act as spin clusters with strong intramolecular interactions. The technique used is a programmable multifrequency irradiation with low amplitude. When it is applied to the liquid crystal, a large number of coherent long-lived \/sup 1\/H response signals can be excited, resulting in a spectrum showing many sharp peaks with controllable frequencies and amplitudes. The spectral resolution is enhanced by using a second weak pulse with a 90 degrees phase shift, so that the 1024 bits of information can be retrieved as a set of well-resolved pseudo-2D spectra reproducing the input pattern","tok_text":"nuclear magnet reson molecular photographi \n a procedur is describ for store a two-dimension ( 2d ) pattern consist of 32 * 32=1024 bit in a spin state of a molecular system and then retriev the store inform as a stack of nuclear magnet reson spectra . the system use is a nemat liquid crystal , the proton of which act as spin cluster with strong intramolecular interact . the techniqu use is a programm multifrequ irradi with low amplitud . when it is appli to the liquid crystal , a larg number of coher long-liv \/sup 1 \/ h respons signal can be excit , result in a spectrum show mani sharp peak with control frequenc and amplitud . the spectral resolut is enhanc by use a second weak puls with a 90 degre phase shift , so that the 1024 bit of inform can be retriev as a set of well-resolv pseudo-2d spectra reproduc the input pattern","ordered_present_kp":[273,323,341,396,428,501,640,676,793,127],"keyphrases":["1024 bit","nematic liquid crystal","spin clusters","strong intramolecular interactions","programmable multifrequency irradiation","low amplitude","coherent long-lived \/sup 1\/H response signals","spectral resolution","second weak pulse","pseudo-2D spectra","NMR molecular photography","2D pattern","molecular system spin state","information storage","spin echoes","Hilbert spaces","high-content molecular information processing","coupled spins","dipole-dipole interactions","spin-locking","proton spin","spin dynamics"],"prmu":["P","P","P","P","P","P","P","P","P","P","M","R","R","M","M","U","M","M","M","U","R","M"]}
{"id":"2163","title":"IT as a key enabler to law firm competitiveness","abstract":"Professional services firms have traditionally been able to thrive in virtually any market conditions. They have been consistently successful for several decades without ever needing to reexamine or change their basic operating model. However, gradual but inexorable change in client expectations and the business environment over recent years now means that more of the same is no longer enough. In future, law firms will increasingly need to exploit IT more effectively in order to remain competitive. To do this, they will need to ensure that all their information systems function as an integrated whole and are available to their staff, clients and business partners. The authors set out the lessons to be learned for law firms in the light of the recent PA Consulting survey","tok_text":"it as a key enabl to law firm competit \n profession servic firm have tradit been abl to thrive in virtual ani market condit . they have been consist success for sever decad without ever need to reexamin or chang their basic oper model . howev , gradual but inexor chang in client expect and the busi environ over recent year now mean that more of the same is no longer enough . in futur , law firm will increasingli need to exploit it more effect in order to remain competit . to do thi , they will need to ensur that all their inform system function as an integr whole and are avail to their staff , client and busi partner . the author set out the lesson to be learn for law firm in the light of the recent pa consult survey","ordered_present_kp":[41,273,295,528,21],"keyphrases":["law firms","professional services firms","client expectations","business environment","information systems"],"prmu":["P","P","P","P","P"]}
{"id":"1957","title":"The two populations' cellular automata model with predation based on the Penna model","abstract":"In Penna's (1995) single-species asexual bit-string model of biological ageing, the Verhulst factor has too strong a restraining effect on the development of the population. Danuta Makowiec gave an improved model based on the lattice, where the restraining factor of the four neighbours take the place of the Verhulst factor. Here, we discuss the two populations' Penna model with predation on the planar lattice of two dimensions. A cellular automata model containing movable wolves and sheep has been built. The results show that both the quantity of the wolves and the sheep fluctuate in accordance with the law that one quantity increases while the other one decreases","tok_text":"the two popul ' cellular automata model with predat base on the penna model \n in penna 's ( 1995 ) single-speci asexu bit-str model of biolog age , the verhulst factor ha too strong a restrain effect on the develop of the popul . danuta makowiec gave an improv model base on the lattic , where the restrain factor of the four neighbour take the place of the verhulst factor . here , we discuss the two popul ' penna model with predat on the planar lattic of two dimens . a cellular automata model contain movabl wolv and sheep ha been built . the result show that both the quantiti of the wolv and the sheep fluctuat in accord with the law that one quantiti increas while the other one decreas","ordered_present_kp":[16,8,64,99,135,152,184,279,512,521,608,45],"keyphrases":["population","cellular automata model","predation","Penna model","single-species asexual bit-string model","biological ageing","Verhulst factor","restraining effect","lattice","wolves","sheep","fluctuation","Lotka-Volterra model"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"229","title":"Simple minds [health care IT]","abstract":"A few things done properly, and soon, is the short-term strategy for the UK NHS IT programme. Can it deliver this time?","tok_text":"simpl mind [ health care it ] \n a few thing done properli , and soon , is the short-term strategi for the uk nh it programm . can it deliv thi time ?","ordered_present_kp":[106,13,89],"keyphrases":["health care","strategy","UK NHS IT programme"],"prmu":["P","P","P"]}
{"id":"251","title":"Central hub for design assets: Adobe GoLive 6.0","abstract":"Adobe GoLive is a strong contender for Web authoring and publishing. Version 6.0 features a flexible GUI environment combined with a comprehensive workgroup and collaboration server, plus tight integration with leading design tools","tok_text":"central hub for design asset : adob goliv 6.0 \n adob goliv is a strong contend for web author and publish . version 6.0 featur a flexibl gui environ combin with a comprehens workgroup and collabor server , plu tight integr with lead design tool","ordered_present_kp":[31,83,137,188],"keyphrases":["Adobe GoLive 6.0","Web authoring","GUI","collaboration server","Flash","Real","Java","application servers","workgroup server","LiveMotion 2.0","animation and scripting tool","Macromedia SWF format","workgroup environment","Web publishing environment","design-centric dynamic content"],"prmu":["P","P","P","P","U","U","U","M","R","U","M","U","R","R","U"]}
{"id":"214","title":"Evolution of the high-end computing market in the USA","abstract":"This paper focuses on the technological change in the high-end computing market. The discussion combines historical analysis with strategic analysis to provide a framework to analyse a key component of the computer industry. This analysis begins from the perspective of government research and development spending; then examines the confusion around the evolution of the high-end computing market in the context of standard theories of technology strategy and new product innovation. Rather than the high-end market being 'dead', one should view the market as changing due to increased capability and competition from the low-end personal computer market. The high-end market is also responding to new product innovation from the introduction of new parallel computing architectures. In the conclusion, key leverage points in the market are identified and the trends in high-end computing are highlighted with implications","tok_text":"evolut of the high-end comput market in the usa \n thi paper focus on the technolog chang in the high-end comput market . the discuss combin histor analysi with strateg analysi to provid a framework to analys a key compon of the comput industri . thi analysi begin from the perspect of govern research and develop spend ; then examin the confus around the evolut of the high-end comput market in the context of standard theori of technolog strategi and new product innov . rather than the high-end market be ' dead ' , one should view the market as chang due to increas capabl and competit from the low-end person comput market . the high-end market is also respond to new product innov from the introduct of new parallel comput architectur . in the conclus , key leverag point in the market are identifi and the trend in high-end comput are highlight with implic","ordered_present_kp":[44,140,160,228,285,305,429,452,580,598,712],"keyphrases":["USA","historical analysis","strategic analysis","computer industry","government research","development spending","technology strategy","new product innovation","competition","low-end personal computer market","parallel computing architectures","high-end computing market evolution","supercomputing"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","U"]}
{"id":"1997","title":"Exact controllability of shells in minimal time","abstract":"We prove an exact controllability result for thin cups using the Fourier method and recent improvements of Ingham (1936) type theorems","tok_text":"exact control of shell in minim time \n we prove an exact control result for thin cup use the fourier method and recent improv of ingham ( 1936 ) type theorem","ordered_present_kp":[6,17,26,76,93],"keyphrases":["controllability","shells","minimal time","thin cups","Fourier method","partial differential equations","Young modulus","Hilbert space","Ingham type theorems"],"prmu":["P","P","P","P","P","U","U","U","R"]}
{"id":"2043","title":"Limits for computational electromagnetics codes imposed by computer architecture","abstract":"The algorithmic complexity of the innermost loops that determine the complexity of algorithms in computational electromagnetics (CEM) codes are analyzed according to their operation count and the impact of the underlying computer hardware. As memory chips are much slower than arithmetic processors, codes that involve a high data movement compared to the number of arithmetic operations are executed comparatively slower. Hence, matrix-matrix multiplications are much faster than matrix-vector multiplications. It is seen that it is not sufficient to compare only the complexity, but also the actual performance of algorithms to judge on faster execution. Implications involve FDTD loops, LU factorizations, and iterative solvers for dense matrices. Run times on two reference platforms, namely an Athlon 900 MHz and an HP PA 8600 processor, verify the findings","tok_text":"limit for comput electromagnet code impos by comput architectur \n the algorithm complex of the innermost loop that determin the complex of algorithm in comput electromagnet ( cem ) code are analyz accord to their oper count and the impact of the underli comput hardwar . as memori chip are much slower than arithmet processor , code that involv a high data movement compar to the number of arithmet oper are execut compar slower . henc , matrix-matrix multipl are much faster than matrix-vector multipl . it is seen that it is not suffici to compar onli the complex , but also the actual perform of algorithm to judg on faster execut . implic involv fdtd loop , lu factor , and iter solver for dens matric . run time on two refer platform , name an athlon 900 mhz and an hp pa 8600 processor , verifi the find","ordered_present_kp":[10,45,70,95,213,254,274,352,438,481,650,662,678,694],"keyphrases":["computational electromagnetics codes","computer architecture","algorithmic complexity","innermost loops","operation count","computer hardware","memory chips","data movement","matrix-matrix multiplications","matrix-vector multiplications","FDTD loops","LU factorizations","iterative solvers","dense matrices","CEM codes"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2006","title":"Lung metastasis detection and visualization on CT images: a knowledge-based method","abstract":"A solution to the problem of lung metastasis detection on computed tomography (CT) scans of the thorax is presented. A knowledge-based top-down approach for image interpretation is used. The method is inspired by the manner in which a radiologist and radiotherapist interpret CT images before radiotherapy is planned. A two-dimensional followed by a three-dimensional analysis is performed. The algorithm first detects the thorax contour, the lungs and the ribs, which further help the detection of metastases. Thus, two types of tumors are detected: nodules and metastases located at the lung extremities. A method to visualize the anatomical structures segmented is also presented. The system was tested on 20 patients (988 total images) from the Oncology Department of La Chaux-de-Fonds Hospital and the results show that the method is reliable as a computer-aided diagnostic tool for clinical purpose in an oncology department","tok_text":"lung metastasi detect and visual on ct imag : a knowledge-bas method \n a solut to the problem of lung metastasi detect on comput tomographi ( ct ) scan of the thorax is present . a knowledge-bas top-down approach for imag interpret is use . the method is inspir by the manner in which a radiologist and radiotherapist interpret ct imag befor radiotherapi is plan . a two-dimension follow by a three-dimension analysi is perform . the algorithm first detect the thorax contour , the lung and the rib , which further help the detect of metastas . thu , two type of tumor are detect : nodul and metastas locat at the lung extrem . a method to visual the anatom structur segment is also present . the system wa test on 20 patient ( 988 total imag ) from the oncolog depart of la chaux-de-fond hospit and the result show that the method is reliabl as a computer-aid diagnost tool for clinic purpos in an oncolog depart","ordered_present_kp":[0,36,122,181,393,848,754,159,217],"keyphrases":["lung metastasis detection","CT images","computed tomography","thorax","knowledge-based top-down approach","image interpretation","three-dimensional analysis","oncology","computer-aided diagnostic tool","data visualization","two-dimensional analysis","medical imaging","knowledge representation"],"prmu":["P","P","P","P","P","P","P","P","P","M","R","M","U"]}
{"id":"309","title":"WEXTOR: a Web-based tool for generating and visualizing experimental designs and procedures","abstract":"WEXTOR is a Javascript-based experiment generator and teaching tool on the World Wide Web that can be used to design laboratory and Web experiments in a guided step-by-step process. It dynamically creates the customized Web pages and Javascripts needed for the experimental procedure and provides experimenters with a print-ready visual display of their experimental design. WEXTOR flexibly supports complete and incomplete factorial designs with between-subjects, within-subjects, and quasi-experimental factors, as well as mixed designs. The software implements client-side response time measurement and contains a content wizard for creating interactive materials, as well as dependent measures (graphical scales, multiple-choice items, etc.), on the experiment pages. However, it does not aim to replace a full-fledged HTML editor. Several methodological features specifically needed in Web experimental design have been implemented in the Web-based tool and are described in this paper. WEXTOR is platform independent. The created Web pages can be uploaded to any type of Web server in which data may be recorded in logfiles or via a database. The current version of WEXTOR is freely available for educational and noncommercial purposes. Its Web address is http:\/\/www.genpsylab.unizh.ch\/wextor\/index.html","tok_text":"wextor : a web-bas tool for gener and visual experiment design and procedur \n wextor is a javascript-bas experi gener and teach tool on the world wide web that can be use to design laboratori and web experi in a guid step-by-step process . it dynam creat the custom web page and javascript need for the experiment procedur and provid experiment with a print-readi visual display of their experiment design . wextor flexibl support complet and incomplet factori design with between-subject , within-subject , and quasi-experiment factor , as well as mix design . the softwar implement client-sid respons time measur and contain a content wizard for creat interact materi , as well as depend measur ( graphic scale , multiple-choic item , etc . ) , on the experi page . howev , it doe not aim to replac a full-fledg html editor . sever methodolog featur specif need in web experiment design have been implement in the web-bas tool and are describ in thi paper . wextor is platform independ . the creat web page can be upload to ani type of web server in which data may be record in logfil or via a databas . the current version of wextor is freeli avail for educ and noncommerci purpos . it web address is http:\/\/www.genpsylab.unizh.ch\/wextor\/index.html","ordered_present_kp":[0,11,90,122,140,259,352,453,584,629,814,1038,1080,1096],"keyphrases":["WEXTOR","Web-based tool","Javascript-based experiment generator","teaching tool","World Wide Web","customized Web pages","print-ready visual display","factorial designs","client-side response time measurement","content wizard","HTML","Web server","logfiles","database","experimental design visualization","free software"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","M"]}
{"id":"288","title":"Complexity transitions in global algorithms for sparse linear systems over finite fields","abstract":"We study the computational complexity of a very basic problem, namely that of finding solutions to a very large set of random linear equations in a finite Galois field modulo q. Using tools from statistical mechanics we are able to identify phase transitions in the structure of the solution space and to connect them to the changes in the performance of a global algorithm, namely Gaussian elimination. Crossing phase boundaries produces a dramatic increase in memory and CPU requirements necessary for the algorithms. In turn, this causes the saturation of the upper bounds for the running time. We illustrate the results on the specific problem of integer factorization, which is of central interest for deciphering messages encrypted with the RSA cryptosystem","tok_text":"complex transit in global algorithm for spars linear system over finit field \n we studi the comput complex of a veri basic problem , name that of find solut to a veri larg set of random linear equat in a finit galoi field modulo q. use tool from statist mechan we are abl to identifi phase transit in the structur of the solut space and to connect them to the chang in the perform of a global algorithm , name gaussian elimin . cross phase boundari produc a dramat increas in memori and cpu requir necessari for the algorithm . in turn , thi caus the satur of the upper bound for the run time . we illustr the result on the specif problem of integ factor , which is of central interest for deciph messag encrypt with the rsa cryptosystem","ordered_present_kp":[0,19,40,65,179,204,246,410,434,642,704,721],"keyphrases":["complexity transitions","global algorithms","sparse linear systems","finite fields","random linear equations","finite Galois field","statistical mechanics","Gaussian elimination","phase boundaries","integer factorization","encryption","RSA cryptosystem","message deciphering","disordered systems"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","R","M"]}
{"id":"26","title":"Learning weights for the quasi-weighted means","abstract":"We study the determination of weights for quasi-weighted means (also called quasi-linear means) when a set of examples is given. We consider first a simple case, the learning of weights for weighted means, and then we extend the approach to the more general case of a quasi-weighted mean. We consider the case of a known arbitrary generator f. The paper finishes considering the use of parametric functions that are suitable when the values to aggregate are measure values or ratio","tok_text":"learn weight for the quasi-weight mean \n we studi the determin of weight for quasi-weight mean ( also call quasi-linear mean ) when a set of exampl is given . we consid first a simpl case , the learn of weight for weight mean , and then we extend the approach to the more gener case of a quasi-weight mean . we consid the case of a known arbitrari gener f. the paper finish consid the use of parametr function that are suitabl when the valu to aggreg are measur valu or ratio","ordered_present_kp":[21,107,0,392,455],"keyphrases":["learning","quasi-weighted means","quasi-linear means","parametric functions","measure values","ratio values"],"prmu":["P","P","P","P","P","R"]}
{"id":"275","title":"Prediction of ultraviolet spectral absorbance using quantitative structure-property relationships","abstract":"High performance liquid chromatography (HPLC) with ultraviolet (UV) spectrophotometric detection is a common method for analyzing reaction products in organic chemistry. This procedure would benefit from a computational model for predicting the relative response of organic molecules. Models are now reported for the prediction of the integrated UV absorbance for a diverse set of organic compounds using a quantitative structure-property relationship (QSPR) approach. A seven-descriptor linear correlation with a squared correlation coefficient (R\/sup 2\/) of 0.815 is reported for a data set of 521.compounds. Using the sum of ZINDO oscillator strengths in the integration range as an additional descriptor allowed reduction in the number of descriptors producing a robust model for 460 compounds with five descriptors and a squared correlation coefficient 0.857. The descriptors used in the models are discussed with respect to the physical nature of the UV absorption process","tok_text":"predict of ultraviolet spectral absorb use quantit structure-properti relationship \n high perform liquid chromatographi ( hplc ) with ultraviolet ( uv ) spectrophotometr detect is a common method for analyz reaction product in organ chemistri . thi procedur would benefit from a comput model for predict the rel respons of organ molecul . model are now report for the predict of the integr uv absorb for a divers set of organ compound use a quantit structure-properti relationship ( qspr ) approach . a seven-descriptor linear correl with a squar correl coeffici ( r \/ sup 2\/ ) of 0.815 is report for a data set of 521.compound . use the sum of zindo oscil strength in the integr rang as an addit descriptor allow reduct in the number of descriptor produc a robust model for 460 compound with five descriptor and a squar correl coeffici 0.857 . the descriptor use in the model are discuss with respect to the physic natur of the uv absorpt process","ordered_present_kp":[43,85,207,227,279,308,503,541,645],"keyphrases":["quantitative structure-property relationship","high performance liquid chromatography","reaction products","organic chemistry","computational model","relative response","seven-descriptor linear correlation","squared correlation coefficient","ZINDO oscillator strengths","ultraviolet spectral absorbance prediction","ultraviolet spectrophotometric detection","combinatorial chemistry","generic quantitation","configuration interaction calculation","CODESSA program","MOS-F package"],"prmu":["P","P","P","P","P","P","P","P","P","R","R","M","M","U","U","U"]}
{"id":"2187","title":"Variable structure intelligent control for PM synchronous servo motor drive","abstract":"The variable structure control (VSC) of discrete time systems based on intelligent control is presented in this paper. A novel approach is proposed for the state estimation. A linear observer is firstly designed. Then a neural network is used for compensating uncertainty. The parameter of the VSC scheme is adjusted online by a neural network. Practical operating results from a PM synchronous motor (PMSM) illustrate the effectiveness and practicability of the proposed approach","tok_text":"variabl structur intellig control for pm synchron servo motor drive \n the variabl structur control ( vsc ) of discret time system base on intellig control is present in thi paper . a novel approach is propos for the state estim . a linear observ is firstli design . then a neural network is use for compens uncertainti . the paramet of the vsc scheme is adjust onlin by a neural network . practic oper result from a pm synchron motor ( pmsm ) illustr the effect and practic of the propos approach","ordered_present_kp":[38,0,110,216,232,273],"keyphrases":["variable structure intelligent control","PM synchronous servo motor drive","discrete time systems","state estimation","linear observer","neural network","control design","uncertainty compensation","control performance"],"prmu":["P","P","P","P","P","P","R","R","M"]}
{"id":"230","title":"2002 in-house fulfillment systems report [publishing]","abstract":"CM's 13th annual survey of in-house fulfillment system suppliers brings you up to date on the current capabilities of the leading publication software packages","tok_text":"2002 in-hous fulfil system report [ publish ] \n cm 's 13th annual survey of in-hous fulfil system supplier bring you up to date on the current capabl of the lead public softwar packag","ordered_present_kp":[66,5,98,162],"keyphrases":["in-house fulfillment system","survey","suppliers","publication software packages"],"prmu":["P","P","P","P"]}
{"id":"368","title":"From a biological to a computational model for the autonomous behavior of an animat","abstract":"Endowing an autonomous system like a robot with intelligent behavior is difficult for several reasons. First, behavior is such a wide topic that a general framework paradigm of inspiration must be chosen in order to obtain a consistent model. Such a framework can be, for example, biological modeling or an artificial intelligence approach. Second, a general framework is not sufficient to determine a fully specified program to be implemented in a robot. Many choices, tuning and tests must be carried out before obtaining a robust system. A biological model is presented, based on the definition of cortex-like automata, representing elementary functions in the perceptive, motor or associative domain. These automata are connected in a network whose architecture, functioning and learning rules are described in a cortical framework. Second, the computational model derived from that biological model is specified. The way units exchange and compute variables through links is explained, with reference to corresponding biological elements. It is then easier to report experiments allowing an autonomous system to learn regularities of a simple environment and to exploit them to satisfy some internal drives. Even if additional biological hints can be added, this model allow us to better understand how a biological model can be implemented and how biological properties can emerge from a distributed set of units","tok_text":"from a biolog to a comput model for the autonom behavior of an animat \n endow an autonom system like a robot with intellig behavior is difficult for sever reason . first , behavior is such a wide topic that a gener framework paradigm of inspir must be chosen in order to obtain a consist model . such a framework can be , for exampl , biolog model or an artifici intellig approach . second , a gener framework is not suffici to determin a fulli specifi program to be implement in a robot . mani choic , tune and test must be carri out befor obtain a robust system . a biolog model is present , base on the definit of cortex-lik automata , repres elementari function in the percept , motor or associ domain . these automata are connect in a network whose architectur , function and learn rule are describ in a cortic framework . second , the comput model deriv from that biolog model is specifi . the way unit exchang and comput variabl through link is explain , with refer to correspond biolog element . it is then easier to report experi allow an autonom system to learn regular of a simpl environ and to exploit them to satisfi some intern drive . even if addit biolog hint can be ad , thi model allow us to better understand how a biolog model can be implement and how biolog properti can emerg from a distribut set of unit","ordered_present_kp":[81,103,40,114,63,503,512,550,335,617,646,692,781,754,19,944,1085,1135],"keyphrases":["computational model","autonomous behavior","animat","autonomous system","robot","intelligent behavior","biological model","tuning","tests","robust system","cortex-like automata","elementary functions","associative domain","architecture","learning rules","links","simple environment","internal drives","perceptive domain","motor domain","variable computation","variable exchange","regularity learning"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R","R","R"]}
{"id":"400","title":"A 120-mW 3-D rendering engine with 6-Mb embedded DRAM and 3.2-GB\/s runtime reconfigurable bus for PDA chip","abstract":"A low-power three-dimensional (3-D) rendering engine is implemented as part of a mobile personal digital assistant (PDA) chip. Six-megabit embedded DRAM macros attached to 8-pixel-parallel rendering logic are logically localized with a 3.2-GB\/s runtime reconfigurable bus, reducing the area by 25% compared with conventional local frame-buffer architectures. The low power consumption is achieved by polygon-dependent access to the embedded DRAM macros with line-block mapping providing read-modify-write data transaction. The 3-D rendering engine with 2.22-Mpolygons\/s drawing speed was fabricated using 0.18- mu m CMOS embedded memory logic technology. Its area is 24 mm\/sup 2\/ and its power consumption is 120 mW","tok_text":"a 120-mw 3-d render engin with 6-mb embed dram and 3.2-gb \/ s runtim reconfigur bu for pda chip \n a low-pow three-dimension ( 3-d ) render engin is implement as part of a mobil person digit assist ( pda ) chip . six-megabit embed dram macro attach to 8-pixel-parallel render logic are logic local with a 3.2-gb \/ s runtim reconfigur bu , reduc the area by 25 % compar with convent local frame-buff architectur . the low power consumpt is achiev by polygon-depend access to the embed dram macro with line-block map provid read-modify-writ data transact . the 3-d render engin with 2.22-mpolygon \/ s draw speed wa fabric use 0.18- mu m cmo embed memori logic technolog . it area is 24 mm \/ sup 2\/ and it power consumpt is 120 mw","ordered_present_kp":[224,251,69,416,448,499,521,634,720],"keyphrases":["reconfigurable bus","embedded DRAM macros","8-pixel-parallel rendering logic","low power consumption","polygon-dependent access","line-block mapping","read-modify-write data transaction","CMOS embedded memory logic technology","120 mW","low-power 3D rendering engine","three-dimensional rendering engine","mobile PDA chip","mobile personal digital assistant chip","3D graphics rendering","6 Mbit","3.2 GB\/s","0.18 micron"],"prmu":["P","P","P","P","P","P","P","P","P","M","R","R","R","M","U","M","U"]}
{"id":"2067","title":"A comparison of the discounted utility model and hyperbolic discounting models in the case of social and private intertemporal preferences for health","abstract":"Whilst there is substantial evidence that hyperbolic discounting models describe intertemporal preferences for monetary outcomes better than the discounted utility (DU) model, there is only very limited evidence in the context of health outcomes. This study elicits private and social intertemporal preferences for non-fatal changes in health. Specific functional forms of the DU model and three hyperbolic models are fitted. The results show that the stationarity axiom is violated, and that the hyperbolic models fit the data better than the DU model. Intertemporal preferences for private and social decisions are found to be very similar","tok_text":"a comparison of the discount util model and hyperbol discount model in the case of social and privat intertempor prefer for health \n whilst there is substanti evid that hyperbol discount model describ intertempor prefer for monetari outcom better than the discount util ( du ) model , there is onli veri limit evid in the context of health outcom . thi studi elicit privat and social intertempor prefer for non-fat chang in health . specif function form of the du model and three hyperbol model are fit . the result show that the stationar axiom is violat , and that the hyperbol model fit the data better than the du model . intertempor prefer for privat and social decis are found to be veri similar","ordered_present_kp":[20,44,101,333,660],"keyphrases":["discounted utility model","hyperbolic discounting models","intertemporal preferences","health outcomes","social decisions","private decisions"],"prmu":["P","P","P","P","P","R"]}
{"id":"2022","title":"Two-step integral imaging for orthoscopic three-dimensional imaging with improved viewing resolution","abstract":"We present a two-step integral imaging system to obtain 3-D orthoscopic real images. By adopting a nonstationary micro-optics technique, we demonstrate experimentally the potential usefulness of two-step integral imaging","tok_text":"two-step integr imag for orthoscop three-dimension imag with improv view resolut \n we present a two-step integr imag system to obtain 3-d orthoscop real imag . by adopt a nonstationari micro-opt techniqu , we demonstr experiment the potenti use of two-step integr imag","ordered_present_kp":[0,96,134,171],"keyphrases":["two-step integral imaging","two-step integral imaging system","3-D orthoscopic real images","nonstationary micro-optics technique","resolution improved viewing","3-D image reconstruction","liquid crystal light valve","display device","LCLV","pickup lenslet array"],"prmu":["P","P","P","P","R","M","U","U","U","U"]}
{"id":"395","title":"Conformal-mapping design tools for coaxial couplers with complex cross section","abstract":"Numerical conformal mapping is exploited as a simple, accurate, and efficient tool for the analysis and design of coaxial waveguides and couplers of complex cross section. An implementation based on the Schwarz-Christoffel Toolbox, a public-domain MATLAB package, is applied to slotted coaxial cables and to symmetrical coaxial couplers, with circular or polygonal inner conductors and external shields. The effect of metallic diaphragms of arbitrary thickness, partially separating the inner conductors, is also easily taken into account. The proposed technique is validated against the results of the finite-element method, showing excellent agreement at a fraction of the computational cost, and is also extended to the case of nonsymmetrical couplers, providing the designer with important additional degrees of freedom","tok_text":"conformal-map design tool for coaxial coupler with complex cross section \n numer conform map is exploit as a simpl , accur , and effici tool for the analysi and design of coaxial waveguid and coupler of complex cross section . an implement base on the schwarz-christoffel toolbox , a public-domain matlab packag , is appli to slot coaxial cabl and to symmetr coaxial coupler , with circular or polygon inner conductor and extern shield . the effect of metal diaphragm of arbitrari thick , partial separ the inner conductor , is also easili taken into account . the propos techniqu is valid against the result of the finite-el method , show excel agreement at a fraction of the comput cost , and is also extend to the case of nonsymmetr coupler , provid the design with import addit degre of freedom","ordered_present_kp":[30,51,171,252,284,326,394,422,452,725],"keyphrases":["coaxial couplers","complex cross section","coaxial waveguides","Schwarz-Christoffel Toolbox","public-domain MATLAB package","slotted coaxial cables","polygonal inner conductors","external shields","metallic diaphragms","nonsymmetrical couplers","conformal mapping design tools","symmetrical couplers","circular inner conductors","numerical conformal transformations"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R","R","M"]}
{"id":"310","title":"ePsych: interactive demonstrations and experiments in psychology","abstract":"ePsych (http:\/\/epsych.msstate.edu), a new Web site currently under active development, is intended to teach students about the discipline of psychology. The site presumes little prior knowledge about the field and so may be used in introductory classes, but it incorporates sufficient depth of coverage to be useful in more advanced classes as well. Numerous interactive and dynamic elements are incorporated into various modules, orientations, and guidebooks. These elements include Java-based experiments and demonstrations, video clips, and animated diagrams. Rapid access to all material is provided through a layer-based navigation system that allows users to visit various \"Worlds of the Mind.\" Active learning is encouraged, by challenging students with puzzles and problems and by providing the opportunity to \"dig deeper\" to learn more about the phenomena at hand","tok_text":"epsych : interact demonstr and experi in psycholog \n epsych ( http:\/\/epsych.msstate.edu ) , a new web site current under activ develop , is intend to teach student about the disciplin of psycholog . the site presum littl prior knowledg about the field and so may be use in introductori class , but it incorpor suffici depth of coverag to be use in more advanc class as well . numer interact and dynam element are incorpor into variou modul , orient , and guidebook . these element includ java-bas experi and demonstr , video clip , and anim diagram . rapid access to all materi is provid through a layer-bas navig system that allow user to visit variou \" world of the mind . \" activ learn is encourag , by challeng student with puzzl and problem and by provid the opportun to \" dig deeper \" to learn more about the phenomena at hand","ordered_present_kp":[0,9,98,150,488,519,536,598,655,677],"keyphrases":["ePsych","interactive demonstrations","Web site","teaching","Java-based experiments","video clips","animated diagrams","layer-based navigation system","Worlds of the Mind","active learning","psychology experiments"],"prmu":["P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"355","title":"An identity-based society oriented signature scheme with anonymous signers","abstract":"In this paper, we propose a new society oriented scheme, based on the Guillou-Quisquater (1989) signature scheme. The scheme is identity-based and the signatures are verified with respect to only one identity. That is, the verifier does not have to know the identity of the co-signers, but just that of the organization they represent","tok_text":"an identity-bas societi orient signatur scheme with anonym signer \n in thi paper , we propos a new societi orient scheme , base on the guillou-quisquat ( 1989 ) signatur scheme . the scheme is identity-bas and the signatur are verifi with respect to onli one ident . that is , the verifi doe not have to know the ident of the co-sign , but just that of the organ they repres","ordered_present_kp":[3,52],"keyphrases":["identity-based society oriented signature scheme","anonymous signers","signature verification"],"prmu":["P","P","M"]}
{"id":"248","title":"Universal dynamic synchronous self-stabilization","abstract":"We prove the existence of a \"universal\" synchronous self-stabilizing protocol, that is, a protocol that allows a distributed system to stabilize to a desired nonreactive behaviour (as long as a protocol stabilizing to that behaviour exists). Previous proposals required drastic increases in asymmetry and knowledge to work, whereas our protocol does not use any additional knowledge, and does not require more symmetry-breaking conditions than available; thus, it is also stabilizing with respect to dynamic changes in the topology. We prove an optimal quiescence time n + D for a synchronous network of n processors and diameter D; the protocol can be made finite state with a negligible loss in quiescence time. Moreover, an optimal D + 1 protocol is given for the case of unique identifiers. As a consequence, we provide an effective proof technique that allows one to show whether self-stabilization to a certain behaviour is possible under a wide range of models","tok_text":"univers dynam synchron self-stabil \n we prove the exist of a \" univers \" synchron self-stabil protocol , that is , a protocol that allow a distribut system to stabil to a desir nonreact behaviour ( as long as a protocol stabil to that behaviour exist ) . previou propos requir drastic increas in asymmetri and knowledg to work , wherea our protocol doe not use ani addit knowledg , and doe not requir more symmetry-break condit than avail ; thu , it is also stabil with respect to dynam chang in the topolog . we prove an optim quiescenc time n + d for a synchron network of n processor and diamet d ; the protocol can be made finit state with a neglig loss in quiescenc time . moreov , an optim d + 1 protocol is given for the case of uniqu identifi . as a consequ , we provid an effect proof techniqu that allow one to show whether self-stabil to a certain behaviour is possibl under a wide rang of model","ordered_present_kp":[0,73,139,177,500,481,522,555,627,528,736,788,23],"keyphrases":["universal dynamic synchronous self-stabilization","self-stabilization","synchronous self-stabilizing protocol","distributed system","nonreactive behaviour","dynamic changes","topology","optimal quiescence time","quiescence time","synchronous network","finite state","unique identifiers","proof technique","optimal protocol","anonymous networks","graph fibrations"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R","M","U"]}
{"id":"1936","title":"A new approach to the decomposition of Boolean functions by the method of q-partitions.II. Repeated decomposition","abstract":"For pt.I. see Upr. Sist. Mash., no. 6, p. 29-42 (1999). A new approach to the decomposition of Boolean,functions that depend on n variables and are represented in various forms is considered. The approach is based on the method of q-partitioning of minterms and on the introduced concept of a decomposition clone. The theorem on simple disjunctive decomposition of full and partial functions is formulated. The approach proposed is illustrated by examples","tok_text":"a new approach to the decomposit of boolean function by the method of q-partit . ii . repeat decomposit \n for pt . i. see upr . sist . mash . , no . 6 , p. 29 - 42 ( 1999 ) . a new approach to the decomposit of boolean , function that depend on n variabl and are repres in variou form is consid . the approach is base on the method of q-partit of minterm and on the introduc concept of a decomposit clone . the theorem on simpl disjunct decomposit of full and partial function is formul . the approach propos is illustr by exampl","ordered_present_kp":[347,388,428,460,70],"keyphrases":["q-partitions","minterms","decomposition clone","disjunctive decomposition","partial functions","Boolean functions decomposition","logic synthesis"],"prmu":["P","P","P","P","P","R","U"]}
{"id":"1973","title":"Affine invariants of convex polygons","abstract":"In this correspondence, we prove that the affine invariants, for image registration and object recognition, proposed recently by Yang and Cohen (see ibid., vol.8, no.7, p.934-46, July 1999) are algebraically dependent. We show how to select an independent and complete set of the invariants. The use of this new set leads to a significant reduction of the computing complexity without decreasing the discrimination power","tok_text":"affin invari of convex polygon \n in thi correspond , we prove that the affin invari , for imag registr and object recognit , propos recent by yang and cohen ( see ibid . , vol.8 , no.7 , p.934 - 46 , juli 1999 ) are algebra depend . we show how to select an independ and complet set of the invari . the use of thi new set lead to a signific reduct of the comput complex without decreas the discrimin power","ordered_present_kp":[0,16,90,107],"keyphrases":["affine invariants","convex polygons","image registration","object recognition","algebraically dependent. invariants","complexity reduction","convex quadruplet","feature vector"],"prmu":["P","P","P","P","R","R","M","U"]}
{"id":"2102","title":"Trust in online advice","abstract":"Many people are now influenced by the information and advice they find on the Internet, much of it of dubious quality. This article describes two studies concerned with those factors capable of influencing people's response to online advice. The first study is a qualitative account of a group of house-hunters attempting to find worthwhile information online. The second study describes a survey of more than 2,500 people who had actively sought advice over the Internet. A framework for understanding trust in online advice is proposed in which first impressions are distinguished from more detailed evaluations. Good Web design can influence the first process, but three key factors-source credibility, personalization, and predictability-are shown to predict whether people actually follow the advice given","tok_text":"trust in onlin advic \n mani peopl are now influenc by the inform and advic they find on the internet , much of it of dubiou qualiti . thi articl describ two studi concern with those factor capabl of influenc peopl 's respons to onlin advic . the first studi is a qualit account of a group of house-hunt attempt to find worthwhil inform onlin . the second studi describ a survey of more than 2,500 peopl who had activ sought advic over the internet . a framework for understand trust in onlin advic is propos in which first impress are distinguish from more detail evalu . good web design can influenc the first process , but three key factors-sourc credibl , person , and predictability-ar shown to predict whether peopl actual follow the advic given","ordered_present_kp":[92,371,577,659,672],"keyphrases":["Internet","survey","Web design","personalization","predictability","online advice trust","online mortgage advice","source credibility","e-commerce","house buying advice"],"prmu":["P","P","P","P","P","R","M","M","U","M"]}
{"id":"2147","title":"Much ado about nothing: Win32.Perrun","abstract":"JPEG files do not contain any executable code and it is impossible to infect such files. The author takes a look at the details surrounding the Win32.Perrun virus and make clear exactly what it does. The main virus feature is its ability to affect JPEG image files (compressed graphic images) and to spread via affected JPEG files. The virus affects, or modifies, or alters JPEG files but does not \"infect\" them","tok_text":"much ado about noth : win32.perrun \n jpeg file do not contain ani execut code and it is imposs to infect such file . the author take a look at the detail surround the win32.perrun viru and make clear exactli what it doe . the main viru featur is it abil to affect jpeg imag file ( compress graphic imag ) and to spread via affect jpeg file . the viru affect , or modifi , or alter jpeg file but doe not \" infect \" them","ordered_present_kp":[22,37,180,281],"keyphrases":["Win32.Perrun","JPEG files","virus","compressed graphic images"],"prmu":["P","P","P","P"]}
{"id":"2063","title":"On emotion and bounded rationality: reply to Hanoch","abstract":"The author refers to the comment made by Hanoch (see ibid. vol.49 (2000)) on his model of bounded rationality and the role of the Yerkes-Dodson law and emotional arousal in it. The author points out that Hanoch's comment, however, conspicuously fails to challenge - much less contradict - the central hypothesis of his paper. In addition, several of Hanoch's criticisms are based on a wrong characterization of the positions","tok_text":"on emot and bound ration : repli to hanoch \n the author refer to the comment made by hanoch ( see ibid . vol.49 ( 2000 ) ) on hi model of bound ration and the role of the yerkes-dodson law and emot arous in it . the author point out that hanoch 's comment , howev , conspicu fail to challeng - much less contradict - the central hypothesi of hi paper . in addit , sever of hanoch 's critic are base on a wrong character of the posit","ordered_present_kp":[3,12,171],"keyphrases":["emotion","bounded rationality","Yerkes-Dodson law","decision-making","psychology"],"prmu":["P","P","P","U","U"]}
{"id":"2026","title":"Iterative regularized least-mean mixed-norm image restoration","abstract":"We develop a regularized mixed-norm image restoration algorithm to deal with various types of noise. A mixed-norm functional is introduced, which combines the least mean square (LMS) and the least mean fourth (LMF) functionals, as well as a smoothing functional. Two regularization parameters are introduced: one to determine the relative importance of the LMS and LMF functionals, which is a function of the kurtosis, and another to determine the relative importance of the smoothing functional. The two parameters are chosen in such a way that the proposed functional is convex, so that a unique minimizer exists. An iterative algorithm is utilized for obtaining the solution, and its convergence is analyzed. The novelty of the proposed algorithm is that no knowledge of the noise distribution is required, and the relative contributions of the LMS, the LMF, and the smoothing functionals are adjusted based on the partially restored image. Experimental results demonstrate the effectiveness of the proposed algorithm","tok_text":"iter regular least-mean mixed-norm imag restor \n we develop a regular mixed-norm imag restor algorithm to deal with variou type of nois . a mixed-norm function is introduc , which combin the least mean squar ( lm ) and the least mean fourth ( lmf ) function , as well as a smooth function . two regular paramet are introduc : one to determin the rel import of the lm and lmf function , which is a function of the kurtosi , and anoth to determin the rel import of the smooth function . the two paramet are chosen in such a way that the propos function is convex , so that a uniqu minim exist . an iter algorithm is util for obtain the solut , and it converg is analyz . the novelti of the propos algorithm is that no knowledg of the nois distribut is requir , and the rel contribut of the lm , the lmf , and the smooth function are adjust base on the partial restor imag . experiment result demonstr the effect of the propos algorithm","ordered_present_kp":[0,131,140,273,295,413,573,596,649,732,850],"keyphrases":["iterative regularized least-mean mixed-norm image restoration","noise","mixed-norm functional","smoothing functional","regularization parameters","kurtosis","unique minimizer","iterative algorithm","convergence","noise distribution","partially restored image","least mean square functionals","mean fourth functionals","convex functional"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"391","title":"Model selection in electromagnetic source analysis with an application to VEFs","abstract":"In electromagnetic source analysis, it is necessary to determine how many sources are required to describe the electroencephalogram or magnetoencephalogram adequately. Model selection procedures (MSPs) or goodness of fit procedures give an estimate of the required number of sources. Existing and new MSPs are evaluated in different source and noise settings: two sources which are close or distant and noise which is uncorrelated or correlated. The commonly used MSP residual variance is seen to be ineffective, that is it often selects too many sources. Alternatives like the adjusted Hotelling's test, Bayes information criterion and the Wald test on source amplitudes are seen to be effective. The adjusted Hotelling's test is recommended if a conservative approach is taken and MSPs such as Bayes information criterion or the Wald test on source amplitudes are recommended if a more liberal approach is desirable. The MSPs are applied to empirical data (visual evoked fields)","tok_text":"model select in electromagnet sourc analysi with an applic to vef \n in electromagnet sourc analysi , it is necessari to determin how mani sourc are requir to describ the electroencephalogram or magnetoencephalogram adequ . model select procedur ( msp ) or good of fit procedur give an estim of the requir number of sourc . exist and new msp are evalu in differ sourc and nois set : two sourc which are close or distant and nois which is uncorrel or correl . the commonli use msp residu varianc is seen to be ineffect , that is it often select too mani sourc . altern like the adjust hotel 's test , bay inform criterion and the wald test on sourc amplitud are seen to be effect . the adjust hotel 's test is recommend if a conserv approach is taken and msp such as bay inform criterion or the wald test on sourc amplitud are recommend if a more liber approach is desir . the msp are appli to empir data ( visual evok field )","ordered_present_kp":[0,16,371,479,628,576,892,62,905],"keyphrases":["model selection","electromagnetic source analysis","VEFs","noise settings","residual variance","adjusted Hotelling's test","Wald test","empirical data","visual evoked fields","MEG source analysis","EEG source analysis","goodness-of-fit","source localization"],"prmu":["P","P","P","P","P","P","P","P","P","M","M","U","M"]}
{"id":"329","title":"Implications of document-level literacy skills for Web site design","abstract":"The proliferation of World Wide Web (Web) sites and the low cost of publishing information on the Web have placed a tremendous amount of information at the fingertips of millions of people. Although most of this information is at least intended to be accurate, there is much that is rumor, innuendo, urban legend, and outright falsehood. This raises problems especially for students (of all ages) trying to do research or learn about some topic. Finding accurate, credible information requires document level literacy skills, such as integration, sourcing, corroboration, and search. This paper discusses these skills and offers a list of simple ways that designers of educational Web sites can help their visitors utilize these skills","tok_text":"implic of document-level literaci skill for web site design \n the prolifer of world wide web ( web ) site and the low cost of publish inform on the web have place a tremend amount of inform at the fingertip of million of peopl . although most of thi inform is at least intend to be accur , there is much that is rumor , innuendo , urban legend , and outright falsehood . thi rais problem especi for student ( of all age ) tri to do research or learn about some topic . find accur , credibl inform requir document level literaci skill , such as integr , sourc , corrobor , and search . thi paper discuss these skill and offer a list of simpl way that design of educ web site can help their visitor util these skill","ordered_present_kp":[10,312,320,331,359,399,544,553,561,434],"keyphrases":["document-level literacy skills","rumor","innuendo","urban legend","falsehood","students","search","integration","sourcing","corroboration","accurate credible information","educational Web site design"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"404","title":"A 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using two-stage wordline\/bitline-oriented tag-compare (WLOTC\/BLOTC) scheme","abstract":"This paper reports a 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using a novel two-stage wordline\/bitline-oriented tag-compare (WLOTC\/BLOTC) and sense wordline\/bitline (SWL\/SBL) tag-sense amplifiers with an eight-transistor (8-T) tag cell in Level 2 (L2) and a 10-T shrunk logic swing (SLS) memory cell. with the ground\/floating (G\/F) data sense amplifier in Level 1 (L1) for high-speed operation for low-voltage low-power VLSI system applications. Owing to the reduced loading at the SWL in the new 11-T tag cell using the WLOTC scheme, the 10-T SLS memory cell with G\/F sense amplifier in L1, and the split comparison of the index signal in the 8-T tag cells with SWL\/SBL tag sense amplifiers in L2, this 0.8-V cache memory implemented in a 1.8-V 0.18- mu m CMOS technology has a measured L1\/L2 hit time of 11.6\/20.5 ns at the average dissipation of 0.77 mW at 50 MHz","tok_text":"a 0.8-v 128-kb four-way set-associ two-level cmo cach memori use two-stag wordlin \/ bitline-ori tag-compar ( wlotc \/ blotc ) scheme \n thi paper report a 0.8-v 128-kb four-way set-associ two-level cmo cach memori use a novel two-stag wordlin \/ bitline-ori tag-compar ( wlotc \/ blotc ) and sens wordlin \/ bitlin ( swl \/ sbl ) tag-sens amplifi with an eight-transistor ( 8-t ) tag cell in level 2 ( l2 ) and a 10-t shrunk logic swing ( sl ) memori cell . with the ground \/ float ( g \/ f ) data sens amplifi in level 1 ( l1 ) for high-spe oper for low-voltag low-pow vlsi system applic . owe to the reduc load at the swl in the new 11-t tag cell use the wlotc scheme , the 10-t sl memori cell with g \/ f sens amplifi in l1 , and the split comparison of the index signal in the 8-t tag cell with swl \/ sbl tag sens amplifi in l2 , thi 0.8-v cach memori implement in a 1.8-v 0.18- mu m cmo technolog ha a measur l1 \/ l2 hit time of 11.6\/20.5 ns at the averag dissip of 0.77 mw at 50 mhz","ordered_present_kp":[35,74,324,526,555,974,963],"keyphrases":["two-level CMOS cache memory","wordline\/bitline-oriented tag-compare","tag-sense amplifiers","high-speed operation","low-power VLSI system applications","0.77 mW","50 MHz","four-way set-associative memory","cache memory architecture","sense wordline\/bitline amplifiers","eight-transistor tag cell","ten-transistor memory cell","shrunk logic swing memory cell","ground\/floating data sense amplifier","low-voltage VLSI system applications","0.8 V","128 kbit","1.8 V","0.18 micron","11.6 ns","20.5 ns"],"prmu":["P","P","P","P","P","P","P","R","M","R","R","M","R","R","R","U","U","U","U","M","M"]}
{"id":"271","title":"On the use of neural network ensembles in QSAR and QSPR","abstract":"Despite their growing popularity among neural network practitioners, ensemble methods have not been widely adopted in structure-activity and structure-property correlation. Neural networks are inherently unstable, in that small changes in the training set and\/or training parameters can lead to large changes in their generalization performance. Recent research has shown that by capitalizing on the diversity of the individual models, ensemble techniques can minimize uncertainty and produce more stable and accurate predictors. In this work, we present a critical assessment of the most common ensemble technique known as bootstrap aggregation, or bagging, as applied to QSAR and QSPR. Although aggregation does offer definitive advantages, we demonstrate that bagging may not be the best possible choice and that simpler techniques such as retraining with the full sample can often produce superior results. These findings are rationalized using Krogh and Vedelsby's (1995) decomposition of the generalization error into a term that measures the average generalization performance of the individual networks and a term that measures the diversity among them. For networks that are designed to resist over-fitting, the benefits of aggregation are clear but not overwhelming","tok_text":"on the use of neural network ensembl in qsar and qspr \n despit their grow popular among neural network practition , ensembl method have not been wide adopt in structure-act and structure-properti correl . neural network are inher unstabl , in that small chang in the train set and\/or train paramet can lead to larg chang in their gener perform . recent research ha shown that by capit on the divers of the individu model , ensembl techniqu can minim uncertainti and produc more stabl and accur predictor . in thi work , we present a critic assess of the most common ensembl techniqu known as bootstrap aggreg , or bag , as appli to qsar and qspr . although aggreg doe offer definit advantag , we demonstr that bag may not be the best possibl choic and that simpler techniqu such as retrain with the full sampl can often produc superior result . these find are ration use krogh and vedelsbi 's ( 1995 ) decomposit of the gener error into a term that measur the averag gener perform of the individu network and a term that measur the divers among them . for network that are design to resist over-fit , the benefit of aggreg are clear but not overwhelm","ordered_present_kp":[14,40,49,267,284,330,450,592,614,782,177],"keyphrases":["neural network ensembles","QSAR","QSPR","structure-property correlation","training set","training parameters","generalization performance","uncertainty","bootstrap aggregation","bagging","retraining","generalization error decomposition","structure-activity correlation"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2183","title":"Knowledge-based structures and organisational commitment","abstract":"Organisational commitment, the emotional attachment of an employee to the employing organisation, has attracted a substantial body of literature, relating the concept to various antecedents, including organisational structure, and to a range of consequences, including financially important performance factors such as productivity and staff turnover. The new areas of knowledge management and learning organisations offer substantial promise as imperatives for the organisation of business enterprises. As organisations in the contemporary environment adopt knowledge-based structures to improve their competitive position, there is value in examining these structures against other performance related factors. Theoretical knowledge-based structures put forward by R. Miles et al. (1997) and J. Quinn et al. (1996) and an existing implementation are examined to determine common features inherent in these approaches. These features are posited as a typical form and their impact on organisational commitment and hence on individual and organisational performance is examined","tok_text":"knowledge-bas structur and organis commit \n organis commit , the emot attach of an employe to the employ organis , ha attract a substanti bodi of literatur , relat the concept to variou anteced , includ organis structur , and to a rang of consequ , includ financi import perform factor such as product and staff turnov . the new area of knowledg manag and learn organis offer substanti promis as imper for the organis of busi enterpris . as organis in the contemporari environ adopt knowledge-bas structur to improv their competit posit , there is valu in examin these structur against other perform relat factor . theoret knowledge-bas structur put forward by r. mile et al . ( 1997 ) and j. quinn et al . ( 1996 ) and an exist implement are examin to determin common featur inher in these approach . these featur are posit as a typic form and their impact on organis commit and henc on individu and organis perform is examin","ordered_present_kp":[0,65,271,294,306,27],"keyphrases":["knowledge-based structures","organisational commitment","emotional attachment","performance factors","productivity","staff turnover","earning organisations"],"prmu":["P","P","P","P","P","P","M"]}
{"id":"234","title":"The UK's National Electronic Site Licensing Initiative (NESLI)","abstract":"In 1998 the UK created the National Electronic Site Licensing Initiative (NESLI) to increase and improve access to electronic journals and to negotiate license agreements on behalf of academic libraries. The use of a model license agreement and the success of site licensing is discussed. Highlights from an interim evaluation by the Joint Information Systems Committee (JISC) are noted and key issues and questions arising from the evaluation are identified","tok_text":"the uk 's nation electron site licens initi ( nesli ) \n in 1998 the uk creat the nation electron site licens initi ( nesli ) to increas and improv access to electron journal and to negoti licens agreement on behalf of academ librari . the use of a model licens agreement and the success of site licens is discuss . highlight from an interim evalu by the joint inform system committe ( jisc ) are note and key issu and question aris from the evalu are identifi","ordered_present_kp":[10,46,157,188,218,354,385],"keyphrases":["National Electronic Site Licensing Initiative","NESLI","electronic journals","license agreements","academic libraries","Joint Information Systems Committee","JISC","usage statistics","ICOLC"],"prmu":["P","P","P","P","P","P","P","U","U"]}
{"id":"22","title":"Analyzing the benefits of 300 mm conveyor-based AMHS","abstract":"While the need for automation in 300 mm fabs is not debated, the form and performance of such automation is still in question. Software simulation that compares conveyor-based continuous flow transport technology to conventional car-based wafer-lot delivery has detailed delivery time and throughput advantages to the former","tok_text":"analyz the benefit of 300 mm conveyor-bas amh \n while the need for autom in 300 mm fab is not debat , the form and perform of such autom is still in question . softwar simul that compar conveyor-bas continu flow transport technolog to convent car-bas wafer-lot deliveri ha detail deliveri time and throughput advantag to the former","ordered_present_kp":[160,243,186,298,280,22],"keyphrases":["300 mm","software simulation","conveyor-based continuous flow transport technology","car-based wafer-lot delivery","delivery time","throughput","automated material handling system","semiconductor fab","wafer processing"],"prmu":["P","P","P","P","P","P","M","M","U"]}
{"id":"2106","title":"Explanations for the perpetration of and reactions to deception in a virtual community","abstract":"Cases of identity deception on the Internet are not uncommon. Several cases of a revealed identity deception have been reported in the media. The authors examine a case of deception in an online community composed primarily of information technology professionals. In this case, an established community member (DF) invented a character (Nowheremom) whom he fell in love with and who was eventually killed in a tragic accident. When other members of the community eventually began to question Nowheremom's actual identity, DF admitted that he invented her. The discussion board was flooded with reactions to DF's revelation. The authors propose several explanations for the perpetration of identity deception, including psychiatric illness, identity play, and expressions of true self. They also analyze the reactions of community members and propose three related explanations (social identity, deviance, and norm violation) to account for their reactions. It is argued that virtual communities' reactions to such threatening events provide invaluable clues for the study of group processes on the Internet","tok_text":"explan for the perpetr of and reaction to decept in a virtual commun \n case of ident decept on the internet are not uncommon . sever case of a reveal ident decept have been report in the media . the author examin a case of decept in an onlin commun compos primarili of inform technolog profession . in thi case , an establish commun member ( df ) invent a charact ( nowheremom ) whom he fell in love with and who wa eventu kill in a tragic accid . when other member of the commun eventu began to question nowheremom 's actual ident , df admit that he invent her . the discuss board wa flood with reaction to df 's revel . the author propos sever explan for the perpetr of ident decept , includ psychiatr ill , ident play , and express of true self . they also analyz the reaction of commun member and propos three relat explan ( social ident , devianc , and norm violat ) to account for their reaction . it is argu that virtual commun ' reaction to such threaten event provid invalu clue for the studi of group process on the internet","ordered_present_kp":[54,79,99,236,269,694,1005],"keyphrases":["virtual community","identity deception","Internet","online community","information technology professionals","psychiatric illness","group processes","social processes","Web sites","psychology","bulletin boards"],"prmu":["P","P","P","P","P","P","P","R","U","U","M"]}
{"id":"2143","title":"Quantum computing with solids","abstract":"Science and technology could be revolutionized by quantum computers, but building them from solid-state devices will not be easy. The author outlines the challenges in scaling up the technology from lab experiments to practical devices","tok_text":"quantum comput with solid \n scienc and technolog could be revolution by quantum comput , but build them from solid-st devic will not be easi . the author outlin the challeng in scale up the technolog from lab experi to practic devic","ordered_present_kp":[0,109],"keyphrases":["quantum computers","solid-state devices"],"prmu":["P","P"]}
{"id":"209","title":"Information interaction: providing a framework for information architecture","abstract":"Information interaction is the process that people use in interacting with the content of an information system. Information architecture is a blueprint and navigational aid to the content of information-rich systems. As such information architecture performs an important supporting role in information interactivity. This article elaborates on a model of information interactivity that crosses the \"no-man's land\" between user and computer articulating a model that includes user, content and system, illustrating the context for information architecture","tok_text":"inform interact : provid a framework for inform architectur \n inform interact is the process that peopl use in interact with the content of an inform system . inform architectur is a blueprint and navig aid to the content of information-rich system . as such inform architectur perform an import support role in inform interact . thi articl elabor on a model of inform interact that cross the \" no-man 's land \" between user and comput articul a model that includ user , content and system , illustr the context for inform architectur","ordered_present_kp":[0,197,225,0],"keyphrases":["information interaction","information interaction","navigational aid","information-rich systems","information interactivity"],"prmu":["P","P","P","P","P"]}
{"id":"1932","title":"Solution of the safe problem on (0,1)-matrices","abstract":"A safe problem with mn locks is studied. It is reduced to a system of linear equations in the modulo 2 residue class. There are three possible variants defined by the numbers m and n evenness, with only one of them having a solution. In two other cases, correction of the initial state of the safe insuring a solution is proposed","tok_text":"solut of the safe problem on ( 0,1)-matric \n a safe problem with mn lock is studi . it is reduc to a system of linear equat in the modulo 2 residu class . there are three possibl variant defin by the number m and n even , with onli one of them have a solut . in two other case , correct of the initi state of the safe insur a solut is propos","ordered_present_kp":[13,65,111,131,29],"keyphrases":["safe problem","(0,1)-matrices","mn locks","linear equations","modulo 2 residue class","computer games","linear Diophantine equations"],"prmu":["P","P","P","P","P","U","M"]}
{"id":"1977","title":"Tracking nonparameterized object contours in video","abstract":"We propose a new method for contour tracking in video. The inverted distance transform of the edge map is used as an edge indicator function for contour detection. Using the concept of topographical distance, the watershed segmentation can be formulated as a minimization. This new viewpoint gives a way to combine the results of the watershed algorithm on different surfaces. In particular, our algorithm determines the contour as a combination of the current edge map and the contour, predicted from the tracking result in the previous frame. We also show that the problem of background clutter can be relaxed by taking the object motion into account. The compensation with object motion allows to detect and remove spurious edges in background. The experimental results confirm the expected advantages of the proposed method over the existing approaches","tok_text":"track nonparameter object contour in video \n we propos a new method for contour track in video . the invert distanc transform of the edg map is use as an edg indic function for contour detect . use the concept of topograph distanc , the watersh segment can be formul as a minim . thi new viewpoint give a way to combin the result of the watersh algorithm on differ surfac . in particular , our algorithm determin the contour as a combin of the current edg map and the contour , predict from the track result in the previou frame . we also show that the problem of background clutter can be relax by take the object motion into account . the compens with object motion allow to detect and remov spuriou edg in background . the experiment result confirm the expect advantag of the propos method over the exist approach","ordered_present_kp":[72,6,154,213,237,272,564,608,37,101,133],"keyphrases":["nonparameterized object contours","video","contour tracking","inverted distance transform","edge map","edge indicator function","topographical distance","watershed segmentation","minimization","background clutter","object motion","motion analysis","motion estimation","edge detection"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","M","M","R"]}
{"id":"314","title":"Information architecture in JASIST: just where did we come from?","abstract":"The emergence of Information Architecture within the information systems world has been simultaneously drawn out yet rapid. Those with an eye on history are quick to point to Wurman's 1976 use of the term \"architecture of information,\" but it has only been in the last 2 years that IA has become the source of sufficient interest for people to label themselves professionally as Information Architects. The impetus for this recent emergence of IA can be traced to a historical summit, supported by ASIS&T in May 2000 at Boston. It was here that several hundred of us gathered to thrash out the questions of just what IA was and what this new field might become. At the time of the summit, invited to present a short talk on my return journey from the annual ACM SIGCHI conference, I entered the summit expecting little and convinced that IA was nothing new. I left 2 days later refreshed, not just by the enthusiasm of the attendees for this term but by IA's potential to unify the disparate perspectives and orientations of professionals from a range of disciplines. It was at this summit that the idea for the special issue took root. I proposed the idea to Don Kraft, hoping he would find someone else to run with it. AS luck would have it, I ended up taking charge of it myself, with initial support from David Blair. From the suggestion to the finished product-has been the best part of 2 years, and in that time more than 50 volunteers reviewed over 20 submissions","tok_text":"inform architectur in jasist : just where did we come from ? \n the emerg of inform architectur within the inform system world ha been simultan drawn out yet rapid . those with an eye on histori are quick to point to wurman 's 1976 use of the term \" architectur of inform , \" but it ha onli been in the last 2 year that ia ha becom the sourc of suffici interest for peopl to label themselv profession as inform architect . the impetu for thi recent emerg of ia can be trace to a histor summit , support by asis&t in may 2000 at boston . it wa here that sever hundr of us gather to thrash out the question of just what ia wa and what thi new field might becom . at the time of the summit , invit to present a short talk on my return journey from the annual acm sigchi confer , i enter the summit expect littl and convinc that ia wa noth new . i left 2 day later refresh , not just by the enthusiasm of the attende for thi term but by ia 's potenti to unifi the dispar perspect and orient of profession from a rang of disciplin . it wa at thi summit that the idea for the special issu took root . i propos the idea to don kraft , hope he would find someon els to run with it . as luck would have it , i end up take charg of it myself , with initi support from david blair . from the suggest to the finish product-ha been the best part of 2 year , and in that time more than 50 volunt review over 20 submiss","ordered_present_kp":[0,106],"keyphrases":["information architecture","information systems","metadata fields","controlled vocabularies","Web sites","CD-ROM","qualified information architect"],"prmu":["P","P","M","U","U","U","M"]}
{"id":"351","title":"Optimal online algorithm for scheduling on two identical machines with machine availability constraints","abstract":"This paper considers the online scheduling on two identical machines with machine availability constraints for minimizing makespan. We assume that machine M\/sub j\/ is unavailable during period from s\/sub j\/ to t\/sub j\/ (0 <or= s\/sub j\/ < t\/sub j\/), j = 1, 2, and the unavailable periods of two machines do not overlap. We show that the competitive ratio of list scheduling is 3. We further give an optimal algorithm with a competitive ratio 5\/2","tok_text":"optim onlin algorithm for schedul on two ident machin with machin avail constraint \n thi paper consid the onlin schedul on two ident machin with machin avail constraint for minim makespan . we assum that machin m \/ sub j\/ is unavail dure period from s \/ sub j\/ to t \/ sub j\/ ( 0 < or= s \/ sub j\/ < t \/ sub j\/ ) , j = 1 , 2 , and the unavail period of two machin do not overlap . we show that the competit ratio of list schedul is 3 . we further give an optim algorithm with a competit ratio 5\/2","ordered_present_kp":[0,414,59],"keyphrases":["optimal online algorithm","machine availability constraints","list scheduling","makespan minimisation","identical machines scheduling"],"prmu":["P","P","P","M","R"]}
{"id":"1953","title":"Social percolation and the influence of mass media","abstract":"In the marketing model of Solomon and Weisbuch, people buy a product only if their neighbours tell them of its quality, and if this quality is higher than their own quality expectations. Now we introduce additional information from the mass media, which is analogous to the ghost field in percolation theory. The mass media shift the percolative phase transition observed in the model, and decrease the time after which the stationary state is reached","tok_text":"social percol and the influenc of mass media \n in the market model of solomon and weisbuch , peopl buy a product onli if their neighbour tell them of it qualiti , and if thi qualiti is higher than their own qualiti expect . now we introduc addit inform from the mass media , which is analog to the ghost field in percol theori . the mass media shift the percol phase transit observ in the model , and decreas the time after which the stationari state is reach","ordered_present_kp":[0,207,298,354,434],"keyphrases":["social percolation","quality expectations","ghost field","percolative phase transition","stationary state","mass media influence","Solomon-Weisbuch marketing model","customers","cinema","external field"],"prmu":["P","P","P","P","P","R","M","U","U","M"]}
{"id":"194","title":"Books on demand: just-in-time acquisitions","abstract":"The Purdue University Libraries Interlibrary Loan unit proposed a pilot project to purchase patrons' loan requests from Amazon. com, lend them to the patrons, and then add the titles to the collection. Staff analyzed previous monograph loans, developed ordering criteria, implemented the proposal as a pilot project for six months, and evaluated the resulting patron comments, statistics, and staff perceptions. As a result of enthusiastic patron comments and a review of the project statistics, the program was extended","tok_text":"book on demand : just-in-tim acquisit \n the purdu univers librari interlibrari loan unit propos a pilot project to purchas patron ' loan request from amazon . com , lend them to the patron , and then add the titl to the collect . staff analyz previou monograph loan , develop order criteria , implement the propos as a pilot project for six month , and evalu the result patron comment , statist , and staff percept . as a result of enthusiast patron comment and a review of the project statist , the program wa extend","ordered_present_kp":[44,251,276,401,370],"keyphrases":["Purdue University Libraries Interlibrary Loan unit","monograph loans","ordering criteria","patron comments","staff perceptions","publication on demand"],"prmu":["P","P","P","P","P","M"]}
{"id":"268","title":"A method for correlations analysis of coordinates: applications for molecular conformations","abstract":"We describe a new method to analyze multiple correlations between subsets of coordinates that represent a sample. The correlation is established only between specific regions of interest at the coordinates. First, the region(s) of interest are selected at each molecular coordinate. Next, a correlation matrix is constructed for the selected regions. The matrix is subject to further analysis, illuminating the multidimensional structural characteristics that exist in the conformational space. The method's abilities are demonstrated in several examples: it is used to analyze the conformational space of complex molecules, it is successfully applied to compare related conformational spaces, and it is used to analyze a diverse set of protein folding trajectories","tok_text":"a method for correl analysi of coordin : applic for molecular conform \n we describ a new method to analyz multipl correl between subset of coordin that repres a sampl . the correl is establish onli between specif region of interest at the coordin . first , the region( ) of interest are select at each molecular coordin . next , a correl matrix is construct for the select region . the matrix is subject to further analysi , illumin the multidimension structur characterist that exist in the conform space . the method 's abil are demonstr in sever exampl : it is use to analyz the conform space of complex molecul , it is success appli to compar relat conform space , and it is use to analyz a divers set of protein fold trajectori","ordered_present_kp":[213,331,302,437,599,492,709,52],"keyphrases":["molecular conformations","regions of interest","molecular coordinate","correlation matrix","multidimensional structural characteristics","conformational spaces","complex molecules","protein folding trajectories","multiple correlation analysis"],"prmu":["P","P","P","P","P","P","P","P","R"]}
{"id":"2122","title":"A fuzzy logic adaptation circuit for control systems of deformable space vehicles: its design","abstract":"A fuzzy-logic adaptation algorithm is designed for adjusting the discreteness period of a control system for ensuring the stability and quality of control process with regard to the elastic structural vibrations of a deformable space vehicle. Its performance is verified by digital modeling of a discrete control system with two objects","tok_text":"a fuzzi logic adapt circuit for control system of deform space vehicl : it design \n a fuzzy-log adapt algorithm is design for adjust the discret period of a control system for ensur the stabil and qualiti of control process with regard to the elast structur vibrat of a deform space vehicl . it perform is verifi by digit model of a discret control system with two object","ordered_present_kp":[2,32,50,137,186,243,316],"keyphrases":["fuzzy logic adaptation circuit","control systems","deformable space vehicles","discreteness period","stability","elastic structural vibrations","digital modeling"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"295","title":"Hours of operation and service in academic libraries: toward a national standard","abstract":"In an effort toward establishing a standard for academic library hours, the article surveys and compares hours of operation and service for ARL libraries and IPEDS survey respondents. The article ranks the ARL (Association for Research Libraries) libraries according to hours of operation and reference hours and then briefly discusses such issues as libraries offering twenty-four access and factors affecting service hour decisions","tok_text":"hour of oper and servic in academ librari : toward a nation standard \n in an effort toward establish a standard for academ librari hour , the articl survey and compar hour of oper and servic for arl librari and ipe survey respond . the articl rank the arl ( associ for research librari ) librari accord to hour of oper and refer hour and then briefli discuss such issu as librari offer twenty-four access and factor affect servic hour decis","ordered_present_kp":[116,195,211,258],"keyphrases":["academic library hours","ARL libraries","IPEDS survey respondents","Association for Research Libraries","operation\/service hours","Integrated Post Secondary Education Data System"],"prmu":["P","P","P","P","M","U"]}
{"id":"2167","title":"Finally! some sensible European legislation on software","abstract":"The European Commission has formally tabled a draft Directive on the Protection by Patents of Computer-Implemented Inventions. The aim of this very important Directive is to harmonise national patent laws relating to inventions using software. It follows an extensive consultation launched by the Commission in October 2000. The impetus behind the Directive was the recognition at EU level of a total lack of unity between the European Patent Office and European national courts in deciding what was or was not deemed patentable when it came to the subject of computer programs","tok_text":"final ! some sensibl european legisl on softwar \n the european commiss ha formal tabl a draft direct on the protect by patent of computer-impl invent . the aim of thi veri import direct is to harmonis nation patent law relat to invent use softwar . it follow an extens consult launch by the commiss in octob 2000 . the impetu behind the direct wa the recognit at eu level of a total lack of uniti between the european patent offic and european nation court in decid what wa or wa not deem patent when it came to the subject of comput program","ordered_present_kp":[54,94,201,21,409,444,527],"keyphrases":["EU","European Commission","Directive on the Protection by Patents of Computer-Implemented Inventions","national patent laws","European Patent Office","national courts","computer programs","law harmonisation"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"388","title":"Noninvasive myocardial activation time imaging: a novel inverse algorithm applied to clinical ECG mapping data","abstract":"Linear approaches like the minimum-norm least-square algorithm show insufficient performance when it comes to estimating the activation time map on the surface of the heart from electrocardiographic (ECG) mapping data. Additional regularization has to be considered leading to a nonlinear problem formulation. The Gauss-Newton approach is one of the standard mathematical tools capable of solving this kind of problem. To our experience, this algorithm has specific drawbacks which are caused by the applied regularization procedure. In particular, under clinical conditions the amount of regularization cannot be determined clearly. For this reason, we have developed an iterative algorithm solving this nonlinear problem by a sequence of regularized linear problems. At each step of iteration, an individual L-curve is computed. Subsequent iteration steps are performed with the individual optimal regularization parameter. This novel approach is compared with the standard Gauss-Newton approach. Both methods are applied to simulated ECG mapping data as well as to single beat sinus rhythm data from two patients recorded in the catheter laboratory. The proposed approach shows excellent numerical and computational performance, even under clinical conditions at which the Gauss-Newton approach begins to break down","tok_text":"noninvas myocardi activ time imag : a novel invers algorithm appli to clinic ecg map data \n linear approach like the minimum-norm least-squar algorithm show insuffici perform when it come to estim the activ time map on the surfac of the heart from electrocardiograph ( ecg ) map data . addit regular ha to be consid lead to a nonlinear problem formul . the gauss-newton approach is one of the standard mathemat tool capabl of solv thi kind of problem . to our experi , thi algorithm ha specif drawback which are caus by the appli regular procedur . in particular , under clinic condit the amount of regular can not be determin clearli . for thi reason , we have develop an iter algorithm solv thi nonlinear problem by a sequenc of regular linear problem . at each step of iter , an individu l-curv is comput . subsequ iter step are perform with the individu optim regular paramet . thi novel approach is compar with the standard gauss-newton approach . both method are appli to simul ecg map data as well as to singl beat sinu rhythm data from two patient record in the cathet laboratori . the propos approach show excel numer and comput perform , even under clinic condit at which the gauss-newton approach begin to break down","ordered_present_kp":[0,18,357,849,1070,571,818,530,44,70],"keyphrases":["noninvasive myocardial activation time imaging","activation time imaging","inverse algorithm","clinical ECG mapping data","Gauss-Newton approach","regularization procedure","clinical conditions","iteration steps","individual optimal regularization parameter","catheter laboratory","electrodiagnostics","L-curve method","noninvasive electrocardiography","tikhonov regularization","heart surface"],"prmu":["P","P","P","P","P","P","P","P","P","P","U","R","M","M","R"]}
{"id":"2087","title":"Re-examining the machining frictional boundary conditions using fractals","abstract":"Presents experimental evidence for the existence of non-Euclidean contact geometry at the tool-chip interface in the machining of aluminium alloy, which challenges conventional assumptions. The geometry of contact at the tool rake face is modelled using fractals and a dimension is computed for its description. The variation in the fractal dimension with the cutting speed is explored","tok_text":"re-examin the machin friction boundari condit use fractal \n present experiment evid for the exist of non-euclidean contact geometri at the tool-chip interfac in the machin of aluminium alloy , which challeng convent assumpt . the geometri of contact at the tool rake face is model use fractal and a dimens is comput for it descript . the variat in the fractal dimens with the cut speed is explor","ordered_present_kp":[14,50,139,175,115,257,376],"keyphrases":["machining frictional boundary conditions","fractals","contact geometry","tool-chip interface","aluminium alloy","tool rake face","cutting speed","nonEuclidean contact geometry","Al"],"prmu":["P","P","P","P","P","P","P","M","U"]}
{"id":"330","title":"Improving computer security for authentication of users: influence of proactive password restrictions","abstract":"Entering a user name-password combination is a widely used procedure for identification and authentication in computer systems. However, it is a notoriously weak method, in that the passwords adopted by many users are easy to crack. In an attempt to, improve security, proactive password checking may be used, in which passwords must meet several criteria to be more resistant to cracking. In two experiments, we examined the influence of proactive password restrictions on the time that it took to generate an acceptable password and to use it subsequently to log in. The required length was a minimum of five characters in experiment I and eight characters in experiment 2. In both experiments, one condition had only the length restriction, and the other had additional restrictions. The additional restrictions greatly increased the time it took to generate the password but had only a small effect on the time it took to use it subsequently to log in. For the five-character passwords, 75% were cracked when no other restrictions were imposed, and this was reduced to 33% with the additional restrictions. For the eight-character passwords, 17% were cracked with no other restrictions, and 12.5% with restrictions. The results indicate that increasing the minimum character length reduces crackability and increases security, regardless of whether additional restrictions are imposed","tok_text":"improv comput secur for authent of user : influenc of proactiv password restrict \n enter a user name-password combin is a wide use procedur for identif and authent in comput system . howev , it is a notori weak method , in that the password adopt by mani user are easi to crack . in an attempt to , improv secur , proactiv password check may be use , in which password must meet sever criteria to be more resist to crack . in two experi , we examin the influenc of proactiv password restrict on the time that it took to gener an accept password and to use it subsequ to log in . the requir length wa a minimum of five charact in experi i and eight charact in experi 2 . in both experi , one condit had onli the length restrict , and the other had addit restrict . the addit restrict greatli increas the time it took to gener the password but had onli a small effect on the time it took to use it subsequ to log in . for the five-charact password , 75 % were crack when no other restrict were impos , and thi wa reduc to 33 % with the addit restrict . for the eight-charact password , 17 % were crack with no other restrict , and 12.5 % with restrict . the result indic that increas the minimum charact length reduc crackabl and increas secur , regardless of whether addit restrict are impos","ordered_present_kp":[7,314,54,711,924,1059],"keyphrases":["computer security","proactive password restrictions","proactive password checking","length restriction","five-character passwords","eight-character passwords","user authentication"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"375","title":"Separation and tracking of multiple broadband sources with one electromagnetic vector sensor","abstract":"A structure for adaptively separating, enhancing and tracking uncorrelated sources with an electromagnetic vector sensor (EMVS) is presented. The structure consists of a set of parallel spatial processors, one for each individual source. Two stages of processing are involved in each spatial processor. The first preprocessing stage rejects all other sources except the one of interest, while the second stage is an adaptive one for maximizing the signal-to-noise ratio (SNR) and tracking the desired source. The preprocessings are designed using the latest source parameter estimates obtained from the source trackers, and a redesign is activated periodically or whenever any source has been detected by the source trackers to have made significant movement. Compared with conventional adaptive beamforming, the algorithm has the advantage that no a priori information on any desired signal location is needed, the sources are separated at maximum SNR, and their locations are available. The structure is also well suited for parallel implementation. Numerical examples are included to illustrate the capability and performance of the algorithm","tok_text":"separ and track of multipl broadband sourc with one electromagnet vector sensor \n a structur for adapt separ , enhanc and track uncorrel sourc with an electromagnet vector sensor ( emv ) is present . the structur consist of a set of parallel spatial processor , one for each individu sourc . two stage of process are involv in each spatial processor . the first preprocess stage reject all other sourc except the one of interest , while the second stage is an adapt one for maxim the signal-to-nois ratio ( snr ) and track the desir sourc . the preprocess are design use the latest sourc paramet estim obtain from the sourc tracker , and a redesign is activ period or whenev ani sourc ha been detect by the sourc tracker to have made signific movement . compar with convent adapt beamform , the algorithm ha the advantag that no a priori inform on ani desir signal locat is need , the sourc are separ at maximum snr , and their locat are avail . the structur is also well suit for parallel implement . numer exampl are includ to illustr the capabl and perform of the algorithm","ordered_present_kp":[128,52,233,362,484,904,981],"keyphrases":["electromagnetic vector sensor","uncorrelated sources","parallel spatial processors","preprocessing stage","signal-to-noise ratio","maximum SNR","parallel implementation","multiple broadband sources separation","multiple broadband sources tracking","single EM vector sensor","adaptive second stage","SNR maximization","signal source location","adaptive source enhancement"],"prmu":["P","P","P","P","P","P","P","R","R","M","R","R","R","R"]}
{"id":"348","title":"Lower bounds on the information rate of secret sharing schemes with homogeneous access structure","abstract":"We present some new lower bounds on the optimal information rate and on the optimal average information rate of secret sharing schemes with homogeneous access structure. These bounds are found by using some covering constructions and a new parameter, the k-degree of a participant, that is introduced in this paper. Our bounds improve the previous ones in almost all cases","tok_text":"lower bound on the inform rate of secret share scheme with homogen access structur \n we present some new lower bound on the optim inform rate and on the optim averag inform rate of secret share scheme with homogen access structur . these bound are found by use some cover construct and a new paramet , the k-degre of a particip , that is introduc in thi paper . our bound improv the previou one in almost all case","ordered_present_kp":[0,124,153,306,19,34,59],"keyphrases":["lower bounds","information rate","secret sharing schemes","homogeneous access structure","optimal information rate","optimal average information rate","k-degree","cryptography"],"prmu":["P","P","P","P","P","P","P","U"]}
{"id":"2047","title":"A generalized PERT\/CPM implementation in a spreadsheet","abstract":"This paper describes the implementation of the traditional PERT\/CPM algorithm for finding the critical path in a project network in a spreadsheet. The problem is of importance due to the recent shift of attention to using the spreadsheet environment as a vehicle for delivering management science\/operations research (MS\/OR) techniques to end-users","tok_text":"a gener pert \/ cpm implement in a spreadsheet \n thi paper describ the implement of the tradit pert \/ cpm algorithm for find the critic path in a project network in a spreadsheet . the problem is of import due to the recent shift of attent to use the spreadsheet environ as a vehicl for deliv manag scienc \/ oper research ( ms \/ or ) techniqu to end-us","ordered_present_kp":[2,34,128],"keyphrases":["generalized PERT\/CPM implementation","spreadsheet","critical path","MS\/OR techniques"],"prmu":["P","P","P","R"]}
{"id":"2002","title":"A new subspace identification approach based on principal component analysis","abstract":"Principal component analysis (PCA) has been widely used for monitoring complex industrial processes with multiple variables and diagnosing process and sensor faults. The objective of this paper is to develop a new subspace identification algorithm that gives consistent model estimates under the errors-in-variables (EIV) situation. In this paper, we propose a new subspace identification approach using principal component analysis. PCA naturally falls into the category of EIV formulation, which resembles total least squares and allows for errors in both process input and output. We propose to use PCA to determine the system observability subspace, the matrices and the system order for an EIV formulation. Standard PCA is modified with instrumental variables in order to achieve consistent estimates of the system matrices. The proposed subspace identification method is demonstrated using a simulated process and a real industrial process for model identification and order determination. For comparison the MOESP algorithm and N4SID algorithm are used as benchmarks to demonstrate the advantages of the proposed PCA based subspace model identification (SMI) algorithm","tok_text":"a new subspac identif approach base on princip compon analysi \n princip compon analysi ( pca ) ha been wide use for monitor complex industri process with multipl variabl and diagnos process and sensor fault . the object of thi paper is to develop a new subspac identif algorithm that give consist model estim under the errors-in-vari ( eiv ) situat . in thi paper , we propos a new subspac identif approach use princip compon analysi . pca natur fall into the categori of eiv formul , which resembl total least squar and allow for error in both process input and output . we propos to use pca to determin the system observ subspac , the matric and the system order for an eiv formul . standard pca is modifi with instrument variabl in order to achiev consist estim of the system matric . the propos subspac identif method is demonstr use a simul process and a real industri process for model identif and order determin . for comparison the moesp algorithm and n4sid algorithm are use as benchmark to demonstr the advantag of the propos pca base subspac model identif ( smi ) algorithm","ordered_present_kp":[6,39,89,609,940,960,1045,1069],"keyphrases":["subspace identification approach","principal component analysis","PCA","system observability subspace","MOESP algorithm","N4SID algorithm","subspace model identification","SMI","complex industrial process monitoring","process fault diagnosis","sensor fault diagnosis","errors-in-variables situation","EIV situation","total least-squares approximation","consistent system matrix estimates"],"prmu":["P","P","P","P","P","P","P","P","R","M","M","R","R","M","M"]}
{"id":"1993","title":"Color plane interpolation using alternating projections","abstract":"Most commercial digital cameras use color filter arrays to sample red, green, and blue colors according to a specific pattern. At the location of each pixel only one color sample is taken, and the values of the other colors must be interpolated using neighboring samples. This color plane interpolation is known as demosaicing; it is one of the important tasks in a digital camera pipeline. If demosaicing is not performed appropriately, images suffer from highly visible color artifacts. In this paper we present a new demosaicing technique that uses inter-channel correlation effectively in an alternating-projections scheme. We have compared this technique with six state-of-the-art demosaicing techniques, and it outperforms all of them, both visually and in terms of mean square error","tok_text":"color plane interpol use altern project \n most commerci digit camera use color filter array to sampl red , green , and blue color accord to a specif pattern . at the locat of each pixel onli one color sampl is taken , and the valu of the other color must be interpol use neighbor sampl . thi color plane interpol is known as demosa ; it is one of the import task in a digit camera pipelin . if demosa is not perform appropri , imag suffer from highli visibl color artifact . in thi paper we present a new demosa techniqu that use inter-channel correl effect in an alternating-project scheme . we have compar thi techniqu with six state-of-the-art demosa techniqu , and it outperform all of them , both visual and in term of mean squar error","ordered_present_kp":[0,25,56,325,73,458,530],"keyphrases":["color plane interpolation","alternating projections","digital cameras","color filter arrays","demosaicing","color artifacts","inter-channel correlation"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"255","title":"The culture of usability","abstract":"Now that most of us agree that usability testing is an integral investment in site development, it's time to recognize that the standard approach falls short. It is possible to do less work and get better results while spending less money. By bringing usability testing in-house and breaking tests into more manageable sessions, you can vastly improve your online offering without affecting your profit margin","tok_text":"the cultur of usabl \n now that most of us agre that usabl test is an integr invest in site develop , it 's time to recogn that the standard approach fall short . it is possibl to do less work and get better result while spend less money . by bring usabl test in-hous and break test into more manag session , you can vastli improv your onlin offer without affect your profit margin","ordered_present_kp":[],"keyphrases":["usability testing program","Web site"],"prmu":["M","M"]}
{"id":"210","title":"When a better interface and easy navigation aren't enough: examining the information architecture in a law enforcement agency","abstract":"An information architecture that allows users to easily navigate through a system and quickly recover from mistakes is often defined as a highly usable system. But usability in systems design goes beyond a good interface and efficient navigation. In this article we describe two database systems in a law enforcement agency. One system is a legacy, text-based system with cumbersome navigation (RMS); the newer system is a graphical user interface with simplified navigation (CopNet). It is hypothesized that law enforcement users will evaluate CopNet higher than RMS, but experts of the older system will evaluate it higher than others will. We conducted two user studies. One study examined what users thought of RMS and CopNet, and compared RMS experts' evaluations with nonexperts. We found that all users evaluated CopNet as more effective, easier to use, and easier to navigate than RMS, and this was especially noticeable for users who were not experts with the older system. The second, follow-up study examined use behavior after CopNet was deployed some time later. The findings revealed that evaluations of CopNet were not associated with its use. If the newer system had a better interface and was easier to navigate than the older, legacy system, why were law enforcement personnel reluctant to switch? We discuss reasons why switching to a new system is difficult, especially for those who are most adept at using the older system. Implications for system design and usability are also discussed","tok_text":"when a better interfac and easi navig are n't enough : examin the inform architectur in a law enforc agenc \n an inform architectur that allow user to easili navig through a system and quickli recov from mistak is often defin as a highli usabl system . but usabl in system design goe beyond a good interfac and effici navig . in thi articl we describ two databas system in a law enforc agenc . one system is a legaci , text-bas system with cumbersom navig ( rm ) ; the newer system is a graphic user interfac with simplifi navig ( copnet ) . it is hypothes that law enforc user will evalu copnet higher than rm , but expert of the older system will evalu it higher than other will . we conduct two user studi . one studi examin what user thought of rm and copnet , and compar rm expert ' evalu with nonexpert . we found that all user evalu copnet as more effect , easier to use , and easier to navig than rm , and thi wa especi notic for user who were not expert with the older system . the second , follow-up studi examin use behavior after copnet wa deploy some time later . the find reveal that evalu of copnet were not associ with it use . if the newer system had a better interfac and wa easier to navig than the older , legaci system , whi were law enforc personnel reluct to switch ? we discuss reason whi switch to a new system is difficult , especi for those who are most adept at use the older system . implic for system design and usabl are also discuss","ordered_present_kp":[66,90,70,486,513,530,561],"keyphrases":["information architecture","RMS","law enforcement agency","graphical user interface","simplified navigation","CopNet","law enforcement users","legacy text-based system"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"2045","title":"Building a better game through dynamic programming: a Flip analysis","abstract":"Flip is a solitaire board game produced by craft woodworkers. We analyze Flip and suggest modifications to the rules to make the game more marketable. In addition to being an interesting application of dynamic programming, this case shows the use of operations research in managerial decision making","tok_text":"build a better game through dynam program : a flip analysi \n flip is a solitair board game produc by craft woodwork . we analyz flip and suggest modif to the rule to make the game more market . in addit to be an interest applic of dynam program , thi case show the use of oper research in manageri decis make","ordered_present_kp":[28,46,272,289,71,101],"keyphrases":["dynamic programming","Flip analysis","solitaire board game","craft woodworkers","operations research","managerial decision making"],"prmu":["P","P","P","P","P","P"]}
{"id":"2000","title":"Generalized predictive control for non-uniformly sampled systems","abstract":"In this paper, we study digital control systems with non-uniform updating and sampling patterns, which include multirate sampled-data systems as special cases. We derive lifted models in the state-space domain. The main obstacle for generalized predictive control (GPC) design using the lifted models is the so-called causality constraint. Taking into account this design constraint, we propose a new GPC algorithm, which results in optimal causal control laws for the non-uniformly sampled systems. The solution applies immediately to multirate sampled-data systems where rates are integer multiples of some base period","tok_text":"gener predict control for non-uniformli sampl system \n in thi paper , we studi digit control system with non-uniform updat and sampl pattern , which includ multir sampled-data system as special case . we deriv lift model in the state-spac domain . the main obstacl for gener predict control ( gpc ) design use the lift model is the so-cal causal constraint . take into account thi design constraint , we propos a new gpc algorithm , which result in optim causal control law for the non-uniformli sampl system . the solut appli immedi to multir sampled-data system where rate are integ multipl of some base period","ordered_present_kp":[79,156,293,339,449,579],"keyphrases":["digital control systems","multirate sampled-data systems","GPC","causality constraint","optimal causal control laws","integer multiples","generalized predictive control design","nonuniformly sampled systems","nonuniform updating patterns","nonuniform sampling patterns","state-space models"],"prmu":["P","P","P","P","P","P","R","M","M","M","R"]}
{"id":"257","title":"Unsafe at any speed?","abstract":"While Sun prides itself on Java's secure sandbox programming model, Microsoft takes a looser approach. Its C# language incorporates C-like concepts, including pointers and memory management. But is unsafe code really a boon to programmers, or is it a step backward?","tok_text":"unsaf at ani speed ? \n while sun pride itself on java 's secur sandbox program model , microsoft take a looser approach . it c # languag incorpor c-like concept , includ pointer and memori manag . but is unsaf code realli a boon to programm , or is it a step backward ?","ordered_present_kp":[146,170,182],"keyphrases":["C-like concepts","pointers","memory management","Microsoft C# language","Sun Java secure sandbox programming model"],"prmu":["P","P","P","R","R"]}
{"id":"212","title":"Knowledge management-capturing the skills of key performers in the power industry","abstract":"The growing pressure to reduce the cost of electrical power in recent years has resulted in an enormous \"brain-drain\" within the power industry. A novel approach has been developed by Eskom to capture these skills before they are lost and to incorporate these into a computer-based programme called \"knowledge management\"","tok_text":"knowledg management-captur the skill of key perform in the power industri \n the grow pressur to reduc the cost of electr power in recent year ha result in an enorm \" brain-drain \" within the power industri . a novel approach ha been develop by eskom to captur these skill befor they are lost and to incorpor these into a computer-bas programm call \" knowledg manag \"","ordered_present_kp":[59,40,0,166,244,321],"keyphrases":["knowledge management","key performers","power industry","brain-drain","Eskom","computer-based programme","skills capture","South Africa","personnel management"],"prmu":["P","P","P","P","P","P","R","U","M"]}
{"id":"1991","title":"A framework for evaluating the data-hiding capacity of image sources","abstract":"An information-theoretic model for image watermarking and data hiding is presented in this paper. Previous theoretical results are used to characterize the fundamental capacity limits of image watermarking and data-hiding systems. Capacity is determined by the statistical model used for the host image, by the distortion constraints on the data hider and the attacker, and by the information available to the data hider, to the attacker, and to the decoder. We consider autoregressive, block-DCT, and wavelet statistical models for images and compute data-hiding capacity for compressed and uncompressed host-image sources. Closed-form expressions are obtained under sparse-model approximations. Models for geometric attacks and distortion measures that are invariant to such attacks are considered","tok_text":"a framework for evalu the data-hid capac of imag sourc \n an information-theoret model for imag watermark and data hide is present in thi paper . previou theoret result are use to character the fundament capac limit of imag watermark and data-hid system . capac is determin by the statist model use for the host imag , by the distort constraint on the data hider and the attack , and by the inform avail to the data hider , to the attack , and to the decod . we consid autoregress , block-dct , and wavelet statist model for imag and comput data-hid capac for compress and uncompress host-imag sourc . closed-form express are obtain under sparse-model approxim . model for geometr attack and distort measur that are invari to such attack are consid","ordered_present_kp":[26,44,60,95,203,280,325,498,572,601,638,672,691],"keyphrases":["data-hiding capacity","image sources","information-theoretic model","watermarking","capacity limits","statistical model","distortion constraints","wavelet statistical models","uncompressed host-image sources","closed-form expressions","sparse-model approximations","geometric attacks","distortion measures","autoregressive statistical models","block-DCT statistical models","compressed host-image sources"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"41","title":"Controller performance analysis with LQG benchmark obtained under closed loop conditions","abstract":"This paper proposes a new method for obtaining a linear quadratic Gaussian (LQG) benchmark in terms of the variances of process input and output from closed-loop data, for assessing the controller performance. LQG benchmark has been proposed in the literature to assess controller performance since the LQG tradeoff curve represents the limit of performance in terms of input and output variances. However, an explicit parametric model is required to calculate the LQG benchmark. In this work, we propose a data driven subspace approach to calculate the LQG benchmark under closed-loop conditions with certain external excitations. The optimal LQG-benchmark variances are obtained directly from the subspace matrices corresponding to the deterministic inputs and the stochastic inputs, which are identified using closed-loop data with setpoint excitation. These variances are used for assessing the controller performance. The method proposed in this paper is applicable to both univariate and multivariate systems. Profit analysis for the implementation of feedforward control to the existing feedback-only control system is also analyzed under the optimal LQG performance framework","tok_text":"control perform analysi with lqg benchmark obtain under close loop condit \n thi paper propos a new method for obtain a linear quadrat gaussian ( lqg ) benchmark in term of the varianc of process input and output from closed-loop data , for assess the control perform . lqg benchmark ha been propos in the literatur to assess control perform sinc the lqg tradeoff curv repres the limit of perform in term of input and output varianc . howev , an explicit parametr model is requir to calcul the lqg benchmark . in thi work , we propos a data driven subspac approach to calcul the lqg benchmark under closed-loop condit with certain extern excit . the optim lqg-benchmark varianc are obtain directli from the subspac matric correspond to the determinist input and the stochast input , which are identifi use closed-loop data with setpoint excit . these varianc are use for assess the control perform . the method propos in thi paper is applic to both univari and multivari system . profit analysi for the implement of feedforward control to the exist feedback-onli control system is also analyz under the optim lqg perform framework","ordered_present_kp":[0,29,217,706,739,765,960,979,1015],"keyphrases":["controller performance analysis","LQG benchmark","closed-loop data","subspace matrices","deterministic inputs","stochastic inputs","multivariate systems","profit analysis","feedforward control","linear quadratic Gaussian benchmark","univariate systems","state space model"],"prmu":["P","P","P","P","P","P","P","P","P","R","R","M"]}
{"id":"2158","title":"Press shop. Industrial IT solutions for the press shop","abstract":"Globalization of the world's markets is challenging the traditional limits of manufacturing efficiency. The competitive advantage belongs to those who understand the new requirements and opportunities, and who commit to integrated solutions that span the value chain all the way from demand to production. ABB's automation and IT expertise and the process know-how gained from its long involvement with the automotive industry, have been brought together in new, state-of-the-art software solutions for press shops. Integrated into Industrial IT architecture, they allow the full potential of the shops to be realized, with advantages at every step in the supply chain","tok_text":"press shop . industri it solut for the press shop \n global of the world 's market is challeng the tradit limit of manufactur effici . the competit advantag belong to those who understand the new requir and opportun , and who commit to integr solut that span the valu chain all the way from demand to product . abb 's autom and it expertis and the process know-how gain from it long involv with the automot industri , have been brought togeth in new , state-of-the-art softwar solut for press shop . integr into industri it architectur , they allow the full potenti of the shop to be realiz , with advantag at everi step in the suppli chain","ordered_present_kp":[0,13,114,317,451,468,627],"keyphrases":["press shops","industrial IT solutions","manufacturing efficiency","automation","state-of-the-art","software solutions","supply chain","market globalisation","car manufacturing business"],"prmu":["P","P","P","P","P","P","P","M","M"]}
{"id":"2120","title":"Control in active systems based on criteria and motivation","abstract":"For active systems where the principal varies the agents' goal functions by adding to them appropriately weighted goal functions of other agents or a balanced system of inter-agent transfers, the paper formulated and solved the problems of control based on criteria and motivation. Linear active systems were considered by way of example","tok_text":"control in activ system base on criteria and motiv \n for activ system where the princip vari the agent ' goal function by ad to them appropri weight goal function of other agent or a balanc system of inter-ag transfer , the paper formul and solv the problem of control base on criteria and motiv . linear activ system were consid by way of exampl","ordered_present_kp":[105,200,298],"keyphrases":["goal functions","inter-agent transfers","linear active systems","criteria-based control","motivation-based control"],"prmu":["P","P","P","M","M"]}
{"id":"39","title":"Supervisory control design based on hybrid systems and fuzzy events detection. Application to an oxichlorination reactor","abstract":"This paper presents a supervisory control scheme based on hybrid systems theory and fuzzy events detection. The fuzzy event detector is a linguistic model, which synthesizes complex relations between process variables and process events incorporating experts' knowledge about the process operation. This kind of detection allows the anticipation of appropriate control actions, which depend upon the selected membership functions used to characterize the process under scrutiny. The proposed supervisory control scheme was successfully implemented for an oxichlorination reactor in a vinyl monomer plant","tok_text":"supervisori control design base on hybrid system and fuzzi event detect . applic to an oxichlorin reactor \n thi paper present a supervisori control scheme base on hybrid system theori and fuzzi event detect . the fuzzi event detector is a linguist model , which synthes complex relat between process variabl and process event incorpor expert ' knowledg about the process oper . thi kind of detect allow the anticip of appropri control action , which depend upon the select membership function use to character the process under scrutini . the propos supervisori control scheme wa success implement for an oxichlorin reactor in a vinyl monom plant","ordered_present_kp":[0,35,87,239,270,292,312,363,427,473,629],"keyphrases":["supervisory control design","hybrid systems","oxichlorination reactor","linguistic model","complex relations","process variables","process events","process operation","control actions","membership functions","vinyl monomer plant","events detection. fuzzy","expert knowledge","reactor stability","raw material consumption","discrete events systems","reactive systems","finite state machines"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","M","U","M","M","U"]}
{"id":"297","title":"The service side of systems librarianship","abstract":"Describes the role of a systems librarian at a small academic library. Although online catalogs and the Internet are making library accessibility more convenient, the need for library buildings and professionals has not diminished. Typical duties of a systems librarian and the effects of new technology on librarianship are discussed. Services provided to other constituencies on campus and the blurring relationship between the library and computer services are also presented","tok_text":"the servic side of system librarianship \n describ the role of a system librarian at a small academ librari . although onlin catalog and the internet are make librari access more conveni , the need for librari build and profession ha not diminish . typic duti of a system librarian and the effect of new technolog on librarianship are discuss . servic provid to other constitu on campu and the blur relationship between the librari and comput servic are also present","ordered_present_kp":[19,4,86,118,140],"keyphrases":["service side","systems librarianship","small academic library","online catalogs","Internet"],"prmu":["P","P","P","P","P"]}
{"id":"2165","title":"Naomi Campbell: drugs, distress and the Data Protection Act","abstract":"In the first case of its kind, Naomi Campbell successfully sued Mirror Group Newspapers for damage and distress caused by breach of the Data Protection Act 1998. Partner N. Wildish and assistant M. Turle of City law firm Field Fisher Waterhouse discuss the case and the legal implications of which online publishers should be aware","tok_text":"naomi campbel : drug , distress and the data protect act \n in the first case of it kind , naomi campbel success su mirror group newspap for damag and distress caus by breach of the data protect act 1998 . partner n. wildish and assist m. turl of citi law firm field fisher waterhous discuss the case and the legal implic of which onlin publish should be awar","ordered_present_kp":[16,23,40,0,330],"keyphrases":["Naomi Campbell","drugs","distress","Data Protection Act","online publishers"],"prmu":["P","P","P","P","P"]}
{"id":"2198","title":"Reconfigurable context-sensitive middleware for pervasive computing","abstract":"Context-sensitive applications need data from sensors, devices, and user actions, and might need ad hoc communication support to dynamically discover new devices and engage in spontaneous information exchange. Reconfigurable Context-Sensitive Middleware facilitates the development and runtime operations of context-sensitive pervasive computing software","tok_text":"reconfigur context-sensit middlewar for pervas comput \n context-sensit applic need data from sensor , devic , and user action , and might need ad hoc commun support to dynam discov new devic and engag in spontan inform exchang . reconfigur context-sensit middlewar facilit the develop and runtim oper of context-sensit pervas comput softwar","ordered_present_kp":[40,0,304,26,56],"keyphrases":["Reconfigurable Context-Sensitive Middleware","middleware","pervasive computing","context-sensitive applications","context-sensitive pervasive computing"],"prmu":["P","P","P","P","P"]}
{"id":"1951","title":"Recording quantum properties of light in a long-lived atomic spin state: towards quantum memory","abstract":"We report an experiment on mapping a quantum state of light onto the ground state spin of an ensemble of Cs atoms with the lifetime of 2 ms. Recording of one of the two quadrature phase operators of light is demonstrated with vacuum and squeezed states of light. The sensitivity of the mapping procedure at the level of approximately 1 photon\/sec per Hz is shown. The results pave the road towards complete (storing both quadrature phase observables) quantum memory for Gaussian states of light. The experiment also sheds new light on fundamental limits of sensitivity of the magneto-optical resonance method","tok_text":"record quantum properti of light in a long-liv atom spin state : toward quantum memori \n we report an experi on map a quantum state of light onto the ground state spin of an ensembl of cs atom with the lifetim of 2 ms . record of one of the two quadratur phase oper of light is demonstr with vacuum and squeez state of light . the sensit of the map procedur at the level of approxim 1 photon \/ sec per hz is shown . the result pave the road toward complet ( store both quadratur phase observ ) quantum memori for gaussian state of light . the experi also shed new light on fundament limit of sensit of the magneto-opt reson method","ordered_present_kp":[38,72,150,174,241,303,345,213,185],"keyphrases":["long-lived atomic spin state","quantum memory","ground state spin","ensemble","Cs","2 ms","two quadrature phase operators","squeezed states","mapping procedure","light quantum properties recording","vacuum states","magnetooptical resonance method"],"prmu":["P","P","P","P","P","P","P","P","P","R","R","M"]}
{"id":"196","title":"On the emergence of rules in neural networks","abstract":"A simple associationist neural network learns to factor abstract rules (i.e., grammars) from sequences of arbitrary input symbols by inventing abstract representations that accommodate unseen symbol sets as well as unseen but similar grammars. The neural network is shown to have the ability to transfer grammatical knowledge to both new symbol vocabularies and new grammars. Analysis of the state-space shows that the network learns generalized abstract structures of the input and is not simply memorizing the input strings. These representations are context sensitive, hierarchical, and based on the state variable of the finite-state machines that the neural network has learned. Generalization to new symbol sets or grammars arises from the spatial nature of the internal representations used by the network, allowing new symbol sets to be encoded close to symbol sets that have already been learned in the hidden unit space of the network. The results are counter to the arguments that learning algorithms based on weight adaptation after each exemplar presentation (such as the long term potentiation found in the mammalian nervous system) cannot in principle extract symbolic knowledge from positive examples as prescribed by prevailing human linguistic theory and evolutionary psychology","tok_text":"on the emerg of rule in neural network \n a simpl associationist neural network learn to factor abstract rule ( i.e. , grammar ) from sequenc of arbitrari input symbol by invent abstract represent that accommod unseen symbol set as well as unseen but similar grammar . the neural network is shown to have the abil to transfer grammat knowledg to both new symbol vocabulari and new grammar . analysi of the state-spac show that the network learn gener abstract structur of the input and is not simpli memor the input string . these represent are context sensit , hierarch , and base on the state variabl of the finite-st machin that the neural network ha learn . gener to new symbol set or grammar aris from the spatial natur of the intern represent use by the network , allow new symbol set to be encod close to symbol set that have alreadi been learn in the hidden unit space of the network . the result are counter to the argument that learn algorithm base on weight adapt after each exemplar present ( such as the long term potenti found in the mammalian nervou system ) can not in principl extract symbol knowledg from posit exampl as prescrib by prevail human linguist theori and evolutionari psycholog","ordered_present_kp":[49,79,95,24,405,1101],"keyphrases":["neural network","associationist neural network","learns","abstract rules","state-space","symbolic knowledge","cognitive neurosciences","associationist learning"],"prmu":["P","P","P","P","P","P","U","R"]}
{"id":"2085","title":"An intelligent fuzzy decision system for a flexible manufacturing system with multi-decision points","abstract":"This paper describes an intelligent fuzzy decision support system for real-time scheduling and dispatching of parts in a flexible manufacturing system (FMS), with alternative routing possibilities for all parts. A fuzzy logic approach is developed to improve the system performance by considering multiple performance measures and at multiple decision points. The characteristics of the system status, instead of parts, are fed back to assign priority to the parts waiting to be processed. A simulation model is developed and it is shown that the proposed intelligent fuzzy decision support system keeps all performance measures at a good level. The proposed intelligent system is a promising tool for dealing with scheduling FMSs, in contrast to traditional rules","tok_text":"an intellig fuzzi decis system for a flexibl manufactur system with multi-decis point \n thi paper describ an intellig fuzzi decis support system for real-tim schedul and dispatch of part in a flexibl manufactur system ( fm ) , with altern rout possibl for all part . a fuzzi logic approach is develop to improv the system perform by consid multipl perform measur and at multipl decis point . the characterist of the system statu , instead of part , are fed back to assign prioriti to the part wait to be process . a simul model is develop and it is shown that the propos intellig fuzzi decis support system keep all perform measur at a good level . the propos intellig system is a promis tool for deal with schedul fmss , in contrast to tradit rule","ordered_present_kp":[37,220,269,370,158,516],"keyphrases":["flexible manufacturing system","scheduling","FMS","fuzzy logic","multiple decision points","simulation","intelligent decision support system","real-time system"],"prmu":["P","P","P","P","P","P","R","R"]}
{"id":"332","title":"Fitting mixed-effects models for repeated ordinal outcomes with the NLMIXED procedure","abstract":"This paper presents an analysis of repeated ordinal outcomes arising from two psychological studies. The first case is a repeated measures analysis of variance; the second is a mixed-effects regression. in a longitudinal design. In both, the subject-specific variation is modeled by including random effects in the linear predictor (inside a link function) of a generalized linear model. The NLMIXED procedure in SAS is used to fit the mixed-effects models for the categorical response data. The presentation emphasizes the parallel between the model. specifications and the SAS statements. The purpose of this paper is to facilitate the use of mixed-effects models in the analysis of repeated ordinal outcomes","tok_text":"fit mixed-effect model for repeat ordin outcom with the nlmix procedur \n thi paper present an analysi of repeat ordin outcom aris from two psycholog studi . the first case is a repeat measur analysi of varianc ; the second is a mixed-effect regress . in a longitudin design . in both , the subject-specif variat is model by includ random effect in the linear predictor ( insid a link function ) of a gener linear model . the nlmix procedur in sa is use to fit the mixed-effect model for the categor respons data . the present emphas the parallel between the model . specif and the sa statement . the purpos of thi paper is to facilit the use of mixed-effect model in the analysi of repeat ordin outcom","ordered_present_kp":[27,139,177,228,256,331,352,400,56,491],"keyphrases":["repeated ordinal outcomes","NLMIXED procedure","psychological studies","repeated measures analysis of variance","mixed-effects regression","longitudinal design","random effects","linear predictor","generalized linear model","categorical response data","subject-specific variation modeling","mixed-effects model fitting","model specifications"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"377","title":"MATLAB code for plotting ambiguity functions","abstract":"A MATLAB code capable of plotting ambiguity functions of many different radar signals is presented. The program makes use of MATLAB's sparse matrix operations, and avoids loops. The program could be useful as a pedagogical tool in radar courses teaching pulse compression","tok_text":"matlab code for plot ambigu function \n a matlab code capabl of plot ambigu function of mani differ radar signal is present . the program make use of matlab 's spars matrix oper , and avoid loop . the program could be use as a pedagog tool in radar cours teach puls compress","ordered_present_kp":[0,99,159,226,242,260],"keyphrases":["MATLAB code","radar signals","sparse matrix operations","pedagogical tool","radar courses","pulse compression","ambiguity functions plotting","matched-filter response","Doppler-shifted signal version"],"prmu":["P","P","P","P","P","P","R","U","M"]}
{"id":"1930","title":"A new method of systemological analysis coordinated with the procedure of object-oriented design. II","abstract":"For pt.I. see Vestn. KhGPU, no.81, p.15-18 (2000). The paper presents the results of development of an object-oriented systemological method used to design complex systems. A formal system representation, as well as an axiomatics of the calculus of systems as functional flow-type objects based on a Node-Function-Object class hierarchy are proposed. A formalized NFO\/UFO analysis algorithm and CASE tools used to support it are considered","tok_text":"a new method of systemolog analysi coordin with the procedur of object-ori design . ii \n for pt . i. see vestn . khgpu , no.81 , p.15 - 18 ( 2000 ) . the paper present the result of develop of an object-ori systemolog method use to design complex system . a formal system represent , as well as an axiomat of the calculu of system as function flow-typ object base on a node-function-object class hierarchi are propos . a formal nfo \/ ufo analysi algorithm and case tool use to support it are consid","ordered_present_kp":[16,64,258,298,334,421,460],"keyphrases":["systemological analysis","object-oriented design","formal system representation","axiomatics","functional flow-type objects","formalized NFO\/UFO analysis algorithm","CASE tools","complex systems design"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"1975","title":"Efficient computation of local geometric moments","abstract":"Local moments have attracted attention as local features in applications such as edge detection and texture segmentation. The main reason for this is that they are inherently integral-based features, so that their use reduces the effect of uncorrelated noise. The computation of local moments, when viewed as a neighborhood operation, can be interpreted as a convolution of the image with a set of masks. Nevertheless, moments computed inside overlapping windows are not independent and convolution does not take this fact into account. By introducing a matrix formulation and the concept of accumulation moments, this paper presents an algorithm which is computationally much more efficient than convolving and yet as simple","tok_text":"effici comput of local geometr moment \n local moment have attract attent as local featur in applic such as edg detect and textur segment . the main reason for thi is that they are inher integral-bas featur , so that their use reduc the effect of uncorrel nois . the comput of local moment , when view as a neighborhood oper , can be interpret as a convolut of the imag with a set of mask . nevertheless , moment comput insid overlap window are not independ and convolut doe not take thi fact into account . by introduc a matrix formul and the concept of accumul moment , thi paper present an algorithm which is comput much more effici than convolv and yet as simpl","ordered_present_kp":[76,107,122,186,306,425,521,554],"keyphrases":["local features","edge detection","texture segmentation","integral-based features","neighborhood operation","overlapping windows","matrix formulation","accumulation moments","local geometric moments computation","image convolution","computationally efficient algorithm","image analysis"],"prmu":["P","P","P","P","P","P","P","P","R","R","R","M"]}
{"id":"2104","title":"Computer-mediated communication and remote management: integration or isolation?","abstract":"The use of intranets and e-mails to communicate with remote staff is increasing rapidly within organizations. For many companies this is viewed as a speedy and cost-effective way of keeping in contact with staff and ensuring their continuing commitment to company goals. This article highlights the problems experienced by staff when managers use intranets and e-mails in an inappropriate fashion for these purposes. Issues of remoteness and isolation are discussed, along with the reports of frustration and disidentification experienced. However, it will be shown that when used appropriately, communication using these technologies can facilitate shared understanding and help remote staff to view their company as alive and exciting. Theoretical aspects are highlighted and the implications of these findings are discussed","tok_text":"computer-medi commun and remot manag : integr or isol ? \n the use of intranet and e-mail to commun with remot staff is increas rapidli within organ . for mani compani thi is view as a speedi and cost-effect way of keep in contact with staff and ensur their continu commit to compani goal . thi articl highlight the problem experienc by staff when manag use intranet and e-mail in an inappropri fashion for these purpos . issu of remot and isol are discuss , along with the report of frustrat and disidentif experienc . howev , it will be shown that when use appropri , commun use these technolog can facilit share understand and help remot staff to view their compani as aliv and excit . theoret aspect are highlight and the implic of these find are discuss","ordered_present_kp":[0,25,69,82,104,142,159,195,31,25],"keyphrases":["computer-mediated communication","remote management","remoteness","managers","intranets","e-mails","remote staff","organizations","companies","cost-effective"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"2141","title":"System embedding. Control with reduced observer","abstract":"Two interrelated problems-design of the reduced observer of plant state separately and together with its control system-were considered from the standpoint of designing the multivariable linear systems from the desired matrix transfer functions. The matrix equations defining the entire constructive class of solutions of the posed problems were obtained using the system embedding technology. As was demonstrated, control based on the reduced observer is capable to provide the desired response to the control input, as well as the response to the nonzero initial conditions, only for the directly measurable part of the components of the state vector. An illustrative example was presented","tok_text":"system embed . control with reduc observ \n two interrel problems-design of the reduc observ of plant state separ and togeth with it control system-wer consid from the standpoint of design the multivari linear system from the desir matrix transfer function . the matrix equat defin the entir construct class of solut of the pose problem were obtain use the system embed technolog . as wa demonstr , control base on the reduc observ is capabl to provid the desir respons to the control input , as well as the respons to the nonzero initi condit , onli for the directli measur part of the compon of the state vector . an illustr exampl wa present","ordered_present_kp":[0,192,231,600],"keyphrases":["system embedding","multivariable linear systems","matrix transfer functions","state vector","reduced observer control","reduced plant state observer design"],"prmu":["P","P","P","P","R","R"]}
{"id":"1988","title":"Integration is key - an introduction to enterprise application integration (EAI) technology","abstract":"Over the past few years, numerous organisations have invested in the latest software applications to drive their business forward. But many are now finding that these systems are becoming redundant on their own. The key to staying ahead of the competition in today's current climate is now to integrate all of these systems, says Justin Opie, Portfolio Director at Imark Communications","tok_text":"integr is key - an introduct to enterpris applic integr ( eai ) technolog \n over the past few year , numer organis have invest in the latest softwar applic to drive their busi forward . but mani are now find that these system are becom redund on their own . the key to stay ahead of the competit in today 's current climat is now to integr all of these system , say justin opi , portfolio director at imark commun","ordered_present_kp":[32,401],"keyphrases":["enterprise application integration","Imark Communications"],"prmu":["P","P"]}
{"id":"2019","title":"Effective moving cast shadow detection for monocular color traffic image sequences","abstract":"For an accurate scene analysis using monocular color traffic image sequences, a robust segmentation of moving vehicles from the stationary background is generally required. However, the presence of moving cast shadow may lead to an inaccurate vehicle segmentation, and as a result, may lead to further erroneous scene analysis. We propose an effective method for the detection of moving cast shadow. By observing the characteristics of cast shadow in the luminance, chrominance, gradient density, and geometry domains, a combined probability map, called a shadow confidence score (SCS), is obtained. From the edge map of the input image, each edge pixel is examined to determine whether it belongs to the vehicle region based on its neighboring SCSs. The cast shadow is identified as those regions with high SCSs, which are outside the convex hull of the selected vehicle edge pixels. The proposed method is tested on 100 vehicle images taken under different lighting conditions (sunny and cloudy), viewing angles (roadside and overhead), vehicle sizes (small, medium, and large), and colors (similar to the road and not). The results indicate that an average error rate of around 14% is obtained while the lowest error rate is around 3% for large vehicles","tok_text":"effect move cast shadow detect for monocular color traffic imag sequenc \n for an accur scene analysi use monocular color traffic imag sequenc , a robust segment of move vehicl from the stationari background is gener requir . howev , the presenc of move cast shadow may lead to an inaccur vehicl segment , and as a result , may lead to further erron scene analysi . we propos an effect method for the detect of move cast shadow . by observ the characterist of cast shadow in the lumin , chromin , gradient densiti , and geometri domain , a combin probabl map , call a shadow confid score ( sc ) , is obtain . from the edg map of the input imag , each edg pixel is examin to determin whether it belong to the vehicl region base on it neighbor scss . the cast shadow is identifi as those region with high scss , which are outsid the convex hull of the select vehicl edg pixel . the propos method is test on 100 vehicl imag taken under differ light condit ( sunni and cloudi ) , view angl ( roadsid and overhead ) , vehicl size ( small , medium , and larg ) , and color ( similar to the road and not ) . the result indic that an averag error rate of around 14 % is obtain while the lowest error rate is around 3 % for larg vehicl","ordered_present_kp":[0,35,81,146,164,185,7,280,343,478,486,496,519,539,567,632,12,830,849,939,908,954,964,975,1012,1125],"keyphrases":["effective moving cast shadow detection","moving cast shadow","cast shadow","monocular color traffic image sequences","accurate scene analysis","robust segmentation","moving vehicles","stationary background","inaccurate vehicle segmentation","erroneous scene analysis","luminance","chrominance","gradient density","geometry domains","combined probability map","shadow confidence score","input image","convex hull","selected vehicle edge pixels","vehicle images","lighting conditions","sunny","cloudy","viewing angles","vehicle sizes","average error rate","image segmentation"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"3","title":"NuVox shows staying power with new cash, new market","abstract":"Who says you can't raise cash in today's telecom market? NuVox Communications positions itself for the long run with $78.5 million in funding and a new credit facility","tok_text":"nuvox show stay power with new cash , new market \n who say you ca n't rais cash in today 's telecom market ? nuvox commun posit itself for the long run with $ 78.5 million in fund and a new credit facil","ordered_present_kp":[92,109],"keyphrases":["telecom","NuVox Communications","competitive carrier market","investors"],"prmu":["P","P","M","U"]}
{"id":"316","title":"Duality revisited: construction of fractional frequency distributions based on two dual Lotka laws","abstract":"Fractional frequency distributions of, for example, authors with a certain (fractional) number of papers are very irregular, and therefore not easy to model or to explain. The article gives a first attempt to this by as suming two simple Lotka laws (with exponent 2): one for the number of authors with n papers (total count here) and one for the number of papers with n authors, n in N. Based on an earlier made convolution model of Egghe, interpreted and reworked now for discrete scores, we are able to produce theoretical fractional frequency distributions with only one parameter, which are in very close agreement with the practical ones as found in a large dataset produced earlier by Rao (1995). The article also shows that (irregular) fractional frequency distributions are a consequence of Lotka's law, and are not examples of breakdowns of this famous historical law","tok_text":"dualiti revisit : construct of fraction frequenc distribut base on two dual lotka law \n fraction frequenc distribut of , for exampl , author with a certain ( fraction ) number of paper are veri irregular , and therefor not easi to model or to explain . the articl give a first attempt to thi by as sume two simpl lotka law ( with expon 2 ): one for the number of author with n paper ( total count here ) and one for the number of paper with n author , n in n. base on an earlier made convolut model of eggh , interpret and rework now for discret score , we are abl to produc theoret fraction frequenc distribut with onli one paramet , which are in veri close agreement with the practic one as found in a larg dataset produc earlier by rao ( 1995 ) . the articl also show that ( irregular ) fraction frequenc distribut are a consequ of lotka 's law , and are not exampl of breakdown of thi famou histor law","ordered_present_kp":[71,484,538],"keyphrases":["dual Lotka laws","convolution model","discrete scores","irregular fractional frequency distributions"],"prmu":["P","P","P","R"]}
{"id":"353","title":"Edit distance of run-length encoded strings","abstract":"Let X and Y be two run-length encoded strings, of encoded lengths k and l, respectively. We present a simple O(|X|l+|Y|k) time algorithm that computes their edit distance","tok_text":"edit distanc of run-length encod string \n let x and y be two run-length encod string , of encod length k and l , respect . we present a simpl o(|x|l+|y|k ) time algorithm that comput their edit distanc","ordered_present_kp":[16,90,161,0],"keyphrases":["edit distance","run-length encoded strings","encoded lengths","algorithm","computation time"],"prmu":["P","P","P","P","R"]}
{"id":"406","title":"Windows XP fast user switching","abstract":"The Windows NT family of operating systems has always supported the concept of multiple user accounts, but they've taken the concept a step further with Windows XP's Fast User Switching feature. Fast User Switching is a new feature of Windows XP that allows multiple users to log on to the same machine and quickly switch between the logged on accounts. Fast User Switching is implemented using some of the built-in capabilities of Terminal Services. Terminal Server has been around for a while but is much more feature rich and integrated in Windows XP. A machine with the terminal services (Remote Desktop) client can log on to and run applications on a remote machine running the terminal server","tok_text":"window xp fast user switch \n the window nt famili of oper system ha alway support the concept of multipl user account , but they 've taken the concept a step further with window xp 's fast user switch featur . fast user switch is a new featur of window xp that allow multipl user to log on to the same machin and quickli switch between the log on account . fast user switch is implement use some of the built-in capabl of termin servic . termin server ha been around for a while but is much more featur rich and integr in window xp . a machin with the termin servic ( remot desktop ) client can log on to and run applic on a remot machin run the termin server","ordered_present_kp":[0,53,97,422,438,568],"keyphrases":["Windows XP Fast User Switching","operating systems","multiple user accounts","Terminal Services","Terminal Server","Remote Desktop","multiple user logon access"],"prmu":["P","P","P","P","P","P","M"]}
{"id":"2061","title":"Acquisitions in the James Ford Bell Library","abstract":"This article presents basic acquisitions philosophy and approaches in a noted special collection, with commentary on \"just saying no\" and on how the electronic revolution has changed the acquisition of special collections materials","tok_text":"acquisit in the jame ford bell librari \n thi articl present basic acquisit philosophi and approach in a note special collect , with commentari on \" just say no \" and on how the electron revolut ha chang the acquisit of special collect materi","ordered_present_kp":[16,109,177],"keyphrases":["James Ford Bell Library","special collections","electronic revolution","library acquisitions philosophy","out-of-print books","University library"],"prmu":["P","P","P","R","U","M"]}
{"id":"2024","title":"Binocular model for figure-ground segmentation in translucent and occluding images","abstract":"A Fourier-based solution to the problem of figure-ground segmentation in short baseline binocular image pairs is presented. Each image is modeled as an additive composite of two component images that exhibit a spatial shift due to the binocular parallax. The segmentation is accomplished by decoupling each Fourier component in one of the resultant additive images into its two constituent phasors, allocating each to its appropriate object-specific spectrum, and then reconstructing the foreground and background using the inverse Fourier transform. It is shown that the foreground and background shifts can be computed from the differences of the magnitudes and phases of the Fourier transform of the binocular image pair. While the model is based on translucent objects, it also works with occluding objects","tok_text":"binocular model for figure-ground segment in transluc and occlud imag \n a fourier-bas solut to the problem of figure-ground segment in short baselin binocular imag pair is present . each imag is model as an addit composit of two compon imag that exhibit a spatial shift due to the binocular parallax . the segment is accomplish by decoupl each fourier compon in one of the result addit imag into it two constitu phasor , alloc each to it appropri object-specif spectrum , and then reconstruct the foreground and background use the invers fourier transform . it is shown that the foreground and background shift can be comput from the differ of the magnitud and phase of the fourier transform of the binocular imag pair . while the model is base on transluc object , it also work with occlud object","ordered_present_kp":[0,20,58,65,74,135,229,256,281,412,447,497,512,531,149,748,784],"keyphrases":["binocular model","figure-ground segmentation","occluding images","images","Fourier-based solution","short baseline binocular image pairs","binocular image pair","component images","spatial shift","binocular parallax","phasors","object-specific spectrum","foreground","background","inverse Fourier transform","translucent objects","occluding objects","translucent images","image segmentation","Fourier component decoupling"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"393","title":"The use of the SPSA method in ECG analysis","abstract":"The classification, monitoring, and compression of electrocardiogram (ECG) signals recorded of a single patient over a relatively long period of time is considered. The particular application we have in mind is high-resolution ECG analysis, such as late potential analysis, morphology changes in QRS during arrythmias, T-wave alternants, or the study of drug effects on ventricular activation. We propose to apply a modification of a classical method of cluster analysis or vector quantization. The novelty of our approach is that we use a new distortion measure to quantify the distance of two ECG cycles, and the class-distortion measure is defined using a min-max criterion. The new class-distortion-measure is much more sensitive to outliers than the usual distortion measures using average-distance. The price of this practical advantage is that computational complexity is significantly increased. The resulting nonsmooth optimization problem is solved by an adapted version of the simultaneous perturbation stochastic approximation (SPSA) method of J. Spall (IEEE Trans. Automat. Contr., vol. 37, p. 332-41, Mar. 1992). The main idea is to generate a smooth approximation by a randomization procedure. The viability of the method is demonstrated on both simulated and real data. An experimental comparison with the widely used correlation method is given on real data","tok_text":"the use of the spsa method in ecg analysi \n the classif , monitor , and compress of electrocardiogram ( ecg ) signal record of a singl patient over a rel long period of time is consid . the particular applic we have in mind is high-resolut ecg analysi , such as late potenti analysi , morpholog chang in qr dure arrythmia , t-wave altern , or the studi of drug effect on ventricular activ . we propos to appli a modif of a classic method of cluster analysi or vector quantiz . the novelti of our approach is that we use a new distort measur to quantifi the distanc of two ecg cycl , and the class-distort measur is defin use a min-max criterion . the new class-distortion-measur is much more sensit to outlier than the usual distort measur use average-dist . the price of thi practic advantag is that comput complex is significantli increas . the result nonsmooth optim problem is solv by an adapt version of the simultan perturb stochast approxim ( spsa ) method of j. spall ( ieee tran . automat . contr . , vol . 37 , p. 332 - 41 , mar. 1992 ) . the main idea is to gener a smooth approxim by a random procedur . the viabil of the method is demonstr on both simul and real data . an experiment comparison with the wide use correl method is given on real data","ordered_present_kp":[655,854,441,1098,1226,526,572],"keyphrases":["cluster analysis","distortion measure","ECG cycles","class-distortion-measure","nonsmooth optimization problem","randomization procedure","correlation method","simultaneous perturbation stochastic approximation method","electrodiagnostics","ECG signals compression"],"prmu":["P","P","P","P","P","P","P","R","U","R"]}
{"id":"20","title":"Adaptive state feedback control for a class of linear systems with unknown bounds of uncertainties","abstract":"The problem of adaptive robust stabilization for a class of linear time-varying systems with disturbance and nonlinear uncertainties is considered. The bounds of the disturbance and uncertainties are assumed to be unknown, being even arbitrary. For such uncertain dynamical systems, the adaptive robust state feedback controller is obtained. And the resulting closed-loop systems are asymptotically stable in theory. Moreover, an adaptive robust state feedback control scheme is given. The scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. Finally, simulations show that the control scheme is effective","tok_text":"adapt state feedback control for a class of linear system with unknown bound of uncertainti \n the problem of adapt robust stabil for a class of linear time-vari system with disturb and nonlinear uncertainti is consid . the bound of the disturb and uncertainti are assum to be unknown , be even arbitrari . for such uncertain dynam system , the adapt robust state feedback control is obtain . and the result closed-loop system are asymptot stabl in theori . moreov , an adapt robust state feedback control scheme is given . the scheme ensur the closed-loop system exponenti practic stabl and can be use in practic engin . final , simul show that the control scheme is effect","ordered_present_kp":[115,144,185,407,315,6],"keyphrases":["state feedback","robust stabilization","linear time-varying systems","nonlinear uncertainties","uncertain dynamical systems","closed-loop systems","adaptive stabilization","adaptive controller","robust control","uncertain systems"],"prmu":["P","P","P","P","P","P","R","R","R","R"]}
{"id":"2139","title":"Generalized confidence sets for a statistically indeterminate random vector","abstract":"A problem is considered for the construction of confidence sets for a random vector, the information on distribution parameters of which is incomplete. To obtain exact estimates and a detailed analysis of the problem, the notion is introduced of a generalized confidence set for a statistically indeterminate random vector. Properties of generalized confidence sets are studied. It is shown that the standard method of estimation, which relies on the unification of confidence sets, leads in many cases to wider confidence estimates. For a normally distributed random vector with an inaccurately known mean value, generalized confidence sets are built tip and the dependence of sizes of a generalized confidence set on the forms and parameters of a set of possible mean values is examined","tok_text":"gener confid set for a statist indetermin random vector \n a problem is consid for the construct of confid set for a random vector , the inform on distribut paramet of which is incomplet . to obtain exact estim and a detail analysi of the problem , the notion is introduc of a gener confid set for a statist indetermin random vector . properti of gener confid set are studi . it is shown that the standard method of estim , which reli on the unif of confid set , lead in mani case to wider confid estim . for a normal distribut random vector with an inaccur known mean valu , gener confid set are built tip and the depend of size of a gener confid set on the form and paramet of a set of possibl mean valu is examin","ordered_present_kp":[0,23,146,510],"keyphrases":["generalized confidence sets","statistically indeterminate random vector","distribution parameters","normally distributed random vector"],"prmu":["P","P","P","P"]}
{"id":"1948","title":"Estimating populations for collective dose calculations","abstract":"The collective dose provides an estimate of the effects of facility operations on the public based on an estimate of the population in the area. Geographic information system software, electronic population data resources, and a personal computer were used to develop estimates of population within 80 km radii of two sites","tok_text":"estim popul for collect dose calcul \n the collect dose provid an estim of the effect of facil oper on the public base on an estim of the popul in the area . geograph inform system softwar , electron popul data resourc , and a person comput were use to develop estim of popul within 80 km radii of two site","ordered_present_kp":[16,88,106,157,190,226],"keyphrases":["collective dose calculations","facility operations","public","geographic information system software","electronic population data resources","personal computer"],"prmu":["P","P","P","P","P","P"]}
{"id":"273","title":"Chemical information based scaling of molecular descriptors: a universal chemical scale for library design and analysis","abstract":"Scaling is a difficult issue for any analysis of chemical properties or molecular topology when disparate descriptors are involved. To compare properties across different data sets, a common scale must be defined. Using several publicly available databases (ACD, CMC, MDDR, and NCI) as a basis, we propose to define chemically meaningful scales for a number of molecular properties and topology descriptors. These chemically derived scaling functions have several advantages. First, it is possible to define chemically relevant scales, greatly simplifying similarity and diversity analyses across data sets. Second, this approach provides a convenient method for setting descriptor boundaries that define chemically reasonable topology spaces. For example, descriptors can be scaled so that compounds with little potential for biological activity, bioavailability, or other drug-like characteristics are easily identified as outliers. We have compiled scaling values for 314 molecular descriptors. In addition the 10th and 90th percentile values for each descriptor have been calculated for use in outlier filtering","tok_text":"chemic inform base scale of molecular descriptor : a univers chemic scale for librari design and analysi \n scale is a difficult issu for ani analysi of chemic properti or molecular topolog when dispar descriptor are involv . to compar properti across differ data set , a common scale must be defin . use sever publicli avail databas ( acd , cmc , mddr , and nci ) as a basi , we propos to defin chemic meaning scale for a number of molecular properti and topolog descriptor . these chemic deriv scale function have sever advantag . first , it is possibl to defin chemic relev scale , greatli simplifi similar and divers analys across data set . second , thi approach provid a conveni method for set descriptor boundari that defin chemic reason topolog space . for exampl , descriptor can be scale so that compound with littl potenti for biolog activ , bioavail , or other drug-lik characterist are easili identifi as outlier . we have compil scale valu for 314 molecular descriptor . in addit the 10th and 90th percentil valu for each descriptor have been calcul for use in outlier filter","ordered_present_kp":[53,78,0,28,171,152,325,613,258,699,872,837,852,917],"keyphrases":["chemical information based scaling","molecular descriptors","universal chemical scale","library design","chemical properties","molecular topology","data sets","databases","diversity analyses","descriptor boundaries","biological activity","bioavailability","drug-like characteristics","outliers","library analysis","similarity analyses"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2181","title":"Development of a health guidance support system for lifestyle improvement","abstract":"The objective is to provide automated advice for lifestyle adjustment based on an assessment of the results of a questionnaire and medical examination or health checkup data. A system was developed that gathers data based on questions regarding weight gain, exercise, smoking, sleep, eating habits, salt intake, animal fat intake, snacks, alcohol, and oral hygiene, body mass index, resting blood pressure, fasting blood sugar, total cholesterol, triglycerides, uric acid and liver function tests. Based on the relationships between the lifestyle data and the health checkup data, a health assessment sheet was generated for persons being allocated to a multiple-risk factor syndrome group. Health assessment and useful advice for lifestyle improvement were automatically extracted with the system, toward the high risk group for life style related diseases. The system is operational. In comparison with conventional, limited advice methods, we developed a practical system that defined the necessity for lifestyle improvement more clearly, and made giving advice easier","tok_text":"develop of a health guidanc support system for lifestyl improv \n the object is to provid autom advic for lifestyl adjust base on an assess of the result of a questionnair and medic examin or health checkup data . a system wa develop that gather data base on question regard weight gain , exercis , smoke , sleep , eat habit , salt intak , anim fat intak , snack , alcohol , and oral hygien , bodi mass index , rest blood pressur , fast blood sugar , total cholesterol , triglycerid , uric acid and liver function test . base on the relationship between the lifestyl data and the health checkup data , a health assess sheet wa gener for person be alloc to a multiple-risk factor syndrom group . health assess and use advic for lifestyl improv were automat extract with the system , toward the high risk group for life style relat diseas . the system is oper . in comparison with convent , limit advic method , we develop a practic system that defin the necess for lifestyl improv more clearli , and made give advic easier","ordered_present_kp":[13,47,158,175,191,274,298,288,306,314,326,339,356,364,378,392,410,431,450,470,484,498],"keyphrases":["health guidance support system","lifestyle improvement","questionnaire","medical examination","health checkup data","weight gain","exercise","smoking","sleep","eating habits","salt intake","animal fat intake","snacks","alcohol","oral hygiene","body mass index","resting blood pressure","fasting blood sugar","total cholesterol","triglycerides","uric acid","liver function tests"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"236","title":"Licensing experiences in the Netherlands","abstract":"The licensing strategy of university libraries in the Netherlands is closely connected with university policies to develop document servers and to make research publications available on the Web. National agreements have been made with major publishers, such as Elsevier Science and Kluwer Academic, to provide access to a wide range of scientific information and to experiment with new ways of providing information and new business models","tok_text":"licens experi in the netherland \n the licens strategi of univers librari in the netherland is close connect with univers polici to develop document server and to make research public avail on the web . nation agreement have been made with major publish , such as elsevi scienc and kluwer academ , to provid access to a wide rang of scientif inform and to experi with new way of provid inform and new busi model","ordered_present_kp":[38,57,21,113,139,167,196,263,281,332,400],"keyphrases":["Netherlands","licensing strategy","university libraries","university policies","document servers","research publications","Web","Elsevier Science","Kluwer Academic","scientific information","business models"],"prmu":["P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"362","title":"On batch-constructing B\/sup +\/-trees: algorithm and its performance evaluation","abstract":"Efficient construction of indexes is very important in bulk-loading a database or adding a new index to an existing database since both of them should handle an enormous volume of data. In this paper, we propose an algorithm for batch-constructing the B\/sup +\/-tree, the most widely used index structure in database systems. The main characteristic of our algorithm is to simultaneously process all the key values to be placed on each B+-tree page when accessing the page. This avoids the overhead due to accessing the same page multiple times, which results from applying the B+-tree insertion algorithm repeatedly. For performance evaluation, we have analyzed our algorithm in terms of the number of disk accesses. The results show that the number of disk accesses excluding those in the relocation process is identical to the number of pages belonging to the B\/sup +\/-tree. Considering that the relocation process is an unavoidable preprocessing step for batch-constructing of B\/sup +\/-trees, our algorithm requires just one disk access per B+-tree page, and therefore turns out to be optimal. We also present the performance tendency in relation with different parameter values via simulation. Finally, we show the performance enhancement effect of our algorithm, compared with the one using repeated insertions through experiments","tok_text":"on batch-construct b \/ sup + \/-tree : algorithm and it perform evalu \n effici construct of index is veri import in bulk-load a databas or ad a new index to an exist databas sinc both of them should handl an enorm volum of data . in thi paper , we propos an algorithm for batch-construct the b \/ sup + \/-tree , the most wide use index structur in databas system . the main characterist of our algorithm is to simultan process all the key valu to be place on each b+-tree page when access the page . thi avoid the overhead due to access the same page multipl time , which result from appli the b+-tree insert algorithm repeatedli . for perform evalu , we have analyz our algorithm in term of the number of disk access . the result show that the number of disk access exclud those in the reloc process is ident to the number of page belong to the b \/ sup + \/-tree . consid that the reloc process is an unavoid preprocess step for batch-construct of b \/ sup + \/-tree , our algorithm requir just one disk access per b+-tree page , and therefor turn out to be optim . we also present the perform tendenc in relat with differ paramet valu via simul . final , we show the perform enhanc effect of our algorithm , compar with the one use repeat insert through experi","ordered_present_kp":[328,462,592,704,785,408],"keyphrases":["index structure","simulation","B+-tree page","B+-tree insertion algorithm","disk accesses","relocation process","B+-tree batch construction","algorithm performance evaluation","database bulk loading","page access"],"prmu":["P","P","P","P","P","P","M","R","M","R"]}
{"id":"2090","title":"All-optical logic NOR gate using two-cascaded semiconductor optical amplifiers","abstract":"The authors present a novel all-optical logic NOR gate using two-cascaded semiconductor optical. amplifiers (SOAs) in a counterpropagating feedback configuration. This configuration accentuates the gain nonlinearity due to the mutual gain modulation of the two SOAs. The all-optical NOR gate feasibility has been demonstrated delivering an extinction ratio higher than 12 dB over a wide range of wavelength","tok_text":"all-opt logic nor gate use two-cascad semiconductor optic amplifi \n the author present a novel all-opt logic nor gate use two-cascad semiconductor optic . amplifi ( soa ) in a counterpropag feedback configur . thi configur accentu the gain nonlinear due to the mutual gain modul of the two soa . the all-opt nor gate feasibl ha been demonstr deliv an extinct ratio higher than 12 db over a wide rang of wavelength","ordered_present_kp":[0,27,165,176,235,261,351],"keyphrases":["all-optical logic NOR gate","two-cascaded semiconductor optical amplifiers","SOA","counterpropagating feedback configuration","gain nonlinearity","mutual gain modulation","extinction ratio","wide wavelength range"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"327","title":"Using latent semantic analysis to assess reader strategies","abstract":"We tested a computer-based procedure for assessing reader strategies that was based on verbal protocols that utilized latent semantic analysis (LSA). Students were given self-explanation-reading training (SERT), which teaches strategies that facilitate self-explanation during reading, such as elaboration based on world knowledge and bridging between text sentences. During a computerized version of SERT practice, students read texts and typed self-explanations into a computer after each sentence. The use of SERT strategies during this practice was assessed by determining the extent to which students used the information in the current sentence versus the prior text or world knowledge in their self-explanations. This assessment was made on the basis of human judgments and LSA. Both human judgments and LSA were remarkably similar and indicated that students who were not complying with SERT tended to paraphrase the text sentences, whereas students who were compliant with SERT tended to explain the sentences in terms of what they knew about the world and of information provided in the prior text context. The similarity between human judgments and LSA indicates that LSA will be useful in accounting for reading strategies in a Web-based version of SERT","tok_text":"use latent semant analysi to assess reader strategi \n we test a computer-bas procedur for assess reader strategi that wa base on verbal protocol that util latent semant analysi ( lsa ) . student were given self-explanation-read train ( sert ) , which teach strategi that facilit self-explan dure read , such as elabor base on world knowledg and bridg between text sentenc . dure a computer version of sert practic , student read text and type self-explan into a comput after each sentenc . the use of sert strategi dure thi practic wa assess by determin the extent to which student use the inform in the current sentenc versu the prior text or world knowledg in their self-explan . thi assess wa made on the basi of human judgment and lsa . both human judgment and lsa were remark similar and indic that student who were not compli with sert tend to paraphras the text sentenc , wherea student who were compliant with sert tend to explain the sentenc in term of what they knew about the world and of inform provid in the prior text context . the similar between human judgment and lsa indic that lsa will be use in account for read strategi in a web-bas version of sert","ordered_present_kp":[4,64,129,206,311,326,716],"keyphrases":["latent semantic analysis","computer-based procedure","verbal protocols","self-explanation-reading training","elaboration","world knowledge","human judgments","reader strategy assessment","text sentence bridging"],"prmu":["P","P","P","P","P","P","P","R","R"]}
{"id":"2028","title":"Centroid detection based on optical correlation","abstract":"We propose three correlation-based methods to simultaneously detect the centroids of multiple objects in an input scene. The first method is based on the modulus of the moment function, the second method is based on squaring the moment function, and the third method works with a single intensity filter. These methods are invariant to changes in the position, orientation, and scale of the object and result in good noise-smoothing performance. We use spatial light modulators (SLMs) to directly implement the input of the image and filter information for the purpose of these approaches. We present results showing simulations from different approaches and provide comparisons between optical-correlation- and digital-moment-based methods. Experimental results corresponding to an optical correlator using SLMs for the centroid detection are also presented","tok_text":"centroid detect base on optic correl \n we propos three correlation-bas method to simultan detect the centroid of multipl object in an input scene . the first method is base on the modulu of the moment function , the second method is base on squar the moment function , and the third method work with a singl intens filter . these method are invari to chang in the posit , orient , and scale of the object and result in good noise-smooth perform . we use spatial light modul ( slm ) to directli implement the input of the imag and filter inform for the purpos of these approach . we present result show simul from differ approach and provid comparison between optical-correlation- and digital-moment-bas method . experiment result correspond to an optic correl use slm for the centroid detect are also present","ordered_present_kp":[24,0,55,0,113,134,302,364,372,385,424,454,684,24],"keyphrases":["centroid detection","centroids","optical correlation","optical correlation","correlation-based methods","multiple objects","input scene","single intensity filter","position","orientation","scale","noise-smoothing performance","spatial light modulators","digital-moment-based methods","moment function modulus","moment function squaring","optical correlator"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","P"]}
{"id":"2170","title":"Evolution of litigation support systems","abstract":"For original paper see ibid., vol. 12, no. 6: \"The E-mail of the Species\". The author responds to that paper and argues that printing, scanning and imaging E-mails or other electronic (rather than paper) documents prior to listing and disclosure seems to be unnecessary, not 'proportionate' (from a costs point of view) and not particularly helpful, to either side. He asks how litigation support systems might evolve to help and support the legal team in their task","tok_text":"evolut of litig support system \n for origin paper see ibid . , vol . 12 , no . 6 : \" the e-mail of the speci \" . the author respond to that paper and argu that print , scan and imag e-mail or other electron ( rather than paper ) document prior to list and disclosur seem to be unnecessari , not ' proportion ' ( from a cost point of view ) and not particularli help , to either side . he ask how litig support system might evolv to help and support the legal team in their task","ordered_present_kp":[10,89,453],"keyphrases":["litigation support systems","E-mail","legal team"],"prmu":["P","P","P"]}
{"id":"2135","title":"A new approach to the problem of structural identification. II","abstract":"The subject under discussion is a new approach to the problem of structural identification, which relies on the recognition of a decisive role of the human factor in the process of structural identification. Potential possibilities of the suggested approach are illustrated by the statement of a new mathematical problem of structural identification","tok_text":"a new approach to the problem of structur identif . ii \n the subject under discuss is a new approach to the problem of structur identif , which reli on the recognit of a decis role of the human factor in the process of structur identif . potenti possibl of the suggest approach are illustr by the statement of a new mathemat problem of structur identif","ordered_present_kp":[33,188],"keyphrases":["structural identification","human factor","mathematical equations","decision-maker"],"prmu":["P","P","M","U"]}
{"id":"282","title":"Recommendations for implementing Internet inquiry projects","abstract":"The purpose of the study presented was to provide recommendations to teachers who are interested in implementing Internet inquiry projects. Four classes of ninth- and tenth-grade honors students (N = 100) participated in an Internet inquiry project in which they were presented with an ecology question that required them to make a decision based on information that they gathered, analyzed, and synthesized from the Internet and their textbook. Students then composed papers with a rationale for their decision. Students in one group had access to pre-selected relevant Web sites, access to the entire Internet, and were provided with less online support. Students in the other group had access to only pre-selected relevant Web sites, but were provided with more online support. Two of the most important recommendations were: 1) to provide students with more online support; and 2) to provide students with pre-selected relevant Web sites and allow them to search the Internet for information","tok_text":"recommend for implement internet inquiri project \n the purpos of the studi present wa to provid recommend to teacher who are interest in implement internet inquiri project . four class of ninth- and tenth-grad honor student ( n = 100 ) particip in an internet inquiri project in which they were present with an ecolog question that requir them to make a decis base on inform that they gather , analyz , and synthes from the internet and their textbook . student then compos paper with a rational for their decis . student in one group had access to pre-select relev web site , access to the entir internet , and were provid with less onlin support . student in the other group had access to onli pre-select relev web site , but were provid with more onlin support . two of the most import recommend were : 1 ) to provid student with more onlin support ; and 2 ) to provid student with pre-select relev web site and allow them to search the internet for inform","ordered_present_kp":[24,109,210,311,549,634],"keyphrases":["Internet inquiry projects","teachers","honors students","ecology question","pre-selected relevant Web sites","online support"],"prmu":["P","P","P","P","P","P"]}
{"id":"1944","title":"A framework of electronic tendering for government procurement: a lesson learned in Taiwan","abstract":"To render government procurement efficient, transparent, nondiscriminating, and accountable, an electronic government procurement system is required. Accordingly, Taiwan government procurement law (TGPL) states that suppliers may employ electronic devices to forward a tender. This investigation demonstrates how the electronic government procurement system functions and reengineers internal procurement processes, which in turn benefits both government bodies and vendors. The system features explored herein include posting\/receiving bids via the Internet, vendor registration, certificate authorization, contract development tools, bid\/request for proposal (RFP) development, online bidding, and online payment, all of which can be integrated easily within most existing information infrastructures","tok_text":"a framework of electron tender for govern procur : a lesson learn in taiwan \n to render govern procur effici , transpar , nondiscrimin , and account , an electron govern procur system is requir . accordingli , taiwan govern procur law ( tgpl ) state that supplier may employ electron devic to forward a tender . thi investig demonstr how the electron govern procur system function and reengin intern procur process , which in turn benefit both govern bodi and vendor . the system featur explor herein includ post \/ receiv bid via the internet , vendor registr , certif author , contract develop tool , bid \/ request for propos ( rfp ) develop , onlin bid , and onlin payment , all of which can be integr easili within most exist inform infrastructur","ordered_present_kp":[15,154,210,385,393,545,562,578,645,661,562],"keyphrases":["electronic tendering","electronic government procurement system","Taiwan government procurement law","reengineering","internal procurement processes","vendor registration","certificate authorization","certificate authorization","contract development tools","online bidding","online payment","Internet bids","request for proposal development","RFP development","certification authority","payment gateway","public key infrastructure"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","R","P","M","M"]}
{"id":"207","title":"Information architecture for bilingual Web sites","abstract":"Creating an information architecture for a bilingual Web site presents particular challenges beyond those that exist for single and multilanguage sites. This article reports work in progress on the development of a content-based bilingual Web site to facilitate the sharing of resources and information between Speech and Language Therapists. The development of the information architecture is based on a combination of two aspects: an abstract structural analysis of existing bilingual Web designs focusing on the presentation of bilingual material, and a bilingual card-sorting activity conducted with potential users. Issues for bilingual developments are discussed, and some observations are made regarding the use of card-sorting activities","tok_text":"inform architectur for bilingu web site \n creat an inform architectur for a bilingu web site present particular challeng beyond those that exist for singl and multilanguag site . thi articl report work in progress on the develop of a content-bas bilingu web site to facilit the share of resourc and inform between speech and languag therapist . the develop of the inform architectur is base on a combin of two aspect : an abstract structur analysi of exist bilingu web design focus on the present of bilingu materi , and a bilingu card-sort activ conduct with potenti user . issu for bilingu develop are discuss , and some observ are made regard the use of card-sort activ","ordered_present_kp":[0,234,325,523,584],"keyphrases":["information architecture","content-based bilingual Web site","language therapists","bilingual card-sorting activity","bilingual developments","speech therapists","World Wide Web"],"prmu":["P","P","P","P","P","R","M"]}
{"id":"242","title":"The California Digital Library and the eScholarship program","abstract":"The eScholarship program was launched in 2000 to foster faculty-led innovation in scholarly publishing. An initiative of the University of California (UC) and a program of the California Digital Library, the eScholarship program has stimulated significant interest in its short life. Its modest but visible accomplishments garner praise from many quarters, within and beyond the University of California. In perhaps the best indication of its timeliness and momentum, there are more proposals submitted to eScholarship today than the CDL can manage. This early success is due in part to the sheer power of an idea whose time has come, but also to the unique approach on which CDL was founded and the eScholarship initiative was first launched","tok_text":"the california digit librari and the escholarship program \n the escholarship program wa launch in 2000 to foster faculty-l innov in scholarli publish . an initi of the univers of california ( uc ) and a program of the california digit librari , the escholarship program ha stimul signific interest in it short life . it modest but visibl accomplish garner prais from mani quarter , within and beyond the univers of california . in perhap the best indic of it timeli and momentum , there are more propos submit to escholarship today than the cdl can manag . thi earli success is due in part to the sheer power of an idea whose time ha come , but also to the uniqu approach on which cdl wa found and the escholarship initi wa first launch","ordered_present_kp":[37,113,4,132,168],"keyphrases":["California Digital Library","eScholarship program","faculty-led innovation","scholarly publishing","University of California"],"prmu":["P","P","P","P","P"]}
{"id":"1979","title":"Combining spatial and scale-space techniques for edge detection to provide a spatially adaptive wavelet-based noise filtering algorithm","abstract":"New methods for detecting edges in an image using spatial and scale-space domains are proposed. A priori knowledge about geometrical characteristics of edges is used to assign a probability factor to the chance of any pixel being on an edge. An improved double thresholding technique is introduced for spatial domain filtering. Probabilities that pixels belong to a given edge are assigned based on pixel similarity across gradient amplitudes, gradient phases and edge connectivity. The scale-space approach uses dynamic range compression to allow wavelet correlation over a wider range of scales. A probabilistic formulation is used to combine the results obtained from filtering in each domain to provide a final edge probability image which has the advantages of both spatial and scale-space domain methods. Decomposing this edge probability image with the same wavelet as the original image permits the generation of adaptive filters that can recognize the characteristics of the edges in all wavelet detail and approximation images regardless of scale. These matched filters permit significant reduction in image noise without contributing to edge distortion. The spatially adaptive wavelet noise-filtering algorithm is qualitatively and quantitatively compared to a frequency domain and two wavelet based noise suppression algorithms using both natural and computer generated noisy images","tok_text":"combin spatial and scale-spac techniqu for edg detect to provid a spatial adapt wavelet-bas nois filter algorithm \n new method for detect edg in an imag use spatial and scale-spac domain are propos . a priori knowledg about geometr characterist of edg is use to assign a probabl factor to the chanc of ani pixel be on an edg . an improv doubl threshold techniqu is introduc for spatial domain filter . probabl that pixel belong to a given edg are assign base on pixel similar across gradient amplitud , gradient phase and edg connect . the scale-spac approach use dynam rang compress to allow wavelet correl over a wider rang of scale . a probabilist formul is use to combin the result obtain from filter in each domain to provid a final edg probabl imag which ha the advantag of both spatial and scale-spac domain method . decompos thi edg probabl imag with the same wavelet as the origin imag permit the gener of adapt filter that can recogn the characterist of the edg in all wavelet detail and approxim imag regardless of scale . these match filter permit signific reduct in imag nois without contribut to edg distort . the spatial adapt wavelet noise-filt algorithm is qualit and quantit compar to a frequenc domain and two wavelet base nois suppress algorithm use both natur and comput gener noisi imag","ordered_present_kp":[19,43,66,200,224,271,337,378,462,483,503,522,564,593,639,732,915,998,1040,1079,1128,1242],"keyphrases":["scale-space techniques","edge detection","spatially adaptive wavelet-based noise filtering algorithm","a priori knowledge","geometrical characteristics","probability factor","double thresholding technique","spatial domain filtering","pixel similarity","gradient amplitudes","gradient phases","edge connectivity","dynamic range compression","wavelet correlation","probabilistic formulation","final edge probability image","adaptive filters","approximation images","matched filters","image noise","spatially adaptive wavelet noise-filtering algorithm","noise suppression","spatial techniques"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"1984","title":"Deriving model parameters from field test measurements [generator control simulation]","abstract":"A major component of any power system simulation is the generating plant. The purpose of DeriveAssist is to speed up the parameter derivation process and to allow engineers less versed in parameter matching and identification to get involved in the process of power plant electric generator modelling","tok_text":"deriv model paramet from field test measur [ gener control simul ] \n a major compon of ani power system simul is the gener plant . the purpos of deriveassist is to speed up the paramet deriv process and to allow engin less vers in paramet match and identif to get involv in the process of power plant electr gener model","ordered_present_kp":[145,177,231,91,51],"keyphrases":["control simulation","power system simulation","DeriveAssist","parameter derivation process","parameter matching","parameter identification","turbine\/governor","power system stability analysis","computer simulation","generator parameter derivation process","steady-state parameters derivation"],"prmu":["P","P","P","P","P","R","U","M","M","R","M"]}
{"id":"2108","title":"Online longitudinal survey research: viability and participation","abstract":"This article explores the viability of conducting longitudinal survey research using the Internet in samples exposed to trauma. A questionnaire battery assessing psychological adjustment following adverse life experiences was posted online. Participants who signed up to take part in the longitudinal aspect of the study were contacted 3 and 6 months after initial participation to complete the second and third waves of the research. Issues of data screening and sample attrition rates are considered and the demographic profiles and questionnaire scores of those who did and did not take part in the study during successive time points are compared. The results demonstrate that it is possible to conduct repeated measures survey research online and that the similarity in characteristics between those who do and do not take part during successive time points mirrors that found in traditional pencil-and-paper trauma surveys","tok_text":"onlin longitudin survey research : viabil and particip \n thi articl explor the viabil of conduct longitudin survey research use the internet in sampl expos to trauma . a questionnair batteri assess psycholog adjust follow advers life experi wa post onlin . particip who sign up to take part in the longitudin aspect of the studi were contact 3 and 6 month after initi particip to complet the second and third wave of the research . issu of data screen and sampl attrit rate are consid and the demograph profil and questionnair score of those who did and did not take part in the studi dure success time point are compar . the result demonstr that it is possibl to conduct repeat measur survey research onlin and that the similar in characterist between those who do and do not take part dure success time point mirror that found in tradit pencil-and-pap trauma survey","ordered_present_kp":[0,132,159,170,198,440,456,493],"keyphrases":["online longitudinal survey research","Internet","trauma","questionnaire","psychological adjustment","data screening","sample attrition rates","demographic profiles","World Wide Web","psychology research"],"prmu":["P","P","P","P","P","P","P","P","U","R"]}
{"id":"2015","title":"Optical recognition of three-dimensional objects with scale invariance using a classical convergent correlator","abstract":"We present a real-time method for recognizing three-dimensional (3-D) objects with scale invariance. The 3-D information of the objects is codified in deformed fringe patterns using the Fourier transform profilometry technique and is correlated using a classical convergent correlator. The scale invariance property is achieved using two different approaches: the Mellin radial harmonic decomposition and the logarithmic radial harmonic filter. Thus, the method is invariant for changes in the scale of the 3-D target within a defined interval of scale factors. Experimental results show the utility of the proposed method","tok_text":"optic recognit of three-dimension object with scale invari use a classic converg correl \n we present a real-tim method for recogn three-dimension ( 3-d ) object with scale invari . the 3-d inform of the object is codifi in deform fring pattern use the fourier transform profilometri techniqu and is correl use a classic converg correl . the scale invari properti is achiev use two differ approach : the mellin radial harmon decomposit and the logarithm radial harmon filter . thu , the method is invari for chang in the scale of the 3-d target within a defin interv of scale factor . experiment result show the util of the propos method","ordered_present_kp":[0,46,65,103,185,223,252,341,403,443,52,569],"keyphrases":["optical recognition","scale invariance","invariant","classical convergent correlator","real-time method","3-D information","deformed fringe patterns","Fourier transform profilometry technique","scale invariance property","Mellin radial harmonic decomposition","logarithmic radial harmonic filter","scale factors","3D object recognition"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"2050","title":"Who Wants To Be A Millionaire(R): The classroom edition","abstract":"This paper introduces a version of the internationally popular television game show Who Wants To Be A Millionaire(R) that has been created for use in the classroom using Microsoft PowerPoint(R). A suggested framework for its classroom use is presented, instructions on operating and editing the classroom version of Who Wants To Be A Millionaire(R) are provided, and sample feedback from students who have played the classroom version of Who Wants To Be A Millionaire(R) is offered","tok_text":"who want to be a millionaire(r ): the classroom edit \n thi paper introduc a version of the intern popular televis game show who want to be a millionaire(r ) that ha been creat for use in the classroom use microsoft powerpoint(r ) . a suggest framework for it classroom use is present , instruct on oper and edit the classroom version of who want to be a millionaire(r ) are provid , and sampl feedback from student who have play the classroom version of who want to be a millionaire(r ) is offer","ordered_present_kp":[38,0,316],"keyphrases":["Who Wants To Be A Millionaire(R)","classroom","classroom version","undergraduate business students","student contestants"],"prmu":["P","P","P","M","M"]}
{"id":"35","title":"Fusion of qualitative bond graph and genetic algorithms: A fault diagnosis application","abstract":"In this paper, the problem of fault diagnosis via integration of genetic algorithms (GA's) and qualitative bond graphs (QBG's) is addressed. We suggest that GA's can be used to search for possible fault components among a system of qualitative equations. The QBG is adopted as the modeling scheme to generate a set of qualitative equations. The qualitative bond graph provides a unified approach for modeling engineering systems, in particular, mechatronic systems. In order to demonstrate the performance of the proposed algorithm, we have tested the proposed algorithm on an in-house designed and built floating disc experimental setup. Results from fault diagnosis in the floating disc system are presented and discussed. Additional measurements will be required to localize the fault when more than one fault candidate is inferred. Fault diagnosis is activated by a fault detection mechanism when a discrepancy between measured abnormal behavior and predicted system behavior is observed. The fault detection mechanism is not presented here","tok_text":"fusion of qualit bond graph and genet algorithm : a fault diagnosi applic \n in thi paper , the problem of fault diagnosi via integr of genet algorithm ( ga 's ) and qualit bond graph ( qbg 's ) is address . we suggest that ga 's can be use to search for possibl fault compon among a system of qualit equat . the qbg is adopt as the model scheme to gener a set of qualit equat . the qualit bond graph provid a unifi approach for model engin system , in particular , mechatron system . in order to demonstr the perform of the propos algorithm , we have test the propos algorithm on an in-hous design and built float disc experiment setup . result from fault diagnosi in the float disc system are present and discuss . addit measur will be requir to local the fault when more than one fault candid is infer . fault diagnosi is activ by a fault detect mechan when a discrep between measur abnorm behavior and predict system behavior is observ . the fault detect mechan is not present here","ordered_present_kp":[10,32,52,262,293,434,465,608,878,905],"keyphrases":["qualitative bond graph","genetic algorithms","fault diagnosis","fault components","qualitative equations","engineering systems","mechatronic systems","floating disc","measured abnormal behavior","predicted system behavior"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"2169","title":"E-government","abstract":"The author provides an introduction to the main issues surrounding E-government modernisation and electronic delivery of all public services by 2005. The author makes it clear that E-government is about transformation, not computers and hints at the special legal issues which may arise","tok_text":"e-govern \n the author provid an introduct to the main issu surround e-govern modernis and electron deliveri of all public servic by 2005 . the author make it clear that e-govern is about transform , not comput and hint at the special legal issu which may aris","ordered_present_kp":[0,77,90,115,234],"keyphrases":["E-government","modernisation","electronic delivery","public services","legal issues"],"prmu":["P","P","P","P","P"]}
{"id":"2194","title":"Data management in location-dependent information services","abstract":"Location-dependent information services have great promise for mobile and pervasive computing environments. They can provide local and nonlocal news, weather, and traffic reports as well as directory services. Before they can be implemented on a large scale, however, several research issues must be addressed","tok_text":"data manag in location-depend inform servic \n location-depend inform servic have great promis for mobil and pervas comput environ . they can provid local and nonloc news , weather , and traffic report as well as directori servic . befor they can be implement on a larg scale , howev , sever research issu must be address","ordered_present_kp":[14,108,165,172,186,0,212],"keyphrases":["data management","location-dependent information services","pervasive computing","news","weather","traffic reports","directory services","wireless networks","mobile computing"],"prmu":["P","P","P","P","P","P","P","U","R"]}
{"id":"223","title":"Broadcasts keep staff in picture [intranets]","abstract":"Mark Hawkins, chief operating officer at UK-based streaming media specialist Twofourtv, explains how firms can benefit by linking their corporate intranets to broadcasting technology","tok_text":"broadcast keep staff in pictur [ intranet ] \n mark hawkin , chief oper offic at uk-bas stream media specialist twofourtv , explain how firm can benefit by link their corpor intranet to broadcast technolog","ordered_present_kp":[166,111,87,185],"keyphrases":["streaming media","Twofourtv","corporate intranets","broadcasting technology"],"prmu":["P","P","P","P"]}
{"id":"266","title":"Pattern recognition strategies for molecular surfaces. I. Pattern generation using fuzzy set theory","abstract":"A new method for the characterization of molecules based on the model approach of molecular surfaces is presented. We use the topographical properties of the surface as well as the electrostatic potential, the local lipophilicity\/hydrophilicity, and the hydrogen bond density on the surface for characterization. The definition and the calculation method for these properties are reviewed. The surface is segmented into overlapping patches with similar molecular properties. These patches can be used to represent the characteristic local features of the molecule in a way that is beyond the atomistic resolution but can nevertheless be applied for the analysis of partial similarities of different molecules as well as for the identification of molecular complementarity in a very general sense. The patch representation can be used for different applications, which will be demonstrated in subsequent articles","tok_text":"pattern recognit strategi for molecular surfac . i. pattern gener use fuzzi set theori \n a new method for the character of molecul base on the model approach of molecular surfac is present . we use the topograph properti of the surfac as well as the electrostat potenti , the local lipophil \/ hydrophil , and the hydrogen bond densiti on the surfac for character . the definit and the calcul method for these properti are review . the surfac is segment into overlap patch with similar molecular properti . these patch can be use to repres the characterist local featur of the molecul in a way that is beyond the atomist resolut but can nevertheless be appli for the analysi of partial similar of differ molecul as well as for the identif of molecular complementar in a veri gener sens . the patch represent can be use for differ applic , which will be demonstr in subsequ articl","ordered_present_kp":[0,30,52,70,143,202,250,276,313,458,485,556,612,677,741,791,282,293],"keyphrases":["pattern recognition strategies","molecular surfaces","pattern generation","fuzzy set theory","model approach","topographical properties","electrostatic potential","local lipophilicity\/hydrophilicity","lipophilicity","hydrophilicity","hydrogen bond density","overlapping patches","molecular properties","local features","atomistic resolution","partial similarities","molecular complementarity","patch representation","segmented surface"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"413","title":"Effects of white space in learning via the Web","abstract":"This study measured the effect of specific white space features on learning from instructional Web materials. The study also measured learners' beliefs regarding Web-based instruction. Prior research indicated that small changes in the handling of presentation elements can affect learning. Achievement results from this study indicated that in on-line materials, when content and overall structure are sound, minor differences regarding table borders and vertical spacing in text do not hinder learning. Beliefs regarding Web-based instruction and instructors who use it did not differ significantly between treatment groups. Implications of the study and cautions regarding generalizing from the results are discussed","tok_text":"effect of white space in learn via the web \n thi studi measur the effect of specif white space featur on learn from instruct web materi . the studi also measur learner ' belief regard web-bas instruct . prior research indic that small chang in the handl of present element can affect learn . achiev result from thi studi indic that in on-lin materi , when content and overal structur are sound , minor differ regard tabl border and vertic space in text do not hinder learn . belief regard web-bas instruct and instructor who use it did not differ significantli between treatment group . implic of the studi and caution regard gener from the result are discuss","ordered_present_kp":[83,184,257,416],"keyphrases":["white space features","Web-based instruction","presentation","table borders","online educational materials","text vertical spacing","Internet"],"prmu":["P","P","P","P","M","R","U"]}
{"id":"2089","title":"World's biggest battery helps to stabilise Alaska","abstract":"In this paper, the author describes a battery energy storage system which is under construction to provide voltage compensation in support of Alaska's 138 kV Northern Intertie","tok_text":"world 's biggest batteri help to stabilis alaska \n in thi paper , the author describ a batteri energi storag system which is under construct to provid voltag compens in support of alaska 's 138 kv northern interti","ordered_present_kp":[87,151,190],"keyphrases":["battery energy storage system","voltage compensation","138 kV","power system stabilisation","USA","interconnected power systems","77 MW"],"prmu":["P","P","P","M","U","M","U"]}
{"id":"2031","title":"Efficient two-level image thresholding method based on Bayesian formulation and the maximum entropy principle","abstract":"An efficient method for two-level thresholding is proposed based on the Bayes formula and the maximum entropy principle, in which no assumptions of the image histogram are made. An alternative criterion is derived based on maximizing entropy and used for speeding up the searching algorithm. Five forms of conditional probability distributions-simple, linear, parabola concave, parabola convex, and S-function-are employed and compared to each other for optimal threshold determination. The effect of precision on optimal threshold determination is discussed and a trade-off precision epsilon =0.001 is selected experimentally. Our experiments demonstrate that the proposed method achieves a significant improvement in speed from 26 to 57 times faster than the exhaustive search method","tok_text":"effici two-level imag threshold method base on bayesian formul and the maximum entropi principl \n an effici method for two-level threshold is propos base on the bay formula and the maximum entropi principl , in which no assumpt of the imag histogram are made . an altern criterion is deriv base on maxim entropi and use for speed up the search algorithm . five form of condit probabl distributions-simpl , linear , parabola concav , parabola convex , and s-function-ar employ and compar to each other for optim threshold determin . the effect of precis on optim threshold determin is discuss and a trade-off precis epsilon = 0.001 is select experiment . our experi demonstr that the propos method achiev a signific improv in speed from 26 to 57 time faster than the exhaust search method","ordered_present_kp":[7,47,71,235,79,337,415,433,505,598,17],"keyphrases":["two-level image thresholding method","image thresholding","Bayesian formulation","maximum entropy principle","entropy","image histogram","searching algorithm","parabola concave","parabola convex","optimal threshold determination","trade-off precision","conditional probability distributions","S-function","image segmentation"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","M","U","M"]}
{"id":"386","title":"Matched-filter template generation via spatial filtering: application to fetal biomagnetic recordings","abstract":"We have developed a two-step procedure for signal processing of fetal biomagnetic recordings that removes cardiac interference and noise. First, a modified matched filter (MF) is applied to remove maternal cardiac interference; then, a simple signal space projection (SSP) is applied to remove noise. The key difference between our MF and a conventional one is that the interference template and the template scaling are derived from a signal that has been spatially filtered to isolate the interference, rather than from the raw signal. Unlike conventional MFs, ours is able to separate maternal and fetal cardiac complexes, even when they have similar morphology and overlap strongly. When followed by a SSP that preserves only the signal subspace, the noise is reduced to a low level","tok_text":"matched-filt templat gener via spatial filter : applic to fetal biomagnet record \n we have develop a two-step procedur for signal process of fetal biomagnet record that remov cardiac interfer and nois . first , a modifi match filter ( mf ) is appli to remov matern cardiac interfer ; then , a simpl signal space project ( ssp ) is appli to remov nois . the key differ between our mf and a convent one is that the interfer templat and the templat scale are deriv from a signal that ha been spatial filter to isol the interfer , rather than from the raw signal . unlik convent mf , our is abl to separ matern and fetal cardiac complex , even when they have similar morpholog and overlap strongli . when follow by a ssp that preserv onli the signal subspac , the nois is reduc to a low level","ordered_present_kp":[293,31,413,548,438,213,258],"keyphrases":["spatial filtering","modified matched filter","maternal cardiac interference","simple signal space projection","interference template","template scaling","raw signal","maternal cardiac interference removal","noise removal","signal subspace preservation","fetal magnetocardiography"],"prmu":["P","P","P","P","P","P","P","R","R","R","M"]}
{"id":"2049","title":"The maximum possible EVPI","abstract":"In this paper we calculate the maximum expected value of perfect information (EVPI) for any probability distribution for the states of the world. This maximum EVPI is an upper bound for the EVPI with given probabilities and thus an upper bound for any partial information about the states of the world","tok_text":"the maximum possibl evpi \n in thi paper we calcul the maximum expect valu of perfect inform ( evpi ) for ani probabl distribut for the state of the world . thi maximum evpi is an upper bound for the evpi with given probabl and thu an upper bound for ani partial inform about the state of the world","ordered_present_kp":[62,109],"keyphrases":["expected value of perfect information","probability distribution","decision analysis","operations research","management science","optimisation"],"prmu":["P","P","U","U","U","U"]}
{"id":"346","title":"Baseball, optimization, and the World Wide Web","abstract":"The competition for baseball play-off spots-the fabled pennant race-is one of the most closely watched American sports traditions. While play-off race statistics, such as games back and magic number, are informative, they are overly conservative and do not account for the remaining schedule of games. Using optimization techniques, one can model schedule effects explicitly and determine precisely when a team has secured a play-off spot or has been eliminated from contention. The RIOT Baseball Play-off Races Web site developed at the University of California, Berkeley, provides automatic updates of new, optimization-based play-off race statistics each day of the major league baseball season. In developing the site, we found that we could determine the first-place elimination status of all teams in a division using a single linear-programming formulation, since a minimum win threshold for teams finishing in first place applies to all teams in a division. We identified a similar (but weaker) result for the problem of play-off elimination with wildcard teams","tok_text":"basebal , optim , and the world wide web \n the competit for basebal play-off spots-th fabl pennant race-i one of the most close watch american sport tradit . while play-off race statist , such as game back and magic number , are inform , they are overli conserv and do not account for the remain schedul of game . use optim techniqu , one can model schedul effect explicitli and determin precis when a team ha secur a play-off spot or ha been elimin from content . the riot basebal play-off race web site develop at the univers of california , berkeley , provid automat updat of new , optimization-bas play-off race statist each day of the major leagu basebal season . in develop the site , we found that we could determin the first-plac elimin statu of all team in a divis use a singl linear-program formul , sinc a minimum win threshold for team finish in first place appli to all team in a divis . we identifi a similar ( but weaker ) result for the problem of play-off elimin with wildcard team","ordered_present_kp":[10,26,164,196,210,469,817],"keyphrases":["optimization","World Wide Web","play-off race statistics","games back","magic number","RIOT Baseball Play-off Races Web site","minimum win threshold","baseball play-off spot competition","pennant race","game schedule","linear programming","LP"],"prmu":["P","P","P","P","P","P","P","R","R","R","U","U"]}
{"id":"303","title":"Visual-word identification thresholds for the 260 fragmented words of the Snodgrass and Vanderwart pictures in Spanish","abstract":"Word difficulty varies from language to language; therefore, normative data of verbal stimuli cannot be imported directly from another language. We present mean identification thresholds for the 260 screen-fragmented words corresponding to the total set of Snodgrass and Vanderwart (1980) pictures. Individual words were fragmented in eight levels using Turbo Pascal, and the resulting program was implemented on a PC microcomputer. The words were presented individually to a group of 40 Spanish observers, using a controlled time procedure. An unspecific learning effect was found showing that performance improved due to practice with the task. Finally, of the 11 psycholinguistic variables that previous researchers have shown to affect word identification, only imagery accounted for a significant amount of variance in the threshold values","tok_text":"visual-word identif threshold for the 260 fragment word of the snodgrass and vanderwart pictur in spanish \n word difficulti vari from languag to languag ; therefor , norm data of verbal stimuli can not be import directli from anoth languag . we present mean identif threshold for the 260 screen-frag word correspond to the total set of snodgrass and vanderwart ( 1980 ) pictur . individu word were fragment in eight level use turbo pascal , and the result program wa implement on a pc microcomput . the word were present individu to a group of 40 spanish observ , use a control time procedur . an unspecif learn effect wa found show that perform improv due to practic with the task . final , of the 11 psycholinguist variabl that previou research have shown to affect word identif , onli imageri account for a signific amount of varianc in the threshold valu","ordered_present_kp":[0,42,63,98,108,179,253,288,426,482,570,597,702,7],"keyphrases":["visual-word identification thresholds","word identification","fragmented words","Snodgrass and Vanderwart pictures","Spanish","word difficulty","verbal stimuli","mean identification thresholds","screen-fragmented words","Turbo Pascal","PC microcomputer","controlled time procedure","unspecific learning effect","psycholinguistic variables"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"1960","title":"Streaming, disruptive interference and power-law behavior in the exit dynamics of confined pedestrians","abstract":"We analyze the exit dynamics of pedestrians who are initially confined in a room. Pedestrians are modeled as cellular automata and compete to escape via a known exit at the soonest possible time. A pedestrian could move forward, backward, left or right within each iteration time depending on adjacent cell vacancy and in accordance with simple rules that determine the compulsion to move and physical capability relative to his neighbors. The arching signatures of jamming were observed and the pedestrians exited in bursts of various sizes. Power-law behavior is found in the burst-size frequency distribution for exit widths w greater than one cell dimension (w > 1). The slope of the power-law curve varies with w from -1.3092 (w = 2) to -1.0720 (w = 20). Streaming which is a diffusive behavior, arises in large burst sizes and is more likely in a single-exit room with w = 1 and leads to a counterintuitive result wherein an average exit throughput Q is obtained that is higher than with w = 2, 3, or 4. For a two-exit room (w = 1), Q is not greater than twice the yield of a single-exit room. If the doors are not separated far enough (< 4w), Q becomes even significantly less due to a collective slow-down that emerges among pedestrians crossing in each other's path (disruptive interference effect). For the same w and door number, Q is also higher with relaxed pedestrians than with anxious ones","tok_text":"stream , disrupt interfer and power-law behavior in the exit dynam of confin pedestrian \n we analyz the exit dynam of pedestrian who are initi confin in a room . pedestrian are model as cellular automata and compet to escap via a known exit at the soonest possibl time . a pedestrian could move forward , backward , left or right within each iter time depend on adjac cell vacanc and in accord with simpl rule that determin the compuls to move and physic capabl rel to hi neighbor . the arch signatur of jam were observ and the pedestrian exit in burst of variou size . power-law behavior is found in the burst-siz frequenc distribut for exit width w greater than one cell dimens ( w > 1 ) . the slope of the power-law curv vari with w from -1.3092 ( w = 2 ) to -1.0720 ( w = 20 ) . stream which is a diffus behavior , aris in larg burst size and is more like in a single-exit room with w = 1 and lead to a counterintuit result wherein an averag exit throughput q is obtain that is higher than with w = 2 , 3 , or 4 . for a two-exit room ( w = 1 ) , q is not greater than twice the yield of a single-exit room . if the door are not separ far enough ( < 4w ) , q becom even significantli less due to a collect slow-down that emerg among pedestrian cross in each other 's path ( disrupt interfer effect ) . for the same w and door number , q is also higher with relax pedestrian than with anxiou one","ordered_present_kp":[0,186,342,362,487,504,605,1201,9,30,56,70],"keyphrases":["streaming","disruptive interference","power-law behavior","exit dynamics","confined pedestrians","cellular automata","iteration time","adjacent cell vacancy","arching signatures","jamming","burst-size frequency distribution","collective slow-down","self-organised criticality"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","U"]}
{"id":"2154","title":"Optimize\/sup IT\/ robot condition monitoring tool","abstract":"As robots have gained more and more 'humanlike' capability, users have looked increasingly to their builders for ways to measure the critical variables-the robotic equivalent of a physical check-up-in order to monitor their condition and schedule maintenance more effectively. This is all the more essential considering the tremendous pressure there is to improve productivity in today's global markets. Developed for ABB robots with an S4-family controller and based on the company's broad process know-how, Optimize\/sup IT\/ robot condition monitoring offers maintenance routines with embedded checklists that give a clear indication of a robot's operating condition. It performs semi-automatic measurements that support engineers during trouble-shooting and enable action to be taken to prevent unplanned stops. By comparing these measurements with reference data, negative trends can be detected early and potential breakdowns predicted. Armed with all these features, Optimize\/sup IT\/ robot condition monitoring provides the ideal basis for reliability-centered maintenance (RCM) for robots","tok_text":"optim \/ sup it\/ robot condit monitor tool \n as robot have gain more and more ' humanlik ' capabl , user have look increasingli to their builder for way to measur the critic variables-th robot equival of a physic check-up-in order to monitor their condit and schedul mainten more effect . thi is all the more essenti consid the tremend pressur there is to improv product in today 's global market . develop for abb robot with an s4-famili control and base on the compani 's broad process know-how , optim \/ sup it\/ robot condit monitor offer mainten routin with embed checklist that give a clear indic of a robot 's oper condit . it perform semi-automat measur that support engin dure trouble-shoot and enabl action to be taken to prevent unplan stop . by compar these measur with refer data , neg trend can be detect earli and potenti breakdown predict . arm with all these featur , optim \/ sup it\/ robot condit monitor provid the ideal basi for reliability-cent mainten ( rcm ) for robot","ordered_present_kp":[0,22,410,428,640,946],"keyphrases":["Optimize\/sup IT\/ robot condition monitoring tool","condition monitoring","ABB robots","S4-family controller","semi-automatic measurements","reliability-centered maintenance","maintenance scheduling"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"2111","title":"Extended depth-of-focus imaging of chlorophyll fluorescence from intact leaves","abstract":"Imaging dynamic changes in chlorophyll a fluorescence provides a valuable means with which to examine localised changes in photosynthetic function. Microscope-based systems provide excellent spatial resolution which allows the response of individual cells to be measured. However, such systems have a restricted depth of focus and, as leaves are inherently uneven, only a small proportion of each image at any given focal plane is in focus. In this report we describe the development of algorithms, specifically adapted for imaging chlorophyll fluorescence and photosynthetic function in living plant cells, which allow extended-focus images to be reconstructed from images taken in different focal planes. We describe how these procedures can be used to reconstruct images of chlorophyll fluorescence and calculated photosynthetic parameters, as well as producing a map of leaf topology. The robustness of this procedure is demonstrated using leaves from a number of different plant species","tok_text":"extend depth-of-focu imag of chlorophyl fluoresc from intact leav \n imag dynam chang in chlorophyl a fluoresc provid a valuabl mean with which to examin localis chang in photosynthet function . microscope-bas system provid excel spatial resolut which allow the respons of individu cell to be measur . howev , such system have a restrict depth of focu and , as leav are inher uneven , onli a small proport of each imag at ani given focal plane is in focu . in thi report we describ the develop of algorithm , specif adapt for imag chlorophyl fluoresc and photosynthet function in live plant cell , which allow extended-focu imag to be reconstruct from imag taken in differ focal plane . we describ how these procedur can be use to reconstruct imag of chlorophyl fluoresc and calcul photosynthet paramet , as well as produc a map of leaf topolog . the robust of thi procedur is demonstr use leav from a number of differ plant speci","ordered_present_kp":[29,54,0,918,774,194,229],"keyphrases":["extended depth-of-focus imaging","chlorophyll fluorescence","intact leaves","microscope-based systems","spatial resolution","calculated photosynthetic parameters","plant species","leaf topology map","individual cells response","charge-coupled device","maximum fluorescence yield","minimum fluorescence yield","variable fluorescence","numerical aperture","primary quinone acceptor","algorithms development","extended-focus images reconstruction","biophysical research technique"],"prmu":["P","P","P","P","P","P","P","R","R","U","M","M","M","U","U","R","R","U"]}
{"id":"344","title":"Student consulting projects benefit faculty and industry","abstract":"Student consulting projects require students to apply OR\/MS tools to obtain insight into the activities of firms in the community. These projects benefit faculty by providing clear feedback on the real capabilities of students, a broad connection to local industry, and material for case studies and research. They benefit companies by stimulating new thinking regarding their activities and delivering results they can use. Projects provide insights into the end-user modeling mode of OR\/MS practice. Projects support continuous improvement as the lessons gained from a crop of projects enable better teaching during the next course offering, which in turn leads to better projects and further insights into teaching","tok_text":"student consult project benefit faculti and industri \n student consult project requir student to appli or \/ ms tool to obtain insight into the activ of firm in the commun . these project benefit faculti by provid clear feedback on the real capabl of student , a broad connect to local industri , and materi for case studi and research . they benefit compani by stimul new think regard their activ and deliv result they can use . project provid insight into the end-us model mode of or \/ ms practic . project support continu improv as the lesson gain from a crop of project enabl better teach dure the next cours offer , which in turn lead to better project and further insight into teach","ordered_present_kp":[0,103],"keyphrases":["student consulting projects","OR\/MS tools","student placements","student capability feedback","case study material"],"prmu":["P","P","M","R","R"]}
{"id":"301","title":"Academic libraries and community: making the connection","abstract":"I explore the theme of academic libraries serving and reaching out to the broader community. I highlight interesting projects reported on in the literature (such as the Through Our Parents' Eyes project) and report on others. I look at challenges to community partnerships and recommendations for making them succeed. Although I focus on links with the broader community, I also took at methods for increasing cooperation among various units on campus, so that the needs of campus community groups-such as distance education students or disabled students-are effectively addressed. Though academic libraries are my focus, we can learn a lot from the community building efforts of public libraries","tok_text":"academ librari and commun : make the connect \n i explor the theme of academ librari serv and reach out to the broader commun . i highlight interest project report on in the literatur ( such as the through our parent ' eye project ) and report on other . i look at challeng to commun partnership and recommend for make them succeed . although i focu on link with the broader commun , i also took at method for increas cooper among variou unit on campu , so that the need of campu commun groups-such as distanc educ student or disabl students-ar effect address . though academ librari are my focu , we can learn a lot from the commun build effort of public librari","ordered_present_kp":[0,276,501,648],"keyphrases":["academic libraries","community partnerships","distance education students","public libraries","campus community groups","disabled students"],"prmu":["P","P","P","P","M","R"]}
{"id":"2156","title":"Pane relief. Robotic solutions for car windshield assembly","abstract":"Just looking through a car's windshield doesn't give us much reason to wonder about how it's made. The idea that special manufacturing expertise might be required can hardly occur to anyone, but that's exactly what is needed to ensure crystal-clear visibility, not to mention a perfect fit every time one is pressed into place on a car production line. Comprising two thin glass sheets joined by a vinyl interlayer, windshields are assembled-usually manually-to very precise product and environmental specifications. To make sure this is done as perfectly as possible, the industry invests heavily in the equipment used for their fabrication. ABB has now developed a robot-based Compact Assembling System for the automatic assembly of laminated windshields that speeds up production and increases cost efficiency","tok_text":"pane relief . robot solut for car windshield assembl \n just look through a car 's windshield doe n't give us much reason to wonder about how it 's made . the idea that special manufactur expertis might be requir can hardli occur to anyon , but that 's exactli what is need to ensur crystal-clear visibl , not to mention a perfect fit everi time one is press into place on a car product line . compris two thin glass sheet join by a vinyl interlay , windshield are assembled-usu manually-to veri precis product and environment specif . to make sure thi is done as perfectli as possibl , the industri invest heavili in the equip use for their fabric . abb ha now develop a robot-bas compact assembl system for the automat assembl of lamin windshield that speed up product and increas cost effici","ordered_present_kp":[176,374,681,378,782,650],"keyphrases":["manufacturing expertise","car production line","production","ABB","Compact Assembling System","cost efficiency","car windshield assembly robots","laminated windshields assembly automation"],"prmu":["P","P","P","P","P","P","R","M"]}
{"id":"2113","title":"Ideal sliding mode in the problems of convex optimization","abstract":"The characteristics of the sliding mode that appears with using continuous convex-programming algorithms based on the exact penalty functions were discussed. For the case under study, the ideal sliding mode was shown to occur in the absence of infinite number of switchings","tok_text":"ideal slide mode in the problem of convex optim \n the characterist of the slide mode that appear with use continu convex-program algorithm base on the exact penalti function were discuss . for the case under studi , the ideal slide mode wa shown to occur in the absenc of infinit number of switch","ordered_present_kp":[0,35,106,151],"keyphrases":["ideal sliding mode","convex optimization","continuous convex-programming algorithms","exact penalty functions"],"prmu":["P","P","P","P"]}
{"id":"259","title":"Nuts and bolts: implementing descriptive standards to enable virtual collections","abstract":"To date, online archival information systems have relied heavily on legacy finding aids for data to encode and provide to end users, despite fairly strong indications in the archival literature that such legacy data is problematic even as a mediated access tool. Archivists have only just begun to study the utility of archival descriptive data for end users in unmediated settings such as via the Web. The ability of future archival information systems to respond to the expectations and needs of end users is inextricably linked to archivists getting their collective data house in order. The General International Standard Archival Description (ISAD(G)) offers the profession a place from which to start extricating ourselves from the idiosyncracies of our legacy data and description practices","tok_text":"nut and bolt : implement descript standard to enabl virtual collect \n to date , onlin archiv inform system have reli heavili on legaci find aid for data to encod and provid to end user , despit fairli strong indic in the archiv literatur that such legaci data is problemat even as a mediat access tool . archivist have onli just begun to studi the util of archiv descript data for end user in unmedi set such as via the web . the abil of futur archiv inform system to respond to the expect and need of end user is inextric link to archivist get their collect data hous in order . the gener intern standard archiv descript ( isad(g ) ) offer the profess a place from which to start extric ourselv from the idiosyncraci of our legaci data and descript practic","ordered_present_kp":[25,52,80,176,221,248,283,304,356,86,551,584],"keyphrases":["descriptive standards","virtual collections","online archival information systems","archival information systems","end users","archival literature","legacy data","mediated access tool","archivists","archival descriptive data","collective data house","General International Standard Archival Description","ISAD","Online Archive of California","OAC"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","U","M","U"]}
{"id":"1962","title":"The Bagsik Oscillator without complex numbers","abstract":"We argue that the analysis of the so-called Bagsik Oscillator, recently published by Piotrowski and Sladkowski (2001), is erroneous due to: (1) the incorrect banking data used and (2) the application of statistical mechanism apparatus to processes that are totally deterministic","tok_text":"the bagsik oscil without complex number \n we argu that the analysi of the so-cal bagsik oscil , recent publish by piotrowski and sladkowski ( 2001 ) , is erron due to : ( 1 ) the incorrect bank data use and ( 2 ) the applic of statist mechan apparatu to process that are total determinist","ordered_present_kp":[4,179,227],"keyphrases":["Bagsik oscillator","incorrect banking data","statistical mechanism apparatus","noncomplex numbers","game theory","deterministic processes"],"prmu":["P","P","P","M","U","R"]}
{"id":"2196","title":"ConChat: a context-aware chat program","abstract":"ConChat is a context-aware chat program that enriches electronic communication by providing contextual information and resolving potential semantic conflicts between users.ConChat uses contextual information to improve electronic communication. Using contextual cues, users can infer during a conversation what the other person is doing and what is happening in his or her immediate surroundings. For example, if a user learns that the other person is talking with somebody else or is involved in some urgent activity, he or she knows to expect a slower response. Conversely, if the user learns that the other person is sitting in a meeting directly related to the conversation, he or she then knows to respond more quickly. Also, by informing users about the other person's context and tagging potentially ambiguous chat messages, ConChat explores how context can improve electronic communication by reducing semantic conflicts","tok_text":"conchat : a context-awar chat program \n conchat is a context-awar chat program that enrich electron commun by provid contextu inform and resolv potenti semant conflict between user . conchat use contextu inform to improv electron commun . use contextu cue , user can infer dure a convers what the other person is do and what is happen in hi or her immedi surround . for exampl , if a user learn that the other person is talk with somebodi els or is involv in some urgent activ , he or she know to expect a slower respons . convers , if the user learn that the other person is sit in a meet directli relat to the convers , he or she then know to respond more quickli . also , by inform user about the other person 's context and tag potenti ambigu chat messag , conchat explor how context can improv electron commun by reduc semant conflict","ordered_present_kp":[12,0,117,152,243],"keyphrases":["ConChat","context-aware chat program","contextual information","semantic conflicts","contextual cues"],"prmu":["P","P","P","P","P"]}
{"id":"264","title":"The archival imagination of David Bearman, revisited","abstract":"Many archivists regard the archival imagination evidenced in the writings of David Bearman as avant-garde. Archivist L. Henry (1998) has sharply criticized Bearman for being irreverent toward the archival theory and practice outlined by classical American archivist T. R. Schellenberg. Although Bearman is sometimes credited (and sometimes berated) for establishing \"a new paradigm\" centered on the archival management of electronic records, his methods and strategies are intended to encompass all forms of record keeping. The article provides general observations on Bearman's archival imagination, lists some of its components, and addresses elements of Henry's critique. Although the long lasting impact of Bearman's imagination upon the archival profession might be questioned, it nonetheless deserves continued consideration by archivists and inclusion as a component of graduate archival education","tok_text":"the archiv imagin of david bearman , revisit \n mani archivist regard the archiv imagin evidenc in the write of david bearman as avant-gard . archivist l. henri ( 1998 ) ha sharpli critic bearman for be irrever toward the archiv theori and practic outlin by classic american archivist t. r. schellenberg . although bearman is sometim credit ( and sometim berat ) for establish \" a new paradigm \" center on the archiv manag of electron record , hi method and strategi are intend to encompass all form of record keep . the articl provid gener observ on bearman 's archiv imagin , list some of it compon , and address element of henri 's critiqu . although the long last impact of bearman 's imagin upon the archiv profess might be question , it nonetheless deserv continu consider by archivist and inclus as a compon of graduat archiv educ","ordered_present_kp":[4,21,221,257,290,409,425,502,704,817],"keyphrases":["archival imagination","David Bearman","archival theory","classical American archivist","Schellenberg","archival management","electronic records","record keeping","archival profession","graduate archival education"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"198","title":"Computational capacity of an odorant discriminator: the linear separability of curves","abstract":"We introduce and study an artificial neural network inspired by the probabilistic receptor affinity distribution model of olfaction. Our system consists of N sensory neurons whose outputs converge on a single processing linear threshold element. The system's aim is to model discrimination of a single target odorant from a large number p of background odorants within a range of odorant concentrations. We show that this is possible provided p does not exceed a critical value p\/sub c\/ and calculate the critical capacity alpha c=p\/sub c\/\/N. The critical capacity depends on the range of concentrations in which the discrimination is to be accomplished. If the olfactory bulb may be thought of as a collection of such processing elements, each responsible for the discrimination of a single odorant, our study provides a quantitative analysis of the potential computational properties of the olfactory bulb. The mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves, embedded in a large-dimensional space. This is accomplished here by a numerical study, using a method that signals whether the discrimination task is realizable, together with a finite-size scaling analysis","tok_text":"comput capac of an odor discrimin : the linear separ of curv \n we introduc and studi an artifici neural network inspir by the probabilist receptor affin distribut model of olfact . our system consist of n sensori neuron whose output converg on a singl process linear threshold element . the system 's aim is to model discrimin of a singl target odor from a larg number p of background odor within a rang of odor concentr . we show that thi is possibl provid p doe not exceed a critic valu p \/ sub c\/ and calcul the critic capac alpha c = p \/ sub c\/\/n. the critic capac depend on the rang of concentr in which the discrimin is to be accomplish . if the olfactori bulb may be thought of as a collect of such process element , each respons for the discrimin of a singl odor , our studi provid a quantit analysi of the potenti comput properti of the olfactori bulb . the mathemat formul of the problem we consid is one of determin the capac for linear separ of continu curv , embed in a large-dimension space . thi is accomplish here by a numer studi , use a method that signal whether the discrimin task is realiz , togeth with a finite-s scale analysi","ordered_present_kp":[88,138,172,260,205,40,19],"keyphrases":["odorant discriminator","linear separability","artificial neural network","receptor affinity distribution","olfaction","sensory neurons","linear threshold element"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"37","title":"Design PID controllers for desired time-domain or frequency-domain response","abstract":"Practical requirements on the design of control systems, especially process control systems, are usually specified in terms of time-domain response, such as overshoot and rise time, or frequency-domain response, such as resonance peak and stability margin. Although numerous methods have been developed for the design of the proportional-integral-derivative (PID) controller, little work has been done in relation to the quantitative time-domain and frequency-domain responses. In this paper, we study the following problem: Given a nominal stable process with time delay, we design a suboptimal PID controller to achieve the required time-domain response or frequency-domain response for the nominal system or the uncertain system. An H\/sub infinity \/ PID controller is developed based on optimal control theory and the parameters are derived analytically. Its properties are investigated and compared with that of two developed suboptimal controllers: an H\/sub 2\/ PID controller and a Maclaurin PID controller","tok_text":"design pid control for desir time-domain or frequency-domain respons \n practic requir on the design of control system , especi process control system , are usual specifi in term of time-domain respons , such as overshoot and rise time , or frequency-domain respons , such as reson peak and stabil margin . although numer method have been develop for the design of the proportional-integral-deriv ( pid ) control , littl work ha been done in relat to the quantit time-domain and frequency-domain respons . in thi paper , we studi the follow problem : given a nomin stabl process with time delay , we design a suboptim pid control to achiev the requir time-domain respons or frequency-domain respons for the nomin system or the uncertain system . an h \/ sub infin \/ pid control is develop base on optim control theori and the paramet are deriv analyt . it properti are investig and compar with that of two develop suboptim control : an h \/ sub 2\/ pid control and a maclaurin pid control","ordered_present_kp":[181,44,127,211,225,275,290,558,912,748,934,795,963],"keyphrases":["frequency-domain response","process control systems","time-domain response","overshoot","rise time","resonance peak","stability margin","nominal stable process","H\/sub infinity \/ PID controller","optimal control","suboptimal controller","H\/sub 2\/ PID controller","Maclaurin PID controller","proportional-integral derivative controller"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"299","title":"Customer in-reach and library strategic systems: the case of ILLiad","abstract":"Libraries have walls. Recognizing this fact, the Interlibrary Loan Department at Virginia Tech is creating systems and services that enable our customers to reach past our walls at anytime from anywhere. Customer in-reach enables Virginia Tech faculty, students, and staff anywhere in the world to obtain information and services heretofore available only to our on-campus customers. ILLiad, Virginia Tech's interlibrary borrowing system, is the library strategic system that attains this goal. The principles that guided development of ILLiad are widely applicable","tok_text":"custom in-reach and librari strateg system : the case of illiad \n librari have wall . recogn thi fact , the interlibrari loan depart at virginia tech is creat system and servic that enabl our custom to reach past our wall at anytim from anywher . custom in-reach enabl virginia tech faculti , student , and staff anywher in the world to obtain inform and servic heretofor avail onli to our on-campu custom . illiad , virginia tech 's interlibrari borrow system , is the librari strateg system that attain thi goal . the principl that guid develop of illiad are wide applic","ordered_present_kp":[20,108,136,0,57,434],"keyphrases":["customer in-reach","library strategic systems","ILLiad","Interlibrary Loan Department","Virginia Tech","interlibrary borrowing system"],"prmu":["P","P","P","P","P","P"]}
{"id":"2033","title":"Optical encoding of color three-dimensional correlation","abstract":"Three-dimensional (3D) correlation of color images, considering the color distribution as the third dimension, has been shown to be useful for color pattern recognition tasks. Nevertheless, 3D correlation cannot be directly performed on an optical correlator, that can only process two-dimensional (2D) signals. We propose a method to encode 3D functions onto 2D ones in such a way that the Fourier transform and correlation of these signals, that can be optically performed, encode the 3D Fourier transform and correlation of the 3D signals. The theory for the encoding is given and experimental results obtained in an optical correlator are shown","tok_text":"optic encod of color three-dimension correl \n three-dimension ( 3d ) correl of color imag , consid the color distribut as the third dimens , ha been shown to be use for color pattern recognit task . nevertheless , 3d correl can not be directli perform on an optic correl , that can onli process two-dimension ( 2d ) signal . we propos a method to encod 3d function onto 2d one in such a way that the fourier transform and correl of these signal , that can be optic perform , encod the 3d fourier transform and correl of the 3d signal . the theori for the encod is given and experiment result obtain in an optic correl are shown","ordered_present_kp":[0,15,214,79,103,169,258,400,485],"keyphrases":["optical encoding","color three-dimensional correlation","color images","color distribution","color pattern recognition tasks","3D correlation","optical correlator","Fourier transform","3D Fourier transform","3D function encoding"],"prmu":["P","P","P","P","P","P","P","P","P","R"]}
{"id":"384","title":"Brightness-independent start-up routine for star trackers","abstract":"Initial attitude acquisition by a modern star tracker is investigated here. Criteria for efficient organization of the on-board database are discussed with reference to a brightness-independent initial acquisition algorithm. Star catalog generation preprocessing is described, with emphasis on the identification of minimum star brightness for detection by a sensor based on a charge coupled device (CCD) photodetector. This is a crucial step for proper evaluation of the attainable sky coverage when selecting the stars to be included in the on-board catalog. Test results are also reported, both for reliability and accuracy, even if the former is considered to be the primary target. Probability of erroneous solution is 0.2% in the case of single runs of the procedure, while attitude determination accuracy is in the order of 0.02 degrees in the average for the computation of the inertial pointing of the boresight axis","tok_text":"brightness-independ start-up routin for star tracker \n initi attitud acquisit by a modern star tracker is investig here . criteria for effici organ of the on-board databas are discuss with refer to a brightness-independ initi acquisit algorithm . star catalog gener preprocess is describ , with emphasi on the identif of minimum star bright for detect by a sensor base on a charg coupl devic ( ccd ) photodetector . thi is a crucial step for proper evalu of the attain sky coverag when select the star to be includ in the on-board catalog . test result are also report , both for reliabl and accuraci , even if the former is consid to be the primari target . probabl of erron solut is 0.2 % in the case of singl run of the procedur , while attitud determin accuraci is in the order of 0.02 degre in the averag for the comput of the inerti point of the boresight axi","ordered_present_kp":[0,40,55,155,247,321,580,852],"keyphrases":["brightness-independent start-up routine","star trackers","initial attitude acquisition","on-board database","star catalog generation preprocessing","minimum star brightness","reliability","boresight axis","gyroless spacecraft","charge coupled device photodetector"],"prmu":["P","P","P","P","P","P","P","P","U","R"]}
{"id":"379","title":"Feedforward maximum power point tracking of PV systems using fuzzy controller","abstract":"A feedforward maximum power (MP) point tracking scheme is developed for the interleaved dual boost (IDB) converter fed photovoltaic (PV) system using fuzzy controller. The tracking algorithm changes the duty ratio of the converter such that the solar cell array (SCA) voltage equals the voltage corresponding to the MP point at that solar insolation. This is done by the feedforward loop, which generates an error signal by comparing the instantaneous array voltage and reference voltage. The reference voltage for the feedforward loop, corresponding to the MP point, is obtained by an off-line trained neural network. Experimental data is used for off-line training of the neural network, which employs back-propagation algorithm. The proposed fuzzy feedforward peak power tracking effectiveness is demonstrated through the simulation and experimental results, and compared with the conventional proportional plus integral (PI) controller based system. Finally, a comparative study of interleaved boost and conventional boost converter for the PV applications is given and their suitability is discussed","tok_text":"feedforward maximum power point track of pv system use fuzzi control \n a feedforward maximum power ( mp ) point track scheme is develop for the interleav dual boost ( idb ) convert fed photovolta ( pv ) system use fuzzi control . the track algorithm chang the duti ratio of the convert such that the solar cell array ( sca ) voltag equal the voltag correspond to the mp point at that solar insol . thi is done by the feedforward loop , which gener an error signal by compar the instantan array voltag and refer voltag . the refer voltag for the feedforward loop , correspond to the mp point , is obtain by an off-lin train neural network . experiment data is use for off-lin train of the neural network , which employ back-propag algorithm . the propos fuzzi feedforward peak power track effect is demonstr through the simul and experiment result , and compar with the convent proport plu integr ( pi ) control base system . final , a compar studi of interleav boost and convent boost convert for the pv applic is given and their suitabl is discuss","ordered_present_kp":[0,41,55,234,260,384,417,451,478,505,609,718,753],"keyphrases":["feedforward maximum power point tracking","PV systems","fuzzy controller","tracking algorithm","duty ratio","solar insolation","feedforward loop","error signal","instantaneous array voltage","reference voltage","off-line trained neural network","back-propagation algorithm","fuzzy feedforward peak power tracking effectiveness","interleaved dual boost converter feed","photovoltaic system","solar cell array voltage"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","M","R","R"]}
{"id":"411","title":"CAD\/CAE software aids converter design [DC\/DC power conversion]","abstract":"Typically, power supply design involves electronic and magnetic components. In this paper, the authors describe, using a flyback converter example, how CAD\/CAE tools can aid the power supply engineer in both areas, reducing prototyping costs and providing insights into system performance","tok_text":"cad \/ cae softwar aid convert design [ dc \/ dc power convers ] \n typic , power suppli design involv electron and magnet compon . in thi paper , the author describ , use a flyback convert exampl , how cad \/ cae tool can aid the power suppli engin in both area , reduc prototyp cost and provid insight into system perform","ordered_present_kp":[73,113,0,267],"keyphrases":["CAD\/CAE software","power supply design","magnetic components","prototyping costs","DC\/DC power convertor design","electronic components","flyback power convertor topology"],"prmu":["P","P","P","P","M","R","M"]}
{"id":"1986","title":"Control centers are here to stay","abstract":"Despite changes with different structures, market rules, and uncertainties, a control center must always be in place to maintain the security, reliability, and quality of electric service. This article focuses on the energy management system (EMS) control center, identifying the major functions that have become standard components of every application software package. The two most important control center functions, security control and load-following control, guarantee the continuity of electric service, which after all, is the end-product of the utility business. New technology trends in the design of control center infrastructures are emerging in the liberalized environment of the energy market. An example of a control center infrastructure is described. The article ends with a concern for the security of the control center itself","tok_text":"control center are here to stay \n despit chang with differ structur , market rule , and uncertainti , a control center must alway be in place to maintain the secur , reliabl , and qualiti of electr servic . thi articl focus on the energi manag system ( em ) control center , identifi the major function that have becom standard compon of everi applic softwar packag . the two most import control center function , secur control and load-follow control , guarante the continu of electr servic , which after all , is the end-product of the util busi . new technolog trend in the design of control center infrastructur are emerg in the liber environ of the energi market . an exampl of a control center infrastructur is describ . the articl end with a concern for the secur of the control center itself","ordered_present_kp":[231,319,344,414,432,587,633,654],"keyphrases":["energy management system","standard components","application software package","security control","load-following control","control center infrastructures","liberalized environment","energy market","EMS control centers","electric service continuity"],"prmu":["P","P","P","P","P","P","P","P","R","R"]}
{"id":"205","title":"Geotensity: combining motion and lighting for 3D surface reconstruction","abstract":"This paper is about automatically reconstructing the full 3D surface of an object observed in motion by a single static camera. Based on the two paradigms, structure from motion and linear intensity subspaces, we introduce the geotensity constraint that governs the relationship between four or more images of a moving object. We show that it is possible in theory to solve for 3D Lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources. The surface may or may not be textured. We then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera","tok_text":"geotens : combin motion and light for 3d surfac reconstruct \n thi paper is about automat reconstruct the full 3d surfac of an object observ in motion by a singl static camera . base on the two paradigm , structur from motion and linear intens subspac , we introduc the geotens constraint that govern the relationship between four or more imag of a move object . we show that it is possibl in theori to solv for 3d lambertian surfac structur for the case of a singl point light sourc and propos that a solut exist for an arbitrari number point light sourc . the surfac may or may not be textur . we then give an exampl of automat surfac reconstruct of a face under a point light sourc use arbitrari unknown object motion and a singl fix camera","ordered_present_kp":[105,155,229,269,411,459,520,621,465],"keyphrases":["full 3D surface","single static camera","linear intensity subspaces","geotensity constraint","3D Lambertian surface structure","single point light source","point light source","arbitrary number point light sources","automatic surface reconstruction","linear image subspaces","structure-from-motion"],"prmu":["P","P","P","P","P","P","P","P","P","R","U"]}
{"id":"240","title":"Project Euclid and the role of research libraries in scholarly publishing","abstract":"Project Euclid, a joint electronic journal publishing initiative of Cornell University Library and Duke University Press is discussed in the broader contexts of the changing patterns of scholarly communication and the publishing scene of mathematics. Specific aspects of the project such as partnerships and the creation of an economic model are presented as well as what it takes to be a publisher. Libraries have gained important and relevant experience through the creation and management of digital libraries, but they need to develop further skills if they want to adopt a new role in the life cycle of scholarly communication","tok_text":"project euclid and the role of research librari in scholarli publish \n project euclid , a joint electron journal publish initi of cornel univers librari and duke univers press is discuss in the broader context of the chang pattern of scholarli commun and the publish scene of mathemat . specif aspect of the project such as partnership and the creation of an econom model are present as well as what it take to be a publish . librari have gain import and relev experi through the creation and manag of digit librari , but they need to develop further skill if they want to adopt a new role in the life cycl of scholarli commun","ordered_present_kp":[0,90,130,157,234,276,324,359,51,31],"keyphrases":["Project Euclid","research libraries","scholarly publishing","joint electronic journal publishing initiative","Cornell University Library","Duke University Press","scholarly communication","mathematics","partnerships","economic model"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"318","title":"Note on \"Deterministic inventory lot-size models under inflation with shortages and deterioration for fluctuating demand\" by Yang et al","abstract":"For original paper see H.-L. Yang et al., ibid., vol.48, p.144-58 (2001). Yang et al. extended the lot-size models to allow for inflation and fluctuating demand. For this model they proved that the optimal replenishment schedule exists and is unique. They also proposed an algorithm to find the optimal policy. The present paper provides examples, which show that the optimal replenishment schedule and consequently the overall optimal policy may not exist","tok_text":"note on \" determinist inventori lot-siz model under inflat with shortag and deterior for fluctuat demand \" by yang et al \n for origin paper see h.-l. yang et al . , ibid . , vol.48 , p.144 - 58 ( 2001 ) . yang et al . extend the lot-siz model to allow for inflat and fluctuat demand . for thi model they prove that the optim replenish schedul exist and is uniqu . they also propos an algorithm to find the optim polici . the present paper provid exampl , which show that the optim replenish schedul and consequ the overal optim polici may not exist","ordered_present_kp":[10,52,89,319],"keyphrases":["deterministic inventory lot-size models","inflation","fluctuating demand","optimal replenishment schedule","optimal policy algorithm","optimal scheduling parameters"],"prmu":["P","P","P","P","R","M"]}
{"id":"2017","title":"Autofocus system for microscope","abstract":"A technique is developed for microscope autofocusing, which is called the eccentric light beam approach with high resolution, wide focusing range, and compact construction. The principle is described. The theoretical formula of the eccentric light beam approach deduced can be applied not only to an object lens whose objective plane is just at the focal plane, but also to an object lens whose objective plane is not at the focal plane. The experimental setup uses a semiconductor laser device as the light source. The laser beam that enters into the microscope is eccentric with the main light axis. A defocused signal is acquired by a symmetrical silicon photocell for the change of the reflected light position caused by differential amplification and processed by a microprocessor. Then the electric signal is power-amplified and drives a dc motor, which moves a fine working platform to an automatic focus of the microscope. The result of the experiments shows a +or-0.1- mu m precision of autofocusing for a range of +or-500- mu m defocusing. The system has high reliability and can meet the requirements of various accurate micro measurement systems","tok_text":"autofocu system for microscop \n a techniqu is develop for microscop autofocus , which is call the eccentr light beam approach with high resolut , wide focus rang , and compact construct . the principl is describ . the theoret formula of the eccentr light beam approach deduc can be appli not onli to an object len whose object plane is just at the focal plane , but also to an object len whose object plane is not at the focal plane . the experiment setup use a semiconductor laser devic as the light sourc . the laser beam that enter into the microscop is eccentr with the main light axi . a defocus signal is acquir by a symmetr silicon photocel for the chang of the reflect light posit caus by differenti amplif and process by a microprocessor . then the electr signal is power-amplifi and drive a dc motor , which move a fine work platform to an automat focu of the microscop . the result of the experi show a + or-0.1- mu m precis of autofocus for a rang of + or-500- mu m defocus . the system ha high reliabl and can meet the requir of variou accur micro measur system","ordered_present_kp":[0,58,98,303,320,462,574,593,623,669,697,732,801,825,1002,1055],"keyphrases":["autofocus system","microscope autofocusing","eccentric light beam approach","object lens","objective plane","semiconductor laser","main light axis","defocused signal","symmetrical silicon photocell","reflected light position","differential amplification","microprocessor","dc motor","fine working platform","high reliability","micro measurement systems","power-amplified electric signal"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2052","title":"Blitzograms - interactive histograms","abstract":"As computers become ever faster, more and more procedures that were once viewed as iterative will continue to become instantaneous. The blitzogram is the application of this trend to histograms, which the author hopes will lead to a better tacit understanding of probability distributions among both students and managers. And this is not just an academic exercise. Commercial Monte Carlo simulation packages like @RISK and Crystal Ball, and my INSIGHT.xla are widely available","tok_text":"blitzogram - interact histogram \n as comput becom ever faster , more and more procedur that were onc view as iter will continu to becom instantan . the blitzogram is the applic of thi trend to histogram , which the author hope will lead to a better tacit understand of probabl distribut among both student and manag . and thi is not just an academ exercis . commerci mont carlo simul packag like @risk and crystal ball , and my insight.xla are wide avail","ordered_present_kp":[0,22,269],"keyphrases":["blitzogram","histograms","probability distributions","MBA","operations research","management science","statistics"],"prmu":["P","P","P","U","U","M","U"]}
{"id":"408","title":".NET obfuscation and intellectual property","abstract":"The author considers obfuscation options for protecting .NET code. Many programs won't need obfuscation because the loss caused by reverse engineering will be nonexistent. Numerous obfuscators are already available for the .NET platform, ranging from a basic renaming obfuscator to a fully functional obfuscator that handles mixed IL\/native code assemblies created in any managed language, including Microsoft's C++ with Managed Extensions. An obfuscator simply makes your application harder to reverse engineer. It does not prevent reverse engineering. However, the cost of obfuscation is insignificant when compared to the cost of a typical software development project. If you feel like an obfuscator provides you any benefit at all, it's probably worth the price","tok_text":".net obfusc and intellectu properti \n the author consid obfusc option for protect .net code . mani program wo n't need obfusc becaus the loss caus by revers engin will be nonexist . numer obfusc are alreadi avail for the .net platform , rang from a basic renam obfusc to a fulli function obfusc that handl mix il \/ nativ code assembl creat in ani manag languag , includ microsoft 's c++ with manag extens . an obfusc simpli make your applic harder to revers engin . it doe not prevent revers engin . howev , the cost of obfusc is insignific when compar to the cost of a typic softwar develop project . if you feel like an obfusc provid you ani benefit at all , it 's probabl worth the price","ordered_present_kp":[0,16,150],"keyphrases":[".NET obfuscation","intellectual property","reverse engineering"],"prmu":["P","P","P"]}
{"id":"360","title":"Numerical representation of binary relations with a multiplicative error function","abstract":"This paper studies the case of the representation of a binary relation via a numerical function with threshold (error) depending on both compared alternatives. The error is considered to be multiplicative, its value being either directly or inversely proportional to the values of the numerical function. For the first case, it is proved that a binary relation is a semiorder. Moreover, any semiorder can be represented in this form. In the second case, the corresponding binary relation is an interval order","tok_text":"numer represent of binari relat with a multipl error function \n thi paper studi the case of the represent of a binari relat via a numer function with threshold ( error ) depend on both compar altern . the error is consid to be multipl , it valu be either directli or invers proport to the valu of the numer function . for the first case , it is prove that a binari relat is a semiord . moreov , ani semiord can be repres in thi form . in the second case , the correspond binari relat is an interv order","ordered_present_kp":[0,19,39,130,150,47,376,490],"keyphrases":["numerical representation","binary relations","multiplicative error function","error","numerical function","threshold","semiorder","interval order"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"2092","title":"Matching PET and CT scans of the head and neck area: Development of method and validation","abstract":"Positron emission tomography (PET) provides important information on tumor biology, but lacks detailed anatomical information. Our aim in the present study was to develop and validate an automatic registration method for matching PET and CT scans of the head and neck. Three difficulties in achieving this goal are (1) nonrigid motions of the neck can hamper the use of automatic ridged body transformations; (2) emission scans contain too little anatomical information to apply standard image fusion methods; and (3) no objective way exists to quantify the quality of the match results. These problems are solved as follows: accurate and reproducible positioning of the patient was achieved by using a radiotherapy treatment mask. The proposed method makes use of the transmission rather than the emission scan. To obtain sufficient (anatomical) information for matching, two bed positions for the transmission scan were included in the protocol. A mutual information-based algorithm was used as a registration technique. PET and CT data were obtained in seven patients. Each patient had two CT scans and one PET scan. The datasets were used to estimate the consistency by matching PET to CT\/sub 1\/, CT\/sub 1\/ to CT\/sub 2\/, and CT\/sub 2\/ to PET using the full circle consistency test. It was found that using our method, consistency could be obtained of 4 mm and 1.3 degrees on average. The PET voxels used for registration were 5.15 mm, so the errors compared quite favorably with the voxel size. Cropping the images (removing the scanner bed from images) did not improve the consistency of the algorithm. The transmission scan, however, could potentially be reduced to a single position using this approach. In conclusion, the represented algorithm and validation technique has several features that are attractive from both theoretical and practical point of view, it is a user-independent, automatic validation technique for matching CT and PET scans of the head and neck, which gives the opportunity to compare different image enhancements","tok_text":"match pet and ct scan of the head and neck area : develop of method and valid \n positron emiss tomographi ( pet ) provid import inform on tumor biolog , but lack detail anatom inform . our aim in the present studi wa to develop and valid an automat registr method for match pet and ct scan of the head and neck . three difficulti in achiev thi goal are ( 1 ) nonrigid motion of the neck can hamper the use of automat ridg bodi transform ; ( 2 ) emiss scan contain too littl anatom inform to appli standard imag fusion method ; and ( 3 ) no object way exist to quantifi the qualiti of the match result . these problem are solv as follow : accur and reproduc posit of the patient wa achiev by use a radiotherapi treatment mask . the propos method make use of the transmiss rather than the emiss scan . to obtain suffici ( anatom ) inform for match , two bed posit for the transmiss scan were includ in the protocol . a mutual information-bas algorithm wa use as a registr techniqu . pet and ct data were obtain in seven patient . each patient had two ct scan and one pet scan . the dataset were use to estim the consist by match pet to ct \/ sub 1\/ , ct \/ sub 1\/ to ct \/ sub 2\/ , and ct \/ sub 2\/ to pet use the full circl consist test . it wa found that use our method , consist could be obtain of 4 mm and 1.3 degre on averag . the pet voxel use for registr were 5.15 mm , so the error compar quit favor with the voxel size . crop the imag ( remov the scanner bed from imag ) did not improv the consist of the algorithm . the transmiss scan , howev , could potenti be reduc to a singl posit use thi approach . in conclus , the repres algorithm and valid techniqu ha sever featur that are attract from both theoret and practic point of view , it is a user-independ , automat valid techniqu for match ct and pet scan of the head and neck , which give the opportun to compar differ imag enhanc","ordered_present_kp":[138,169,241,29,38,359,409,497,697,852,870,917,962,670,1208,1378,1450,1877],"keyphrases":["head","neck","tumor biology","anatomical information","automatic registration method","nonrigid motions","automatic ridged body transformations","standard image fusion methods","patients","radiotherapy treatment mask","bed positions","transmission scan","mutual information-based algorithm","registration technique","full circle consistency test","errors","scanner bed","image enhancements","positron emission tomography scans","computerised tomography scans","user-independent automatic validation technique"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","M","R"]}
{"id":"325","title":"Open courseware and shared knowledge in higher education","abstract":"Most college and university campuses in the United States and much of the developed world today maintain one, two, or several learning management systems (LMSs), which are courseware products that provide students and faculty with Web-based tools to manage course-related applications. Since the mid-1990s, two predominant models of Web courseware management systems have emerged: commercial and noncommercial. Some of the commercial products available today were created in academia as noncommercial but have since become commercially encumbered. Other products remain noncommercial but are struggling to survive in a world of fierce commercial competition. This article argues for an ethics of pedagogy in higher education that would be based on the guiding assumptions of the non-proprietary, peer-to-peer, open-source software movement","tok_text":"open coursewar and share knowledg in higher educ \n most colleg and univers campus in the unit state and much of the develop world today maintain one , two , or sever learn manag system ( lmss ) , which are coursewar product that provid student and faculti with web-bas tool to manag course-rel applic . sinc the mid-1990 , two predomin model of web coursewar manag system have emerg : commerci and noncommerci . some of the commerci product avail today were creat in academia as noncommerci but have sinc becom commerci encumb . other product remain noncommerci but are struggl to surviv in a world of fierc commerci competit . thi articl argu for an ethic of pedagogi in higher educ that would be base on the guid assumpt of the non-proprietari , peer-to-p , open-sourc softwar movement","ordered_present_kp":[0,19,37,166,56,67,345,424,651,760],"keyphrases":["open courseware","shared knowledge","higher education","college","university","learning management systems","Web courseware management systems","commercial products","ethics","open-source software","Internet"],"prmu":["P","P","P","P","P","P","P","P","P","P","U"]}
{"id":"1946","title":"Integrating building management system and facilities management on the Internet","abstract":"Recently, it is of great interest to adopt the Internet\/intranet to develop building management systems (BMS) and facilities management systems (FMS). This paper addresses two technical issues: the Web-based access (including database integration) and the integration of BMS and FMS. These should be addressed for accessing BMS remotely via the Internet, integrating control networks using the Internet protocols and infrastructures, and using Internet\/intranet for building facilities management. An experimental Internet-enabled system that integrates building and facilities management systems has been developed and tested. This system integrated open control networks with the Internet and is developed utilizing the embedded Web server, the PC Web server and the Distributed Component Object Model (DCOM) software development technology on the platform of an open control network. Three strategies for interconnecting BMS local networks via Internet\/intranet are presented and analyzed","tok_text":"integr build manag system and facil manag on the internet \n recent , it is of great interest to adopt the internet \/ intranet to develop build manag system ( bm ) and facil manag system ( fm ) . thi paper address two technic issu : the web-bas access ( includ databas integr ) and the integr of bm and fm . these should be address for access bm remot via the internet , integr control network use the internet protocol and infrastructur , and use internet \/ intranet for build facil manag . an experiment internet-en system that integr build and facil manag system ha been develop and test . thi system integr open control network with the internet and is develop util the embed web server , the pc web server and the distribut compon object model ( dcom ) softwar develop technolog on the platform of an open control network . three strategi for interconnect bm local network via internet \/ intranet are present and analyz","ordered_present_kp":[117,7,158,167,188,236,260,401,673,696,718,750,757,610],"keyphrases":["building management systems","intranet","BMS","facilities management systems","FMS","Web-based access","database integration","Internet protocols","open control network","embedded Web server","PC Web server","Distributed Component Object Model","DCOM","software development technology","local network interconnection"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"238","title":"The Open Archives Initiative: realizing simple and effective digital library interoperability","abstract":"The Open Archives Initiative (OAI) is dedicated to solving problems of digital library interoperability. Its focus has been on defining simple protocols, most recently for the exchange of metadata from archives. The OAI evolved out of a need to increase access to scholarly publications by supporting the creation of interoperable digital libraries. As a first step towards such interoperability, a metadata harvesting protocol was developed to support the streaming of metadata from one repository to another, ultimately to a provider of user services such as browsing, searching, or annotation. This article provides an overview of the mission, philosophy, and technical framework of the OAI","tok_text":"the open archiv initi : realiz simpl and effect digit librari interoper \n the open archiv initi ( oai ) is dedic to solv problem of digit librari interoper . it focu ha been on defin simpl protocol , most recent for the exchang of metadata from archiv . the oai evolv out of a need to increas access to scholarli public by support the creation of interoper digit librari . as a first step toward such interoper , a metadata harvest protocol wa develop to support the stream of metadata from one repositori to anoth , ultim to a provid of user servic such as brows , search , or annot . thi articl provid an overview of the mission , philosophi , and technic framework of the oai","ordered_present_kp":[4,48,189,303,415,578,566,558,538],"keyphrases":["Open Archives Initiative","digital library interoperability","protocols","scholarly publications","metadata harvesting protocol","user services","browsing","searching","annotation","exchange metadata","streaming metadata"],"prmu":["P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2172","title":"Electronic data exchange for real estate","abstract":"With HM Land Registry's consultation now underway, no one denies that the property industry is facing a period of unprecedented change. PISCES (Property Information Systems Common Exchange) is a property-focused electronic data exchange standard. The standard is a set of definitions and rules to facilitate electronic transfer of data between key business areas and between different types of software packages that are used regularly by the property industry. It is not itself a piece of software but an enabling technology that allows software providers to prepare solutions within their own packages to transfer data between databases. This provides the attractive prospect of seamless transfer of data within and between systems and organisations","tok_text":"electron data exchang for real estat \n with hm land registri 's consult now underway , no one deni that the properti industri is face a period of unpreced chang . pisc ( properti inform system common exchang ) is a property-focus electron data exchang standard . the standard is a set of definit and rule to facilit electron transfer of data between key busi area and between differ type of softwar packag that are use regularli by the properti industri . it is not itself a piec of softwar but an enabl technolog that allow softwar provid to prepar solut within their own packag to transfer data between databas . thi provid the attract prospect of seamless transfer of data within and between system and organis","ordered_present_kp":[44,108,163,170,0,252,391,605,650],"keyphrases":["electronic data exchange","HM Land Registry","property industry","PISCES","Property Information Systems Common Exchange","standard","software packages","databases","seamless transfer"],"prmu":["P","P","P","P","P","P","P","P","P"]}
{"id":"2137","title":"Stabilization of a linear object by frequency-modulated pulsed signals","abstract":"A control system consisting of an unstable continuous linear part and a pulse-frequency modulator in the feedback circuit is studied. Conditions for the boundedness of the solutions of the system under any initial data are determined","tok_text":"stabil of a linear object by frequency-modul puls signal \n a control system consist of an unstabl continu linear part and a pulse-frequ modul in the feedback circuit is studi . condit for the bounded of the solut of the system under ani initi data are determin","ordered_present_kp":[0,29,61,149],"keyphrases":["stabilization","frequency-modulated pulsed signals","control system","feedback circuit","discrete systems","linear stationary object","solution boundedness"],"prmu":["P","P","P","P","M","M","R"]}
{"id":"280","title":"Entrepreneurs in Action: a Web-case model","abstract":"Much of the traditional schooling in America is built around systems of compliance and control, characteristics which stifle the creative and entrepreneurial instincts of the children who are subjected to these tactics. The article explores a different approach to education, one that involves capturing the interest of the student through the use of problem and project-based instruction delivered via the Internet. Called Entrepreneurs in Action, this program seeks to involve students in a problem at the outset and to promote the learning of traditional subject areas as a process of the problem-solving activities that are undertaken. The program's details are explained, from elementary school through university level courses, and the authors outline their plans to test the efficacy of the program at each level","tok_text":"entrepreneur in action : a web-cas model \n much of the tradit school in america is built around system of complianc and control , characterist which stifl the creativ and entrepreneuri instinct of the children who are subject to these tactic . the articl explor a differ approach to educ , one that involv captur the interest of the student through the use of problem and project-bas instruct deliv via the internet . call entrepreneur in action , thi program seek to involv student in a problem at the outset and to promot the learn of tradit subject area as a process of the problem-solv activ that are undertaken . the program 's detail are explain , from elementari school through univers level cours , and the author outlin their plan to test the efficaci of the program at each level","ordered_present_kp":[0,27,55,72,171,372,407,537,577,659,685],"keyphrases":["Entrepreneurs in Action","Web-case model","traditional schooling","America","entrepreneurial instincts","project-based instruction","Internet","traditional subject areas","problem-solving activities","elementary school","university level courses"],"prmu":["P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2013","title":"Novel denoising algorithm for obtaining a superresolved position estimation","abstract":"We present a new algorithm that uses the randomness of the noise pattern to achieve high positioning accuracy by applying a modified averaging operation. Using the suggested approach, noise sensitivity of the positioning accuracy can be significantly reduced. This new improved algorithm can improve the performances of tracking systems used for military as well as civil applications. The concept is demonstrated theoretically as well as by optical experiment","tok_text":"novel denois algorithm for obtain a superresolv posit estim \n we present a new algorithm that use the random of the nois pattern to achiev high posit accuraci by appli a modifi averag oper . use the suggest approach , nois sensit of the posit accuraci can be significantli reduc . thi new improv algorithm can improv the perform of track system use for militari as well as civil applic . the concept is demonstr theoret as well as by optic experi","ordered_present_kp":[6,36,139,170,218,332,373,434],"keyphrases":["denoising algorithm","superresolved position estimation","high positioning accuracy","modified averaging operation","noise sensitivity","tracking systems","civil applications","optical experiment","noise pattern randomness","military applications"],"prmu":["P","P","P","P","P","P","P","P","R","R"]}
{"id":"2056","title":"Gifts to a science academic librarian","abstract":"Gifts, by their altruistic nature, perfectly fit into the environment of universities and academic libraries. As a university's community and general public continue to donate materials, libraries accept donations willingly, both in-kind and monetary. Eight steps of gift processing are listed in the paper. Positive and negative aspects of gift acceptance are discussed. Gifts bring value for academic libraries. Gifts can be considered additional routes to contribute to library collections without direct purchases, options to add money to the library budget, and the cement of social relationships. But, unfortunately, large donations are time-consuming, labor-intensive and costly to process. Great amounts of staff time and processing space are two main negative aspects that cause concern and put the value of gift acceptance under consideration by librarians. Some strategies in handling gifts are recommended. To be effective, academic science librarians need to approach gifts as an investment. Librarians are not to be forced by moral and public notions and should be able to make professional decisions in evaluating proposed collections","tok_text":"gift to a scienc academ librarian \n gift , by their altruist natur , perfectli fit into the environ of univers and academ librari . as a univers 's commun and gener public continu to donat materi , librari accept donat willingli , both in-kind and monetari . eight step of gift process are list in the paper . posit and neg aspect of gift accept are discuss . gift bring valu for academ librari . gift can be consid addit rout to contribut to librari collect without direct purchas , option to add money to the librari budget , and the cement of social relationship . but , unfortun , larg donat are time-consum , labor-intens and costli to process . great amount of staff time and process space are two main neg aspect that caus concern and put the valu of gift accept under consider by librarian . some strategi in handl gift are recommend . to be effect , academ scienc librarian need to approach gift as an invest . librarian are not to be forc by moral and public notion and should be abl to make profession decis in evalu propos collect","ordered_present_kp":[10,17,183,273,443,519,667,1002],"keyphrases":["science academic librarian","academic libraries","donations","gift processing","library collections","budget","staff time","professional decisions","research libraries","acquisitions","gift books"],"prmu":["P","P","P","P","P","P","P","P","M","U","M"]}
{"id":"359","title":"Neighborhood operator systems and approximations","abstract":"This paper presents a framework for the study of generalizing the standard notion of equivalence relation in rough set approximation space with various categories of k-step neighborhood systems. Based on a binary relation on a finite universe, six families of binary relations are obtained, and the corresponding six classes of k-step neighborhood systems are derived. Extensions of Pawlak's (1982) rough set approximation operators based on such neighborhood systems are proposed. Properties of neighborhood operator systems and rough set approximation operators are investigated, and their connections are examined","tok_text":"neighborhood oper system and approxim \n thi paper present a framework for the studi of gener the standard notion of equival relat in rough set approxim space with variou categori of k-step neighborhood system . base on a binari relat on a finit univers , six famili of binari relat are obtain , and the correspond six class of k-step neighborhood system are deriv . extens of pawlak 's ( 1982 ) rough set approxim oper base on such neighborhood system are propos . properti of neighborhood oper system and rough set approxim oper are investig , and their connect are examin","ordered_present_kp":[0,116,133,182,221,239],"keyphrases":["neighborhood operator systems","equivalence relation","rough set approximation space","k-step neighborhood systems","binary relation","finite universe"],"prmu":["P","P","P","P","P","P"]}
{"id":"201","title":"Correction to construction of panoramic image mosaics with global and local alignment","abstract":"For original paper see ibid., vol. 36, no. 2, p. 101-30 (2000). The authors had given a method for the construction of panoramic image mosaics with global and local alignment. Unfortunately a mistake had led to an incorrect equation which whilst making little difference in many cases, for faster (and assured) convergence, the correct formulae given here should be used","tok_text":"correct to construct of panoram imag mosaic with global and local align \n for origin paper see ibid . , vol . 36 , no . 2 , p. 101 - 30 ( 2000 ) . the author had given a method for the construct of panoram imag mosaic with global and local align . unfortun a mistak had led to an incorrect equat which whilst make littl differ in mani case , for faster ( and assur ) converg , the correct formula given here should be use","ordered_present_kp":[24,60],"keyphrases":["panoramic image mosaics","local alignment","global alignment","resampled image"],"prmu":["P","P","R","M"]}
{"id":"244","title":"Symbiosis or alienation: advancing the university press\/research library relationship through electronic scholarly communication","abstract":"University presses and research libraries have a long tradition of collaboration. The rapidly expanding electronic scholarly communication environment offers important new opportunities for cooperation and for innovative new models of publishing. The economics of libraries and scholarly publishers have strained the working relationship and promoted debates on important information policy issues. This article explores the context for advancing the partnership, cites examples of joint efforts in electronic publishing, and presents an action plan for working together","tok_text":"symbiosi or alien : advanc the univers press \/ research librari relationship through electron scholarli commun \n univers press and research librari have a long tradit of collabor . the rapidli expand electron scholarli commun environ offer import new opportun for cooper and for innov new model of publish . the econom of librari and scholarli publish have strain the work relationship and promot debat on import inform polici issu . thi articl explor the context for advanc the partnership , cite exampl of joint effort in electron publish , and present an action plan for work togeth","ordered_present_kp":[31,85,312,413,524],"keyphrases":["university press\/research library relationship","electronic scholarly communication","economics","information policy","electronic publishing"],"prmu":["P","P","P","P","P"]}
{"id":"1982","title":"Verifying resonant grounding in distribution systems","abstract":"The authors describe RESFAL, a software tool that can check on the behavior of distribution network resonant grounding systems with regard to compensation coil tuning and to fault detection","tok_text":"verifi reson ground in distribut system \n the author describ resfal , a softwar tool that can check on the behavior of distribut network reson ground system with regard to compens coil tune and to fault detect","ordered_present_kp":[137,172,197],"keyphrases":["resonant grounding systems","compensation coil tuning","fault detection","RESFAL software tool","computer simulation","power distribution systems"],"prmu":["P","P","P","R","U","M"]}
{"id":"2176","title":"Why your Web strategy is, err, wrong","abstract":"An awkward look at a few standard views from the author, who thinks that most people have got it, err, wrong. Like every other investment, when the time comes to sign the contract, the question that should be asked is not whether it is a good investment, but whether it is the best investment the firm can make with the money. the author argues that he would be surprised if any law firm Web site he has seen yet would jump that particular hurdle","tok_text":"whi your web strategi is , err , wrong \n an awkward look at a few standard view from the author , who think that most peopl have got it , err , wrong . like everi other invest , when the time come to sign the contract , the question that should be ask is not whether it is a good invest , but whether it is the best invest the firm can make with the money . the author argu that he would be surpris if ani law firm web site he ha seen yet would jump that particular hurdl","ordered_present_kp":[9,406],"keyphrases":["Web strategy","law firm Web site"],"prmu":["P","P"]}
{"id":"2133","title":"Nonlockability in multirings and hypercubes at serial transmission of data blocks","abstract":"For the multiring and hypercube, a method of conflictless realization of an arbitrary permutation of \"large\" data items that can be divided into many \"smaller\" data blocks was considered, and its high efficiency was demonstrated","tok_text":"nonlock in multir and hypercub at serial transmiss of data block \n for the multir and hypercub , a method of conflictless realiz of an arbitrari permut of \" larg \" data item that can be divid into mani \" smaller \" data block wa consid , and it high effici wa demonstr","ordered_present_kp":[0,11,22],"keyphrases":["nonlockability","multirings","hypercubes","data block serial transmission","multiprocessor computer systems"],"prmu":["P","P","P","R","U"]}
{"id":"284","title":"Project-based learning: teachers learning and using high-tech to preserve Cajun culture","abstract":"Using project-based learning pedagogy in EdTc 658 Advances in Educational Technology, the author has trained inservice teachers in Southwestern Louisiana with an advanced computer multimedia program called Director(R) (Macromedia, Inc.). The content of this course focused on modeling the project-based learning pedagogy and researching Acadian's traditions and legacy. With the multi-functions of microcomputers, new technologies were used to preserve and celebrate the local culture with superiority of text, graphics, animation, sound, and video. The article describes how several groups of school teachers in the surrounding areas of a regional state university of Louisiana learned computer multimedia using project-based learning and integrated their learning into local cultural heritage","tok_text":"project-bas learn : teacher learn and use high-tech to preserv cajun cultur \n use project-bas learn pedagogi in edtc 658 advanc in educ technolog , the author ha train inservic teacher in southwestern louisiana with an advanc comput multimedia program call director(r ) ( macromedia , inc. ) . the content of thi cours focus on model the project-bas learn pedagogi and research acadian 's tradit and legaci . with the multi-funct of microcomput , new technolog were use to preserv and celebr the local cultur with superior of text , graphic , anim , sound , and video . the articl describ how sever group of school teacher in the surround area of a region state univers of louisiana learn comput multimedia use project-bas learn and integr their learn into local cultur heritag","ordered_present_kp":[0,20,63,82,112,168,219,272,447,496,608,649,226,757],"keyphrases":["project-based learning","teachers","Cajun culture","project-based learning pedagogy","EdTc 658 Advances in Educational Technology","inservice teachers","advanced computer multimedia program","computer multimedia","Macromedia","new technologies","local culture","school teachers","regional state university","local cultural heritage","Director","Acadian traditions"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","U","R"]}
{"id":"1942","title":"Precoded OFDM with adaptive vector channel allocation for scalable video transmission over frequency-selective fading channels","abstract":"Orthogonal frequency division multiplexing (OFDM) has been applied in broadband wireline and wireless systems for high data rate transmission where severe intersymbol interference (ISI) always occurs. The conventional OFDM system provides advantages through conversion of an ISI channel into ISI-free subchannels at multiple frequency bands. However, it may suffer from channel spectral nulls and heavy data rate overhead due to cyclic prefix insertion. Previously, a new OFDM framework, the precoded OFDM, has been proposed to mitigate the above two problems through precoding and conversion of an ISI channel into ISI-free vector channels. In this paper, we consider the application of the precoded OFDM system to efficient scalable video transmission. We propose to enhance the precoded OFDM system with adaptive vector channel allocation to provide stronger protection against errors to more important layers in the layered bit stream structure of scalable video. The more critical layers, or equivalently, the lower layers, are allocated vector channels of higher transmission quality. The channel quality is characterized by Frobenius norm metrics; based on channel estimation at the receiver. The channel allocation information is fed back periodically to the transmitter through a control channel. Simulation results have demonstrated the robustness of the proposed scheme to noise and fading inherent in wireless channels","tok_text":"precod ofdm with adapt vector channel alloc for scalabl video transmiss over frequency-select fade channel \n orthogon frequenc divis multiplex ( ofdm ) ha been appli in broadband wirelin and wireless system for high data rate transmiss where sever intersymbol interfer ( isi ) alway occur . the convent ofdm system provid advantag through convers of an isi channel into isi-fre subchannel at multipl frequenc band . howev , it may suffer from channel spectral null and heavi data rate overhead due to cyclic prefix insert . previous , a new ofdm framework , the precod ofdm , ha been propos to mitig the abov two problem through precod and convers of an isi channel into isi-fre vector channel . in thi paper , we consid the applic of the precod ofdm system to effici scalabl video transmiss . we propos to enhanc the precod ofdm system with adapt vector channel alloc to provid stronger protect against error to more import layer in the layer bit stream structur of scalabl video . the more critic layer , or equival , the lower layer , are alloc vector channel of higher transmiss qualiti . the channel qualiti is character by frobeniu norm metric ; base on channel estim at the receiv . the channel alloc inform is fed back period to the transmitt through a control channel . simul result have demonstr the robust of the propos scheme to nois and fade inher in wireless channel","ordered_present_kp":[0,48,77,109,443,469,353,671,17,938,1024,992,1097,1129,1160,1194,1261,1310],"keyphrases":["precoded OFDM","adaptive vector channel allocation","scalable video transmission","frequency-selective fading channels","orthogonal frequency division multiplexing","ISI channel","channel spectral nulls","heavy data rate overhead","ISI-free vector channels","layered bit stream structure","critical layers","lower layers","channel quality","Frobenius norm metrics","channel estimation","channel allocation information","control channel","robustness"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"279","title":"Computer mediated communication and university international students","abstract":"The design for the preliminary study presented was based on the experiences of the international students and faculty members of a small southwest university being surveyed and interviewed. The data collection procedure blends qualitative and quantitative data. A strong consensus was found that supports the study's premise that there is an association between the use of computer mediated communication (CMC) and teaching and learning performance of international students. Both groups believe CMC to be an effective teaching and learning tool by: increasing the frequency and quality of communication between students and instructors; improving language skills through increased writing and communication opportunities; allowing students and instructors to stay current and to compete effectively; providing alternative teaching and learning methods to increase students' confidence in their ability to communicate effectively with peers and instructors; and improving the instructors' pedagogical focus and questioning techniques","tok_text":"comput mediat commun and univers intern student \n the design for the preliminari studi present wa base on the experi of the intern student and faculti member of a small southwest univers be survey and interview . the data collect procedur blend qualit and quantit data . a strong consensu wa found that support the studi 's premis that there is an associ between the use of comput mediat commun ( cmc ) and teach and learn perform of intern student . both group believ cmc to be an effect teach and learn tool by : increas the frequenc and qualiti of commun between student and instructor ; improv languag skill through increas write and commun opportun ; allow student and instructor to stay current and to compet effect ; provid altern teach and learn method to increas student ' confid in their abil to commun effect with peer and instructor ; and improv the instructor ' pedagog focu and question techniqu","ordered_present_kp":[0,25,143,163,217,256,397,407,417,598,638,578,825,875,892],"keyphrases":["computer mediated communication","university international students","faculty members","small southwest university","data collection procedure","quantitative data","CMC","teaching","learning performance","instructors","language skills","communication opportunities","peers","pedagogical focus","questioning techniques","qualitative data","student confidence"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"364","title":"MACLP: multi agent constraint logic programming","abstract":"Multi agent systems (MAS) have become the key technology for decomposing complex problems in order to solve them more efficiently, or for problems distributed in nature. However, many industrial applications, besides their distributed nature, also involve a large number of parameters and constraints, i.e. they are combinatorial. Solving such particularly hard problems efficiently requires programming tools that combine MAS technology with a programming schema that facilitates the modeling and solution of constraints. This paper presents MACLP (multi agent constraint logic programming), a logic programming platform for building, in a declarative way, multi agent systems with constraint-solving capabilities. MACLP extends CSPCONS, a logic programming system that permits distributed program execution through communicating sequential Prolog processes with constraints, by providing all the necessary facilities for communication between agents. These facilities abstract from the programmer all the low-level details of the communication and allow him to focus on the development of the agent itself","tok_text":"maclp : multi agent constraint logic program \n multi agent system ( ma ) have becom the key technolog for decompos complex problem in order to solv them more effici , or for problem distribut in natur . howev , mani industri applic , besid their distribut natur , also involv a larg number of paramet and constraint , i.e. they are combinatori . solv such particularli hard problem effici requir program tool that combin ma technolog with a program schema that facilit the model and solut of constraint . thi paper present maclp ( multi agent constraint logic program ) , a logic program platform for build , in a declar way , multi agent system with constraint-solv capabl . maclp extend cspcon , a logic program system that permit distribut program execut through commun sequenti prolog process with constraint , by provid all the necessari facil for commun between agent . these facil abstract from the programm all the low-level detail of the commun and allow him to focu on the develop of the agent itself","ordered_present_kp":[8,47,293,369,733,766],"keyphrases":["multi agent constraint logic programming","multi agent systems","parameters","hard problems","distributed program execution","communicating sequential Prolog processes","combinatorial problems","constraint solving"],"prmu":["P","P","P","P","P","P","R","R"]}
{"id":"2096","title":"A spatial rainfall simulator for crop production modeling in Southern Africa","abstract":"This paper describes a methodology for simulating rainfall in dekads across a set of spatial units in areas where long-term meteorological records are available for a small number of sites only. The work forms part of a larger simulation model of the food system in a district of Zimbabwe, which includes a crop production component for yields of maize, small grains and groundnuts. Only a limited number of meteorological stations are available within or surrounding the district that have long time series of rainfall records. Preliminary analysis of rainfall data for these stations suggested that intra-seasonal temporal correlation was negligible, but that rainfall at any given station was correlated with rainfall at neighbouring stations. This spatial correlation structure can be modeled using a multivariate normal distribution consisting of 30 related variables, representing dekadly rainfall in each of the 30 wards. For each ward, log-transformed rainfall for each of the 36 dekads in the year was characterized by a mean and standard deviation, which were interpolated from surrounding meteorological stations. A covariance matrix derived from a distance measure was then used to represent the spatial correlation between wards. Sets of random numbers were then drawn from this distribution to simulate rainfall across the wards in any given dekad. Cross-validation of estimated rainfall parameters against observed parameters for the one meteorological station within the district suggests that the interpolation process works well. The methodology developed is useful in situations where long-term climatic records are scarce and where rainfall shows pronounced spatial correlation, but negligible temporal correlation","tok_text":"a spatial rainfal simul for crop product model in southern africa \n thi paper describ a methodolog for simul rainfal in dekad across a set of spatial unit in area where long-term meteorolog record are avail for a small number of site onli . the work form part of a larger simul model of the food system in a district of zimbabw , which includ a crop product compon for yield of maiz , small grain and groundnut . onli a limit number of meteorolog station are avail within or surround the district that have long time seri of rainfal record . preliminari analysi of rainfal data for these station suggest that intra-season tempor correl wa neglig , but that rainfal at ani given station wa correl with rainfal at neighbour station . thi spatial correl structur can be model use a multivari normal distribut consist of 30 relat variabl , repres dekadli rainfal in each of the 30 ward . for each ward , log-transform rainfal for each of the 36 dekad in the year wa character by a mean and standard deviat , which were interpol from surround meteorolog station . a covari matrix deriv from a distanc measur wa then use to repres the spatial correl between ward . set of random number were then drawn from thi distribut to simul rainfal across the ward in ani given dekad . cross-valid of estim rainfal paramet against observ paramet for the one meteorolog station within the district suggest that the interpol process work well . the methodolog develop is use in situat where long-term climat record are scarc and where rainfal show pronounc spatial correl , but neglig tempor correl","ordered_present_kp":[103,28,320,1061,525,565,736,779,50],"keyphrases":["crop production modeling","Southern Africa","simulating rainfall","Zimbabwe","rainfall records","rainfall data","spatial correlation","multivariate normal distribution","covariance matrix","parameter estimation"],"prmu":["P","P","P","P","P","P","P","P","P","R"]}
{"id":"321","title":"A multimodal data collection tool using REALbasic and Mac OS X","abstract":"This project uses REALbasic 3.5 in the Mac OS X environment for development of a configuration tool that builds a data collection procedure for investigating the effectiveness of sonified graphs. The advantage of using REALbasic with the Mac OS X system is that it provides rapid development of stimulus presentation, direct recording of data to files, and control over other procedural issues. The program can be made to run natively on the new Mac OS X system, older Mac OS systems, and Windows (98SE, ME, 2000 PRO). With modification, similar programs could be used to present any number of visual\/auditory stimulus combinations, complete with questions for each stimulus","tok_text":"a multimod data collect tool use realbas and mac os x \n thi project use realbas 3.5 in the mac os x environ for develop of a configur tool that build a data collect procedur for investig the effect of sonifi graph . the advantag of use realbas with the mac os x system is that it provid rapid develop of stimulu present , direct record of data to file , and control over other procedur issu . the program can be made to run nativ on the new mac os x system , older mac os system , and window ( 98se , me , 2000 pro ) . with modif , similar program could be use to present ani number of visual \/ auditori stimulu combin , complet with question for each stimulu","ordered_present_kp":[2,33,91,125,11,201,595,304,485],"keyphrases":["multimodal data collection tool","data collection","REALbasic","Mac OS X environment","configuration tool","sonified graphs","stimulus presentation","Windows","auditory stimulus","visual data comprehension","psychology","visual stimulus","direct data recording"],"prmu":["P","P","P","P","P","P","P","P","P","M","U","R","R"]}
{"id":"399","title":"Low-voltage DRAM sensing scheme with offset-cancellation sense amplifier","abstract":"A novel bitline sensing scheme is proposed for low-voltage DRAM to achieve low power dissipation and compatibility with low-voltage CMOS. One of the major obstacles in low-voltage DRAM is the degradation of data-retention time due to low signal level at the memory cell, which requires power-consuming refresh operations more frequently. This paper proposes an offset-cancellation sense-amplifier scheme (OCSA) that improves data-retention time significantly even at low supply voltage. It also improves die efficiency, because the proposed scheme reduces the number of sense amplifiers by supporting more cells in each sense amplifier. Measurements show that the data-retention time of the proposed scheme at 1.5-V supply voltage is 2.4 times of the conventional scheme at 2.0 V","tok_text":"low-voltag dram sens scheme with offset-cancel sens amplifi \n a novel bitlin sens scheme is propos for low-voltag dram to achiev low power dissip and compat with low-voltag cmo . one of the major obstacl in low-voltag dram is the degrad of data-retent time due to low signal level at the memori cell , which requir power-consum refresh oper more frequent . thi paper propos an offset-cancel sense-amplifi scheme ( ocsa ) that improv data-retent time significantli even at low suppli voltag . it also improv die effici , becaus the propos scheme reduc the number of sens amplifi by support more cell in each sens amplifi . measur show that the data-retent time of the propos scheme at 1.5-v suppli voltag is 2.4 time of the convent scheme at 2.0 v","ordered_present_kp":[70,129,240,288,315],"keyphrases":["bitline sensing scheme","low power dissipation","data-retention time","memory cell","power-consuming refresh operations","LV DRAM sensing scheme","low-voltage sensing scheme","offset-cancellation sense amplifier scheme","low-voltage CMOS compatibility","differential amplifier configuration","sensing margin","1.5 V"],"prmu":["P","P","P","P","P","M","R","R","R","M","M","M"]}
{"id":"218","title":"ISCSI poised to lower SAN costs","abstract":"IT managers building storage area networks or expanding their capacity may be able to save money by using iSCSI and IP systems rather than Fibre Channel technologies","tok_text":"iscsi pois to lower san cost \n it manag build storag area network or expand their capac may be abl to save money by use iscsi and ip system rather than fibr channel technolog","ordered_present_kp":[20,46,0,130],"keyphrases":["iSCSI","SAN costs","storage area networks","IP systems"],"prmu":["P","P","P","P"]}
{"id":"1966","title":"Application of nonlinear time series analysis techniques to high-frequency currency exchange data","abstract":"In this work we have applied nonlinear time series analysis to high-frequency currency exchange data. The time series studied are the exchange rates between the US Dollar and 18 other foreign currencies from within and without the Euro zone. Our goal was to determine if their dynamical behaviours were in some way correlated. The nonexistence of stationarity called for the application of recurrence quantification analysis as a tool for this analysis, and is based on the definition of several parameters that allow for the quantification of recurrence plots. The method was checked using the European Monetary System currency exchanges. The results show, as expected, the high correlation between the currencies that are part of the Euro, but also a strong correlation between the Japanese Yen, the Canadian Dollar and the British Pound. Singularities of the series are also demonstrated taking into account historical events, in 1996, in the Euro zone","tok_text":"applic of nonlinear time seri analysi techniqu to high-frequ currenc exchang data \n in thi work we have appli nonlinear time seri analysi to high-frequ currenc exchang data . the time seri studi are the exchang rate between the us dollar and 18 other foreign currenc from within and without the euro zone . our goal wa to determin if their dynam behaviour were in some way correl . the nonexist of stationar call for the applic of recurr quantif analysi as a tool for thi analysi , and is base on the definit of sever paramet that allow for the quantif of recurr plot . the method wa check use the european monetari system currenc exchang . the result show , as expect , the high correl between the currenc that are part of the euro , but also a strong correl between the japanes yen , the canadian dollar and the british pound . singular of the seri are also demonstr take into account histor event , in 1996 , in the euro zone","ordered_present_kp":[10,50,203,228,251,295,398,431,556,598,772,790,814,887],"keyphrases":["nonlinear time series","high-frequency currency exchange data","exchange rates","US Dollar","foreign currencies","Euro zone","stationarity","recurrence quantification analysis","recurrence plots","European Monetary System","Japanese Yen","Canadian Dollar","British Pound","historical events","econophysics","nonlinear dynamics"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","U","R"]}
{"id":"2152","title":"Virtual engineering office: a state-of-the-art platform for engineering collaboration","abstract":"A sales force in Latin America, the design department in Europe, and production in Asia? Arrangements of this kind are the new business reality for today's global manufacturing companies. But how are such global operations to be effectively coordinated? ABB's answer was to develop and implement a new platform for high-performance, real-time collaboration. Globally distributed engineering teams can now work together, regardless of time, location or the CAD system they use, making ABB easier to do business with, for customers as well as suppliers","tok_text":"virtual engin offic : a state-of-the-art platform for engin collabor \n a sale forc in latin america , the design depart in europ , and product in asia ? arrang of thi kind are the new busi realiti for today 's global manufactur compani . but how are such global oper to be effect coordin ? abb 's answer wa to develop and implement a new platform for high-perform , real-tim collabor . global distribut engin team can now work togeth , regardless of time , locat or the cad system they use , make abb easier to do busi with , for custom as well as supplier","ordered_present_kp":[0,24,184,210,290,470,386],"keyphrases":["virtual engineering office","state-of-the-art","business","global manufacturing companies","ABB","globally distributed engineering teams","CAD system","engineering collaboration platform"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"2117","title":"The p-p rearrangement and failure-tolerance of double p-ary multirings and generalized hypercubes","abstract":"It is shown that an arbitrary grouped p-element permutation can be implemented in a conflict-free way through the commutation of channels on the double p-ary multiring or the double p-ary hypercube. It is revealed that in arbitrary single-element permutations, these commutators display the property of the (p-1)-nodal failure-tolerance and the generalized hypercube displays in addition the property of the (p-1)-channel failure-tolerance","tok_text":"the p-p rearrang and failure-toler of doubl p-ari multir and gener hypercub \n it is shown that an arbitrari group p-element permut can be implement in a conflict-fre way through the commut of channel on the doubl p-ari multir or the doubl p-ari hypercub . it is reveal that in arbitrari single-el permut , these commut display the properti of the ( p-1)-nodal failure-toler and the gener hypercub display in addit the properti of the ( p-1)-channel failure-toler","ordered_present_kp":[4,21,38,61,114,287,182],"keyphrases":["p-p rearrangement","failure-tolerance","double p-ary multirings","generalized hypercubes","p-element permutation","commutators","single-element permutations","conflict-free implementation"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"340","title":"Temelin casts its shadow [nuclear power plant]","abstract":"Reservations about Temelin nuclear plant in the Czech Republic are political rather than technical. This paper discusses the problems of turbogenerator vibrations and how they were diagnosed. The paper also discusses some of the other problems of commissioning the power plant. The simulator used for training new staff is also mentioned","tok_text":"temelin cast it shadow [ nuclear power plant ] \n reserv about temelin nuclear plant in the czech republ are polit rather than technic . thi paper discuss the problem of turbogener vibrat and how they were diagnos . the paper also discuss some of the other problem of commiss the power plant . the simul use for train new staff is also mention","ordered_present_kp":[62,91,169],"keyphrases":["Temelin nuclear plant","Czech Republic","turbogenerator vibrations","power plant commissioning","training simulator"],"prmu":["P","P","P","R","R"]}
{"id":"305","title":"Full-screen ultrafast video modes over-clocked by simple VESA routines and registers reprogramming under MS-DOS","abstract":"Fast full-screen presentation of stimuli is necessary in psychological research. Although Spitczok von Brisinski (1994) introduced a method that achieved ultrafast display by reprogramming the registers, he could not produce an acceptable full-screen display. In this report, the author introduces a new method combining VESA routine calling with register reprogramming that can yield a display at 640 * 480 resolution, with a refresh rate of about 150 Hz","tok_text":"full-screen ultrafast video mode over-clock by simpl vesa routin and regist reprogram under ms-do \n fast full-screen present of stimuli is necessari in psycholog research . although spitczok von brisinski ( 1994 ) introduc a method that achiev ultrafast display by reprogram the regist , he could not produc an accept full-screen display . in thi report , the author introduc a new method combin vesa routin call with regist reprogram that can yield a display at 640 * 480 resolut , with a refresh rate of about 150 hz","ordered_present_kp":[0,152,396,92,69],"keyphrases":["full-screen ultrafast video modes","register reprogramming","MS-DOS","psychological research","VESA routine calling","fast full-screen stimuli presentation"],"prmu":["P","P","P","P","P","R"]}
{"id":"338","title":"Down up [IT projects]","abstract":"Despite the second quarter's gloomy GDP report, savvy CIOs are forging ahead with big IT projects that will position their companies to succeed when the economy soars again","tok_text":"down up [ it project ] \n despit the second quarter 's gloomi gdp report , savvi cio are forg ahead with big it project that will posit their compani to succeed when the economi soar again","ordered_present_kp":[],"keyphrases":["strategic technology projects","Walgreen","Ford","Caterpillar","Victoria's Secret","Morgan Stanley","Staples"],"prmu":["M","U","U","U","M","U","U"]}
{"id":"2037","title":"Regularization of linear regression problems","abstract":"The study considers robust estimation of linear regression parameters by the regularization method, the pseudoinverse method, and the Bayesian method allowing for correlations and errors in the data. Regularizing algorithms are constructed and their relationship with pseudoinversion, the Bayesian approach, and BLUE is investigated","tok_text":"regular of linear regress problem \n the studi consid robust estim of linear regress paramet by the regular method , the pseudoinvers method , and the bayesian method allow for correl and error in the data . regular algorithm are construct and their relationship with pseudoinvers , the bayesian approach , and blue is investig","ordered_present_kp":[53,69,120,150,120,286,310],"keyphrases":["robust estimation","linear regression parameters","pseudoinverse method","pseudoinversion","Bayesian method","Bayesian approach","BLUE","linear regression problems regularization"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"380","title":"Quantitative speed control for SRM drive using fuzzy adapted inverse model","abstract":"Quantitative and robust speed control for a switched reluctance motor (SRM) drive is considered to be rather difficult and challenging owing to its highly nonlinear dynamic behavior. A speed control scheme having two-degree-of-freedom (2DOF) structure is developed here to improve the speed dynamic response of an SRM drive. In the proposed control scheme, the feedback controller is quantitatively designed to meet the desired regulation control requirements first. Then a reference model and a command feedforward controller based on an inverse plant model are employed to yield the desired tracking response at nominal case. As the variations of system parameters and operating conditions occur, the prescribed control specifications may not be satisfied any more. To improve this, the inverse model is adaptively tuned by a fuzzy control scheme so that the model-following tracking error is significantly reduced. In addition, a simple disturbance cancellation robust controller is added to improve the tracking and regulation control performances further","tok_text":"quantit speed control for srm drive use fuzzi adapt invers model \n quantit and robust speed control for a switch reluct motor ( srm ) drive is consid to be rather difficult and challeng owe to it highli nonlinear dynam behavior . a speed control scheme have two-degree-of-freedom ( 2dof ) structur is develop here to improv the speed dynam respons of an srm drive . in the propos control scheme , the feedback control is quantit design to meet the desir regul control requir first . then a refer model and a command feedforward control base on an invers plant model are employ to yield the desir track respons at nomin case . as the variat of system paramet and oper condit occur , the prescrib control specif may not be satisfi ani more . to improv thi , the invers model is adapt tune by a fuzzi control scheme so that the model-follow track error is significantli reduc . in addit , a simpl disturb cancel robust control is ad to improv the track and regul control perform further","ordered_present_kp":[0,26,40,106,203,328,454,490,508,547,596,643,662,695,792,825],"keyphrases":["quantitative speed control","SRM drive","fuzzy adapted inverse model","switched reluctance motor","nonlinear dynamic behavior","speed dynamic response","regulation control requirements","reference model","command feedforward controller","inverse plant model","tracking response","system parameters","operating conditions","control specifications","fuzzy control scheme","model-following tracking error","two-degree-of-freedom structure","disturbance cancellation controller"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"33","title":"Fuzzy control of multivariable process by modified error decoupling","abstract":"In this paper, a control concept for the squared (equal number of inputs and outputs) multivariable process systems is given. The proposed control system consists of two parts, single loop fuzzy controllers in each loop and a centralized decoupling unit. The fuzzy control system uses feedback control to minimize the error in the loop and the decoupler uses an adaptive technique to mitigate loop interactions. The decoupler predicts the interacting loop changes and modifies the input (error) of the loop controller. The controller was tested on the simulation model of \"single component vaporizer\" process","tok_text":"fuzzi control of multivari process by modifi error decoupl \n in thi paper , a control concept for the squar ( equal number of input and output ) multivari process system is given . the propos control system consist of two part , singl loop fuzzi control in each loop and a central decoupl unit . the fuzzi control system use feedback control to minim the error in the loop and the decoupl use an adapt techniqu to mitig loop interact . the decoupl predict the interact loop chang and modifi the input ( error ) of the loop control . the control wa test on the simul model of \" singl compon vapor \" process","ordered_present_kp":[17,38,273,325],"keyphrases":["multivariable process","modified error decoupling","centralized decoupling unit","feedback control","squared multivariable process systems","square multivariable process systems","single-loop fuzzy controllers","error minimization","loop interaction mitigation","single component vaporizer process","set point changes","load changes"],"prmu":["P","P","P","P","R","R","M","R","R","R","M","M"]}
{"id":"2192","title":"The ubiquitous provisioning of internet services to portable devices","abstract":"Advances in mobile telecommunications and device miniaturization call for providing both standard and novel location- and context-dependent Internet services to mobile clients. Mobile agents are dynamic, asynchronous, and autonomous, making the MA programming paradigm suitable for developing novel middleware for mobility-enabled services","tok_text":"the ubiquit provis of internet servic to portabl devic \n advanc in mobil telecommun and devic miniatur call for provid both standard and novel location- and context-depend internet servic to mobil client . mobil agent are dynam , asynchron , and autonom , make the ma program paradigm suitabl for develop novel middlewar for mobility-en servic","ordered_present_kp":[67,88,22,191,206,325,311],"keyphrases":["Internet services","mobile telecommunications","device miniaturization","mobile clients","mobile agents","middleware","mobility-enabled services"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"225","title":"The eyes have it [hotel security]","abstract":"CCTV systems can help lodging establishments accomplish a range of objectives, from deterring criminals to observing staff interactions with clientele. But pitfalls can arise if the CCTV system has not been properly integrated into the overall hotel security plan. CCTV system designs at new hotel properties are often too sophisticated, too complicated, and too costly, and do not take into consideration the security realities of site management. These problems arise when the professionals designing or installing the system, including architects, construction engineers, integrators, and consultants, are not familiar with a hotel's operating strategies or security standards","tok_text":"the eye have it [ hotel secur ] \n cctv system can help lodg establish accomplish a rang of object , from deter crimin to observ staff interact with clientel . but pitfal can aris if the cctv system ha not been properli integr into the overal hotel secur plan . cctv system design at new hotel properti are often too sophist , too complic , and too costli , and do not take into consider the secur realiti of site manag . these problem aris when the profession design or instal the system , includ architect , construct engin , integr , and consult , are not familiar with a hotel 's oper strategi or secur standard","ordered_present_kp":[18,34,408,583],"keyphrases":["hotel security","CCTV system","site management","operating strategies"],"prmu":["P","P","P","P"]}
{"id":"260","title":"Prospecting virtual collections","abstract":"Virtual collections are a distinct sub-species of digital collections and digital archives. Archivists and curators as archivists and curators do not construct virtual collections; rather they enable virtual collections through the application of descriptive and other standards. Virtual collections are constructed by end users","tok_text":"prospect virtual collect \n virtual collect are a distinct sub-speci of digit collect and digit archiv . archivist and curat as archivist and curat do not construct virtual collect ; rather they enabl virtual collect through the applic of descript and other standard . virtual collect are construct by end user","ordered_present_kp":[9,71,89,104,118,301,71],"keyphrases":["virtual collections","digital collections","digitization","digital archives","archivists","curators","end users","descriptive standards"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"2035","title":"Search for efficient solutions of multi-criterion problems by target-level method","abstract":"The target-level method is considered for solving continuous multi-criterion maximization problems. In the first step, the decision-maker specifies a target-level point (the desired criterion values); then in the set of vector evaluations we seek points that are closest to the target point in the Chebyshev metric. The vector evaluations obtained in this way are in general weakly efficient. To identify the efficient evaluations, the second step maximizes the sum of the criteria on the set generated in step 1. We prove the relationship between the evaluations and decisions obtained by the proposed procedure, on the one hand, and the efficient (weakly efficient) evaluations and decisions, on the other hand. If the Edgeworth-Pareto hull of the set of vector evaluations is convex, the set of efficient vector evaluations can be approximated by the proposed method","tok_text":"search for effici solut of multi-criterion problem by target-level method \n the target-level method is consid for solv continu multi-criterion maxim problem . in the first step , the decision-mak specifi a target-level point ( the desir criterion valu ) ; then in the set of vector evalu we seek point that are closest to the target point in the chebyshev metric . the vector evalu obtain in thi way are in gener weakli effici . to identifi the effici evalu , the second step maxim the sum of the criteria on the set gener in step 1 . we prove the relationship between the evalu and decis obtain by the propos procedur , on the one hand , and the effici ( weakli effici ) evalu and decis , on the other hand . if the edgeworth-pareto hull of the set of vector evalu is convex , the set of effici vector evalu can be approxim by the propos method","ordered_present_kp":[27,54,119,206,346,717],"keyphrases":["multi-criterion problems","target-level method","continuous multi-criterion maximization problems","target-level point","Chebyshev metric","Edgeworth-Pareto hull"],"prmu":["P","P","P","P","P","P"]}
{"id":"382","title":"Robust wavelet neuro control for linear brushless motors","abstract":"Design, simulation and experimental implementation of a wavelet basis function network learning controller for linear brushless dc motors (LBDCM) are considered. Stability robustness with position tracking is the primary concern. The proposed controller deals mainly with external disturbances, e.g. nonlinear friction force and payload variation in motion control of linear motors. It consists of two parts, one is a state feedback component, and the other one is a learning feedback component. The state feedback controller is designed on the basis of a simple linear model, and the learning feedback component is a wavelet neural controller. The attenuation effect of wavelet neural networks on friction force is first verified by the numerical method. The learning effect of wavelet neural networks on friction force is also shown in the numerical results. Then, a wavelet neural network is applied on a real LBDCM to on-line suppress the friction force, which may be variable due to the different lubrication. The effectiveness of the proposed control schemes is demonstrated by simulated and experimental results","tok_text":"robust wavelet neuro control for linear brushless motor \n design , simul and experiment implement of a wavelet basi function network learn control for linear brushless dc motor ( lbdcm ) are consid . stabil robust with posit track is the primari concern . the propos control deal mainli with extern disturb , e.g. nonlinear friction forc and payload variat in motion control of linear motor . it consist of two part , one is a state feedback compon , and the other one is a learn feedback compon . the state feedback control is design on the basi of a simpl linear model , and the learn feedback compon is a wavelet neural control . the attenu effect of wavelet neural network on friction forc is first verifi by the numer method . the learn effect of wavelet neural network on friction forc is also shown in the numer result . then , a wavelet neural network is appli on a real lbdcm to on-lin suppress the friction forc , which may be variabl due to the differ lubric . the effect of the propos control scheme is demonstr by simul and experiment result","ordered_present_kp":[0,33,103,179,200,219,292,314,342,360,427,474,637,324,963],"keyphrases":["robust wavelet neuro control","linear brushless motors","wavelet basis function network","LBDCM","stability robustness","position tracking","external disturbances","nonlinear friction force","friction force","payload variation","motion control","state feedback component","learning feedback component","attenuation effect","lubrication"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2070","title":"Modularity in technology and organization","abstract":"The paper is an attempt to raid both the literature on modular design and the literature on property rights to create the outlines of a modularity theory of the firm. Such a theory will look at firms, and other organizations, in terms of the partitioning of rights-understood as protected spheres of authority-among cooperating parties. It will assert that organizations reflect nonmodular structures, that is, structures in which decision rights, rights of alienation, and residual claims to income do not all reside in the same hands","tok_text":"modular in technolog and organ \n the paper is an attempt to raid both the literatur on modular design and the literatur on properti right to creat the outlin of a modular theori of the firm . such a theori will look at firm , and other organ , in term of the partit of rights-understood as protect sphere of authority-among cooper parti . it will assert that organ reflect nonmodular structur , that is , structur in which decis right , right of alien , and residu claim to incom do not all resid in the same hand","ordered_present_kp":[0,11,25,123,324,373,423,437],"keyphrases":["modularity","technology","organization","property rights","cooperating parties","nonmodular structures","decision rights","rights of alienation","partitioning of rights","authority","transaction costs"],"prmu":["P","P","P","P","P","P","P","P","R","U","U"]}
{"id":"1959","title":"Quantum market games","abstract":"We propose a quantum-like description of markets and economics. The approach has roots in the recently developed quantum game theory","tok_text":"quantum market game \n we propos a quantum-lik descript of market and econom . the approach ha root in the recent develop quantum game theori","ordered_present_kp":[0,69,121],"keyphrases":["quantum market games","economics","quantum game theory","quantum strategies","financial markets"],"prmu":["P","P","P","M","M"]}
{"id":"2190","title":"Standards for service discovery and delivery","abstract":"For the past five years, competing industries and standards developers have been hotly pursuing automatic configuration, now coined the broader term service discovery. Jini, Universal Plug and Play (UPnP), Salutation, and Service Location Protocol are among the front-runners in this new race. However, choosing service discovery as the topic of the hour goes beyond the need for plug-and-play solutions or support for the SOHO (small office\/home office) user. Service discovery's potential in mobile and pervasive computing environments motivated my choice","tok_text":"standard for servic discoveri and deliveri \n for the past five year , compet industri and standard develop have been hotli pursu automat configur , now coin the broader term servic discoveri . jini , univers plug and play ( upnp ) , salut , and servic locat protocol are among the front-runn in thi new race . howev , choos servic discoveri as the topic of the hour goe beyond the need for plug-and-play solut or support for the soho ( small offic \/ home offic ) user . servic discoveri 's potenti in mobil and pervas comput environ motiv my choic","ordered_present_kp":[13,193,200,233,245,511],"keyphrases":["service discovery","Jini","Universal Plug and Play","Salutation","Service Location Protocol","pervasive computing","mobile computing"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"227","title":"Relativistic constraints on the distinguishability of orthogonal quantum states","abstract":"The constraints imposed by special relativity on the distinguishability of quantum states are discussed. An explicit expression relating the probability of an error in distinguishing two orthogonal single-photon states to their structure, the time t at which a measurement starts, and the interval of time T elapsed from the start of the measurement until the time at which the outcome is obtained by an observer is given as an example","tok_text":"relativist constraint on the distinguish of orthogon quantum state \n the constraint impos by special rel on the distinguish of quantum state are discuss . an explicit express relat the probabl of an error in distinguish two orthogon single-photon state to their structur , the time t at which a measur start , and the interv of time t elaps from the start of the measur until the time at which the outcom is obtain by an observ is given as an exampl","ordered_present_kp":[0,44,93,224,421],"keyphrases":["relativistic constraints","orthogonal quantum states","special relativity","orthogonal single-photon states","observer","time interval","nonrelativistic quantum information theory","quantum communication channels","quantum-state distinguishability"],"prmu":["P","P","P","P","P","R","M","M","M"]}
{"id":"262","title":"The impact of EAD adoption on archival programs: a pilot survey of early implementers","abstract":"The article reports the results of a survey conducted to assess the impact that the implementation of Encoded Archival Description (EAD) has on archival programs. By gathering data related to the funding, staffing, and evaluation of EAD programs and about institutional goals for EAD implementation, the study explored how EAD has affected the operations of the institutions which are utilizing it and the extent to which EAD has become a part of regular repository functions","tok_text":"the impact of ead adopt on archiv program : a pilot survey of earli implement \n the articl report the result of a survey conduct to assess the impact that the implement of encod archiv descript ( ead ) ha on archiv program . by gather data relat to the fund , staf , and evalu of ead program and about institut goal for ead implement , the studi explor how ead ha affect the oper of the institut which are util it and the extent to which ead ha becom a part of regular repositori function","ordered_present_kp":[14,27,172,253,260,280,302,320,461],"keyphrases":["EAD adoption","archival programs","Encoded Archival Description","funding","staffing","EAD programs","institutional goals","EAD implementation","regular repository functions","archival descriptive standards","diffusion of innovation"],"prmu":["P","P","P","P","P","P","P","P","P","M","M"]}
{"id":"31","title":"Adaptive neural\/fuzzy control for interpolated nonlinear systems","abstract":"Adaptive control for nonlinear time-varying systems is of both theoretical and practical importance. We propose an adaptive control methodology for a class of nonlinear systems with a time-varying structure. This class of systems is composed of interpolations of nonlinear subsystems which are input-output feedback linearizable. Both indirect and direct adaptive control methods are developed, where the spatially localized models (in the form of Takagi-Sugeno fuzzy systems or radial basis function neural networks) are used as online approximators to learn the unknown dynamics of the system. Without assumptions on rate of change of system dynamics, the proposed adaptive control methods guarantee that all internal signals of the system are bounded and the tracking error is asymptotically stable. The performance of the adaptive controller is demonstrated using a jet engine control problem","tok_text":"adapt neural \/ fuzzi control for interpol nonlinear system \n adapt control for nonlinear time-vari system is of both theoret and practic import . we propos an adapt control methodolog for a class of nonlinear system with a time-vari structur . thi class of system is compos of interpol of nonlinear subsystem which are input-output feedback lineariz . both indirect and direct adapt control method are develop , where the spatial local model ( in the form of takagi-sugeno fuzzi system or radial basi function neural network ) are use as onlin approxim to learn the unknown dynam of the system . without assumpt on rate of chang of system dynam , the propos adapt control method guarante that all intern signal of the system are bound and the track error is asymptot stabl . the perform of the adapt control is demonstr use a jet engin control problem","ordered_present_kp":[0,33,89,422,459,489,538,566,743,826],"keyphrases":["adaptive neural\/fuzzy control","interpolated nonlinear systems","time-varying systems","spatially localized models","Takagi-Sugeno fuzzy systems","radial basis function neural networks","online approximators","unknown dynamics","tracking error","jet engine control","input-output feedback linearizable systems","indirect control","direct control","stability analysis"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R","R","U"]}
{"id":"2128","title":"An optimal control algorithm based on reachability set approximation and linearization","abstract":"The terminal functional of a general control system is refined by studying an analogous problem for a variational system and regularization. A sequential refinement method is designed by combining the local approximation of the reachability set and reduction. The corresponding algorithm has relaxation properties. An illustrative example is given","tok_text":"an optim control algorithm base on reachabl set approxim and linear \n the termin function of a gener control system is refin by studi an analog problem for a variat system and regular . a sequenti refin method is design by combin the local approxim of the reachabl set and reduct . the correspond algorithm ha relax properti . an illustr exampl is given","ordered_present_kp":[3,35,61,74,158,176,188,234,310],"keyphrases":["optimal control algorithm","reachability set approximation","linearization","terminal functional","variational system","regularization","sequential refinement method","local approximation","relaxation properties","determinate systems"],"prmu":["P","P","P","P","P","P","P","P","P","M"]}
{"id":"2150","title":"Data storage: re-format. Closely tracking a fast-moving sector","abstract":"In the past few years the data center market has changed dramatically, forcing many companies into consolidation or bankruptcy. Gone are the days when companies raised millions of dollars to acquire large industrial buildings and transform them into glittering, high-tech palaces filled with the latest telecommunication and data technology. Whereas manufacturers of communication technology deliver the racked equipment in these, often mission-critical, facilities, ABB focuses mainly on the building infrastructure. Besides the very important redundant power supply, ABB also provides the redundant air conditioning and the security system","tok_text":"data storag : re-format . close track a fast-mov sector \n in the past few year the data center market ha chang dramat , forc mani compani into consolid or bankruptci . gone are the day when compani rais million of dollar to acquir larg industri build and transform them into glitter , high-tech palac fill with the latest telecommun and data technolog . wherea manufactur of commun technolog deliv the rack equip in these , often mission-crit , facil , abb focus mainli on the build infrastructur . besid the veri import redund power suppli , abb also provid the redund air condit and the secur system","ordered_present_kp":[83,477,453,521,563,589],"keyphrases":["data centers","ABB","building infrastructure","redundant power supply","redundant air conditioning","security system","building management","mission-critical facilities","engineering management","project management","installation","commissioning"],"prmu":["P","P","P","P","P","P","M","R","U","U","U","U"]}
{"id":"2115","title":"Control of combustion processes in an internal combustion engine by low-temperature plasma","abstract":"A new method of operation of internal combustion engines enhances power and reduces fuel consumption and exhaust toxicity. Low-temperature plasma control combines working processes of thermal engines and steam machines into a single process","tok_text":"control of combust process in an intern combust engin by low-temperatur plasma \n a new method of oper of intern combust engin enhanc power and reduc fuel consumpt and exhaust toxic . low-temperatur plasma control combin work process of thermal engin and steam machin into a singl process","ordered_present_kp":[11,33,57,149,167,220,236,254],"keyphrases":["combustion processes","internal combustion engine","low-temperature plasma","fuel consumption","exhaust toxicity","working processes","thermal engines","steam machines"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"1999","title":"Performance comparison between PID and dead-time compensating controllers","abstract":"This paper is intended to answer the question: \"When can a simple dead-time compensator be expected to perform better than a PID?\". The performance criterion used is the integrated absolute error (IAE). It is compared for PI and PID controllers and a simple dead-time compensator (DTC) when a step load disturbance is applied at the plant input. Both stable and integrating processes are considered. For a fair comparison the controllers should provide equal robustness in some sense. Here, as a measure of robustness, the H\/sub infinity \/ norm of the sum of the absolute values of the sensitivity function and the complementary sensitivity function is used. Performance of the DTC's is given also as a function of dead-time margin (D\/sub M\/)","tok_text":"perform comparison between pid and dead-tim compens control \n thi paper is intend to answer the question : \" when can a simpl dead-tim compens be expect to perform better than a pid ? \" . the perform criterion use is the integr absolut error ( iae ) . it is compar for pi and pid control and a simpl dead-tim compens ( dtc ) when a step load disturb is appli at the plant input . both stabl and integr process are consid . for a fair comparison the control should provid equal robust in some sens . here , as a measur of robust , the h \/ sub infin \/ norm of the sum of the absolut valu of the sensit function and the complementari sensit function is use . perform of the dtc 's is given also as a function of dead-tim margin ( d \/ sub m\/ )","ordered_present_kp":[0,276,35,192,221,244,35,319,332,395,471,617,709],"keyphrases":["performance comparison","dead-time compensating controllers","dead-time compensator","performance criterion","integrated absolute error","IAE","PID controllers","DTC","step load disturbance","integrating processes","equal robustness","complementary sensitivity function","dead-time margin","PI controllers","stable processes","absolute value sum H\/sub infinity \/ norm"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"1964","title":"Antipersistent Markov behavior in foreign exchange markets","abstract":"A quantitative check of efficiency in US dollar\/Deutsche mark exchange rates is developed using high-frequency (tick by tick) data. The antipersistent Markov behavior of log-price fluctuations of given size implies, in principle, the possibility of a statistical forecast. We introduce and measure the available information of the quote sequence, and we show how it can be profitable following a particular trading rule","tok_text":"antipersist markov behavior in foreign exchang market \n a quantit check of effici in us dollar \/ deutsch mark exchang rate is develop use high-frequ ( tick by tick ) data . the antipersist markov behavior of log-pric fluctuat of given size impli , in principl , the possibl of a statist forecast . we introduc and measur the avail inform of the quot sequenc , and we show how it can be profit follow a particular trade rule","ordered_present_kp":[0,31,75,85,97,110,208,279,345,413,287],"keyphrases":["antipersistent Markov behavior","foreign exchange markets","efficiency","US dollar","Deutsche mark","exchange rates","log-price fluctuations","statistical forecast","forecasting","quote sequence","trading rule","high-frequency data","Shannon entropy"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","U"]}
{"id":"342","title":"Mount Sinai Hospital uses integer programming to allocate operating room time","abstract":"An integer-programming model and a post-solution heuristic allocates operating room time to the five surgical divisions at Toronto's Mount Sinai Hospital. The hospital has used this approach for several years and credits it with both administrative savings and the ability to produce quickly an equitable master surgical schedule","tok_text":"mount sinai hospit use integ program to alloc oper room time \n an integer-program model and a post-solut heurist alloc oper room time to the five surgic divis at toronto 's mount sinai hospit . the hospit ha use thi approach for sever year and credit it with both administr save and the abil to produc quickli an equit master surgic schedul","ordered_present_kp":[0,23,162,94],"keyphrases":["Mount Sinai Hospital","integer programming","post-solution heuristic","Toronto","operating room time allocation","Ontario","Canada"],"prmu":["P","P","P","P","R","U","U"]}
{"id":"307","title":"Computer program to generate operant schedules","abstract":"A computer program for programming schedules of reinforcement is described. Students can use the program to experience schedules of reinforcement that are typically used with nonhuman subjects. Accumulative recording of a student's response can be shown on the screen and\/or printed with the computer's printer. The program can also be used to program operant schedules for animal subjects. The program was tested with human subjects experiencing fixed ratio, variable ratio, fixed interval, and variable interval schedules. Performance for human subjects on a given schedule was similar to performance for nonhuman subjects on the same schedule","tok_text":"comput program to gener oper schedul \n a comput program for program schedul of reinforc is describ . student can use the program to experi schedul of reinforc that are typic use with nonhuman subject . accumul record of a student 's respons can be shown on the screen and\/or print with the comput 's printer . the program can also be use to program oper schedul for anim subject . the program wa test with human subject experienc fix ratio , variabl ratio , fix interv , and variabl interv schedul . perform for human subject on a given schedul wa similar to perform for nonhuman subject on the same schedul","ordered_present_kp":[0,183,366,475,186],"keyphrases":["computer program","nonhuman subjects","human subjects","animal subjects","variable interval schedules","operant schedule generation","reinforcement schedule programming","cumulative student response recording","fixed ratio schedules","variable ratio schedules","fixed interval schedules"],"prmu":["P","P","P","P","P","R","R","M","R","R","R"]}
{"id":"2008","title":"Building 3D anatomical scenes on the Web","abstract":"We propose a new service for building user-defined 3D anatomical structures on the Web. The Web server is connected to a database storing more than 1000 3D anatomical models reconstructed from the Visible Human. Users may combine existing models as well as planar oblique slices in order to create their own structured anatomical scenes. Furthermore, they may record sequences of scene construction and visualization actions. These actions enable the server to construct high-quality video animations, downloadable by the user. Professionals and students in anatomy, medicine and related disciplines are invited to use the server and create their own anatomical scenes","tok_text":"build 3d anatom scene on the web \n we propos a new servic for build user-defin 3d anatom structur on the web . the web server is connect to a databas store more than 1000 3d anatom model reconstruct from the visibl human . user may combin exist model as well as planar obliqu slice in order to creat their own structur anatom scene . furthermor , they may record sequenc of scene construct and visual action . these action enabl the server to construct high-qual video anim , download by the user . profession and student in anatomi , medicin and relat disciplin are invit to use the server and creat their own anatom scene","ordered_present_kp":[6,68,115,142,171,208,262,310,394,374,453],"keyphrases":["3D anatomical scenes","user-defined 3D anatomical structures","Web server","database","3D anatomical models","Visible Human","planar oblique slices","structured anatomical scenes","scene construction","visualization","high-quality video animation","World Wide Web","volume visualization","surface reconstruction","applet-based rendering engine","Java"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","M","M","M","U","U"]}
{"id":"2189","title":"Mobile computing \"Killer app\" competition","abstract":"Design competitions offer students an excellent way to gain hands-on experience in engineering and computer science courses. The University of Florida, in partnership with Motorola, has held two mobile computing design competitions. In Spring and Fall 2001, students in Abdelsalam Helal's Mobile Computing class designed killer apps for a Motorola smart phone","tok_text":"mobil comput \" killer app \" competit \n design competit offer student an excel way to gain hands-on experi in engin and comput scienc cours . the univers of florida , in partnership with motorola , ha held two mobil comput design competit . in spring and fall 2001 , student in abdelsalam helal 's mobil comput class design killer app for a motorola smart phone","ordered_present_kp":[0,349,186,39],"keyphrases":["mobile computing","design competitions","Motorola","smart phone"],"prmu":["P","P","P","P"]}
{"id":"1940","title":"New lower bounds of the size of error-correcting codes for the Z-channel","abstract":"Optimization problems on graphs are formulated to obtain new lower bounds of the size of error-correcting codes for the Z-channel","tok_text":"new lower bound of the size of error-correct code for the z-channel \n optim problem on graph are formul to obtain new lower bound of the size of error-correct code for the z-channel","ordered_present_kp":[4,31,58,70,87],"keyphrases":["lower bounds","error-correcting codes","Z-channel","optimization problems","graphs"],"prmu":["P","P","P","P","P"]}
{"id":"2174","title":"Spam solution?","abstract":"The author describes a solution to spam E-mails: disposable E-mail addresses (DEA). Mailshell's free trial Web-based E-mail service allows you, if you start getting spammed on that DEA, just to delete the DEA in Mailshell, and all E-mail thereafter sent to that address will automatically be junked (though you can later restore that address if you want). Mailshell allows any number of DEA","tok_text":"spam solut ? \n the author describ a solut to spam e-mail : dispos e-mail address ( dea ) . mailshel 's free trial web-bas e-mail servic allow you , if you start get spam on that dea , just to delet the dea in mailshel , and all e-mail thereaft sent to that address will automat be junk ( though you can later restor that address if you want ) . mailshel allow ani number of dea","ordered_present_kp":[45,59,91,114],"keyphrases":["spam E-mails","disposable E-mail addresses","Mailshell","Web-based E-mail"],"prmu":["P","P","P","P"]}
{"id":"2131","title":"Diagnosis of the technical state of heat systems","abstract":"A step-by-step approach to the diagnosis of the technical state of heat systems is stated. The class of physical defects is supplemented by the behavioral defects of objects, which are related to the disturbance of the modes of their operation. The implementation of the approach is illustrated by an example of the solution of a specific problem of the diagnosis of a closed heat consumption system","tok_text":"diagnosi of the technic state of heat system \n a step-by-step approach to the diagnosi of the technic state of heat system is state . the class of physic defect is supplement by the behavior defect of object , which are relat to the disturb of the mode of their oper . the implement of the approach is illustr by an exampl of the solut of a specif problem of the diagnosi of a close heat consumpt system","ordered_present_kp":[],"keyphrases":["heat system technical state diagnosis","step-by-step diagnosis","operational mode disturbance","closed heat consumption system diagnosis"],"prmu":["R","R","R","R"]}
{"id":"286","title":"Real-time tissue characterization on the basis of in vivo Raman spectra","abstract":"The application of in vivo Raman spectroscopy for clinical diagnosis demands dedicated software that can perform the necessary signal processing and subsequent (multivariate) data analysis, enabling clinically relevant parameters to be extracted and made available in real time. Here we describe the design and implementation of a software package that allows for real-time signal processing and data analysis of Raman spectra. The design is based on automatic data exchange between Grams, a spectroscopic data acquisition and analysis program, and Matlab, a program designed for array-based calculations. The data analysis software has a modular design providing great flexibility in developing custom data analysis routines for different applications. The implementation is illustrated by a computationally demanding application for the classification of skin spectra using principal component analysis and linear discriminant analysis","tok_text":"real-tim tissu character on the basi of in vivo raman spectra \n the applic of in vivo raman spectroscopi for clinic diagnosi demand dedic softwar that can perform the necessari signal process and subsequ ( multivari ) data analysi , enabl clinic relev paramet to be extract and made avail in real time . here we describ the design and implement of a softwar packag that allow for real-tim signal process and data analysi of raman spectra . the design is base on automat data exchang between gram , a spectroscop data acquisit and analysi program , and matlab , a program design for array-bas calcul . the data analysi softwar ha a modular design provid great flexibl in develop custom data analysi routin for differ applic . the implement is illustr by a comput demand applic for the classif of skin spectra use princip compon analysi and linear discrimin analysi","ordered_present_kp":[0,582,755,631,605,109,132,462,491,552,839],"keyphrases":["real-time tissue characterization","clinical diagnosis","dedicated software","automatic data exchange","Grams","Matlab","array-based calculations","data analysis software","modular design","computationally demanding application","linear discriminant analysis","clinically relevant parameters extraction","multivariate data analysis","skin spectra classification"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"28","title":"Uncertainty bounds and their use in the design of interval type-2 fuzzy logic systems","abstract":"We derive inner- and outer-bound sets for the type-reduced set of an interval type-2 fuzzy logic system (FLS), based on a new mathematical interpretation of the Karnik-Mendel iterative procedure for computing the type-reduced set. The bound sets can not only provide estimates about the uncertainty contained in the output of an interval type-2 FLS, but can also be used to design an interval type-2 FLS. We demonstrate, by means of a simulation experiment, that the resulting system can operate without type-reduction and can achieve similar performance to one that uses type-reduction. Therefore, our new design method, based on the bound sets, can relieve the computation burden of an interval type-2 FLS during its operation, which makes an interval type-2 FLS useful for real-time applications","tok_text":"uncertainti bound and their use in the design of interv type-2 fuzzi logic system \n we deriv inner- and outer-bound set for the type-reduc set of an interv type-2 fuzzi logic system ( fl ) , base on a new mathemat interpret of the karnik-mendel iter procedur for comput the type-reduc set . the bound set can not onli provid estim about the uncertainti contain in the output of an interv type-2 fl , but can also be use to design an interv type-2 fl . we demonstr , by mean of a simul experi , that the result system can oper without type-reduct and can achiev similar perform to one that use type-reduct . therefor , our new design method , base on the bound set , can reliev the comput burden of an interv type-2 fl dure it oper , which make an interv type-2 fl use for real-tim applic","ordered_present_kp":[0,49,104,128,231,772],"keyphrases":["uncertainty bounds","interval type-2 fuzzy logic systems","outer-bound sets","type-reduced set","Karnik-Mendel iterative procedure","real-time applications","inner-bound sets","time-series forecasting"],"prmu":["P","P","P","P","P","P","M","U"]}
{"id":"2069","title":"The ultimate control group","abstract":"Empirical research on the organization of firms requires that firms be classified on the basis of their control structures. This should be done in a way that can potentially be made operational. It is easy to identify the ultimate controller of a hierarchical organization, and the literature has largely focused on this case. However, many organizational structures mix hierarchy with collective choice procedures such as voting, or use circular structures under which superiors are accountable to their subordinates. The author develops some analytic machinery that can be used to map the authority structures of such organizations, and show that under mild restrictions there is a well-defined ultimate control group. The results are consistent with intuitions about the nature of control in familiar economic settings","tok_text":"the ultim control group \n empir research on the organ of firm requir that firm be classifi on the basi of their control structur . thi should be done in a way that can potenti be made oper . it is easi to identifi the ultim control of a hierarch organ , and the literatur ha larg focus on thi case . howev , mani organiz structur mix hierarchi with collect choic procedur such as vote , or use circular structur under which superior are account to their subordin . the author develop some analyt machineri that can be use to map the author structur of such organ , and show that under mild restrict there is a well-defin ultim control group . the result are consist with intuit about the natur of control in familiar econom set","ordered_present_kp":[4,237,313,533],"keyphrases":["ultimate control group","hierarchical organization","organizational structures","authority structures","committees","control rights","firm organization"],"prmu":["P","P","P","P","U","M","R"]}
{"id":"366","title":"Selecting rail grade crossing investments with a decision support system","abstract":"The Federal Railroad Administration (FRA) has developed a series of rail and rail-related analysis tools that assist FRA officials, Metropolitan Planning Organizations (MPOs), state Department of Transportation (DOT), and other constituents in evaluating the cost and benefits of potential infrastructure projects. To meet agency objectives, the FRA wants to add a high-speed rail grade crossing analysis tool to its package of rail and rail-related intermodal software products. This paper presents a conceptual decision support system (DSS) that can assist officials in achieving this goal. The paper first introduces the FRA's objectives and the role of cost benefit analysis in achieving these objectives. Next, there is a discussion of the models needed to assess the feasibility of proposed high-speed rail grade crossing investments and the presentation of a decision support system (DSS) that can deliver these models transparently to users. Then, the paper illustrates a system session and examines the potential benefits from system use","tok_text":"select rail grade cross invest with a decis support system \n the feder railroad administr ( fra ) ha develop a seri of rail and rail-rel analysi tool that assist fra offici , metropolitan plan organ ( mpo ) , state depart of transport ( dot ) , and other constitu in evalu the cost and benefit of potenti infrastructur project . to meet agenc object , the fra want to add a high-spe rail grade cross analysi tool to it packag of rail and rail-rel intermod softwar product . thi paper present a conceptu decis support system ( dss ) that can assist offici in achiev thi goal . the paper first introduc the fra 's object and the role of cost benefit analysi in achiev these object . next , there is a discuss of the model need to assess the feasibl of propos high-spe rail grade cross invest and the present of a decis support system ( dss ) that can deliv these model transpar to user . then , the paper illustr a system session and examin the potenti benefit from system use","ordered_present_kp":[38,65,175,215,305,374,438,635],"keyphrases":["decision support system","Federal Railroad Administration","Metropolitan Planning Organizations","Department of Transportation","infrastructure projects","high-speed rail grade crossing analysis tool","rail-related intermodal software products","cost benefit analysis","rail grade crossing investment selection","rail intermodal software products"],"prmu":["P","P","P","P","P","P","P","P","R","R"]}
{"id":"2094","title":"Statistical inference with partial prior information based on a Gauss-type inequality","abstract":"Potter and Anderson (1983) have developed a Bayesian decision procedure requiring the specification of a class of prior distributions restricted to have a minimal probability content for a given subset of the parameter space. They do not, however, provide a method for the selection of that subset. We show how a generalization of Gauss' inequality can be used to determine the relevant parameter subset","tok_text":"statist infer with partial prior inform base on a gauss-typ inequ \n potter and anderson ( 1983 ) have develop a bayesian decis procedur requir the specif of a class of prior distribut restrict to have a minim probabl content for a given subset of the paramet space . they do not , howev , provid a method for the select of that subset . we show how a gener of gauss ' inequ can be use to determin the relev paramet subset","ordered_present_kp":[112,168,203,251,19],"keyphrases":["partial prior information","Bayesian decision procedure","prior distributions","minimal probability content","parameter space","Gauss inequality","prior-to-posterior sensitivity"],"prmu":["P","P","P","P","P","R","U"]}
{"id":"323","title":"A server-side program for delivering experiments with animations","abstract":"A server-side program for animation experiments is presented. The program is capable of delivering an experiment composed of discrete animation sequences in various file formats, collecting a discrete or continuous response from the observer, evaluating the appropriateness of the response, and ensuring that the user is not proceeding at an unreasonable rate. Most parameters of the program are controllable by experimenter-edited text files or simple switches in the program code, thereby minimizing the need for programming to create new experiments. A simple demonstration experiment is discussed and is freely available","tok_text":"a server-sid program for deliv experi with anim \n a server-sid program for anim experi is present . the program is capabl of deliv an experi compos of discret anim sequenc in variou file format , collect a discret or continu respons from the observ , evalu the appropri of the respons , and ensur that the user is not proceed at an unreason rate . most paramet of the program are control by experimenter-edit text file or simpl switch in the program code , therebi minim the need for program to creat new experi . a simpl demonstr experi is discuss and is freeli avail","ordered_present_kp":[2,151,182,391],"keyphrases":["server-side program","discrete animation sequences","file formats","experimenter-edited text files","animation experiment delivery","Web based psychological experiments","Internet"],"prmu":["P","P","P","P","M","M","U"]}
{"id":"2011","title":"Innovative phase unwrapping algorithm: hybrid approach","abstract":"We present a novel algorithm based on a hybrid of the global and local treatment of a wrapped map. The proposed algorithm is especially effective for the unwrapping of speckle-coded interferogram contour maps. In contrast to earlier unwrapping algorithms by region, we propose a local discontinuity-restoring criterion to serve as the preprocessor or postprocessor of our hybrid algorithm, which makes the unwrapping by region much easier and more efficient. With this hybrid algorithm, a robust, stable, and especially time effective phase unwrapping can be achieved. Additionally, the criterion and limitation of this hybrid algorithm are fully described. The robustness, stability, and speed of this hybrid algorithm are also studied. The proposed algorithm can be easily upgraded with minor modifications to solve the unwrapping problem of maps with phase inconsistency. Both numerical simulation and experimental applications demonstrate the effectiveness of the proposed algorithm","tok_text":"innov phase unwrap algorithm : hybrid approach \n we present a novel algorithm base on a hybrid of the global and local treatment of a wrap map . the propos algorithm is especi effect for the unwrap of speckle-cod interferogram contour map . in contrast to earlier unwrap algorithm by region , we propos a local discontinuity-restor criterion to serv as the preprocessor or postprocessor of our hybrid algorithm , which make the unwrap by region much easier and more effici . with thi hybrid algorithm , a robust , stabl , and especi time effect phase unwrap can be achiev . addit , the criterion and limit of thi hybrid algorithm are fulli describ . the robust , stabil , and speed of thi hybrid algorithm are also studi . the propos algorithm can be easili upgrad with minor modif to solv the unwrap problem of map with phase inconsist . both numer simul and experiment applic demonstr the effect of the propos algorithm","ordered_present_kp":[6,113,134,201,12,305,373,394,794,821,844],"keyphrases":["phase unwrapping algorithm","unwrapping algorithms","local treatment","wrapped map","speckle-coded interferogram contour maps","local discontinuity-restoring criterion","postprocessor","hybrid algorithm","unwrapping problem","phase inconsistency","numerical simulation","global treatment","robust stable time effective phase unwrapping","interferogram analysis","light interferometry"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","M","U"]}
{"id":"2054","title":"Teaching modeling in management science","abstract":"This essay discusses how we can most effectively teach Management Science to students in MBA or similar programs who will be, at best, part-time practitioners of these arts. I take as a working hypothesis the radical proposition that the heart of Management Science itself is not the impressive array of tools that have been built up over the years (optimization, simulation, decision analysis, queuing, and so on) but rather the art of reasoning logically with formal models. I believe it is necessary with this group of students to teach basic modeling skills, and in fact it is only when such students have these basic skills as a foundation that they are prepared to acquire the more sophisticated skills needed to employ Management Science. In this paper I present a hierarchy of modeling skills, from numeracy skills through sophisticated Management Science skills, as a framework within which to plan courses for the occasional practitioner","tok_text":"teach model in manag scienc \n thi essay discuss how we can most effect teach manag scienc to student in mba or similar program who will be , at best , part-tim practition of these art . i take as a work hypothesi the radic proposit that the heart of manag scienc itself is not the impress array of tool that have been built up over the year ( optim , simul , decis analysi , queu , and so on ) but rather the art of reason logic with formal model . i believ it is necessari with thi group of student to teach basic model skill , and in fact it is onli when such student have these basic skill as a foundat that they are prepar to acquir the more sophist skill need to employ manag scienc . in thi paper i present a hierarchi of model skill , from numeraci skill through sophist manag scienc skill , as a framework within which to plan cours for the occasion practition","ordered_present_kp":[15,6,747,434,359],"keyphrases":["modeling","management science","decision analysis","formal models","numeracy skills"],"prmu":["P","P","P","P","P"]}
{"id":"1980","title":"Lossy to lossless object-based coding of 3-D MRI data","abstract":"We propose a fully three-dimensional (3-D) object-based coding system exploiting the diagnostic relevance of the different regions of the volumetric data for rate allocation. The data are first decorrelated via a 3-D discrete wavelet transform. The implementation via the lifting steps scheme allows to map integer-to-integer values, enabling lossless coding, and facilitates the definition of the object-based inverse transform. The coding process assigns disjoint segments of the bitstream to the different objects, which can be independently accessed and reconstructed at any up-to-lossless quality. Two fully 3-D coding strategies are considered: embedded zerotree coding (EZW-3D) and multidimensional layered zero coding (MLZC), both generalized for region of interest (ROI)-based processing. In order to avoid artifacts along region boundaries, some extra coefficients must be encoded for each object. This gives rise to an overheading of the bitstream with respect to the case where the volume is encoded as a whole. The amount of such extra information depends on both the filter length and the decomposition depth. The system is characterized on a set of head magnetic resonance images. Results show that MLZC and EZW-3D have competitive performances. In particular, the best MLZC mode outperforms the others state-of-the-art techniques on one of the datasets for which results are available in the literature","tok_text":"lossi to lossless object-bas code of 3-d mri data \n we propos a fulli three-dimension ( 3-d ) object-bas code system exploit the diagnost relev of the differ region of the volumetr data for rate alloc . the data are first decorrel via a 3-d discret wavelet transform . the implement via the lift step scheme allow to map integer-to-integ valu , enabl lossless code , and facilit the definit of the object-bas invers transform . the code process assign disjoint segment of the bitstream to the differ object , which can be independ access and reconstruct at ani up-to-lossless qualiti . two fulli 3-d code strategi are consid : embed zerotre code ( ezw-3d ) and multidimension layer zero code ( mlzc ) , both gener for region of interest ( roi)-bas process . in order to avoid artifact along region boundari , some extra coeffici must be encod for each object . thi give rise to an overhead of the bitstream with respect to the case where the volum is encod as a whole . the amount of such extra inform depend on both the filter length and the decomposit depth . the system is character on a set of head magnet reson imag . result show that mlzc and ezw-3d have competit perform . in particular , the best mlzc mode outperform the other state-of-the-art techniqu on one of the dataset for which result are avail in the literatur","ordered_present_kp":[9,37,129,172,190,237,291,321,398,452,476,627,648,661,791,1021,1043,1098],"keyphrases":["lossless object-based coding","3-D MRI data","diagnostic relevance","volumetric data","rate allocation","3-D discrete wavelet transform","lifting steps scheme","integer-to-integer values","object-based inverse transform","disjoint segments","bitstream","embedded zerotree coding","EZW-3D","multidimensional layered zero coding","region boundaries","filter length","decomposition depth","head magnetic resonance images","lossy object-based coding","NMZQ","region of interest-based processing","ROI-based processing"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","U","M","M"]}
{"id":"2149","title":"Security crisis management - the basics","abstract":"Of the more pervasive problems in any kind of security event is how the security event is managed from the inception to the end. There's a lot written about how to manage a specific incident or how to deal with a point problem such as a firewall log, but little tends to be written about how to deal with the management of a security event as part of corporate crisis management. This article discusses the basics of security crisis management and of the logical steps required to ensure that a security crisis does not get out of hand","tok_text":"secur crisi manag - the basic \n of the more pervas problem in ani kind of secur event is how the secur event is manag from the incept to the end . there 's a lot written about how to manag a specif incid or how to deal with a point problem such as a firewal log , but littl tend to be written about how to deal with the manag of a secur event as part of corpor crisi manag . thi articl discuss the basic of secur crisi manag and of the logic step requir to ensur that a secur crisi doe not get out of hand","ordered_present_kp":[0,74,250,354],"keyphrases":["security crisis management","security event","firewall log","corporate crisis management"],"prmu":["P","P","P","P"]}
{"id":"203","title":"Plenoptic image editing","abstract":"This paper presents a new class of interactive image editing operations designed to maintain consistency between multiple images of a physical 3D scene. The distinguishing feature of these operations is that edits to any one image propagate automatically to all other images as if the (unknown) 3D scene had itself been modified. The modified scene can then be viewed interactively from any other camera viewpoint and under different scene illuminations. The approach is useful first as a power-assist that enables a user to quickly modify many images by editing just a few, and second as a means for constructing and editing image-based scene representations by manipulating a set of photographs. The approach works by extending operations like image painting, scissoring, and morphing so that they alter a scene's plenoptic function in a physically-consistent way, thereby affecting scene appearance from all viewpoints simultaneously. A key element in realizing these operations is a new volumetric decomposition technique for reconstructing an scene's plenoptic function from an incomplete set of camera viewpoints","tok_text":"plenopt imag edit \n thi paper present a new class of interact imag edit oper design to maintain consist between multipl imag of a physic 3d scene . the distinguish featur of these oper is that edit to ani one imag propag automat to all other imag as if the ( unknown ) 3d scene had itself been modifi . the modifi scene can then be view interact from ani other camera viewpoint and under differ scene illumin . the approach is use first as a power-assist that enabl a user to quickli modifi mani imag by edit just a few , and second as a mean for construct and edit image-bas scene represent by manipul a set of photograph . the approach work by extend oper like imag paint , scissor , and morph so that they alter a scene 's plenopt function in a physically-consist way , therebi affect scene appear from all viewpoint simultan . a key element in realiz these oper is a new volumetr decomposit techniqu for reconstruct an scene 's plenopt function from an incomplet set of camera viewpoint","ordered_present_kp":[53,112,130,307,361,566,663,676,690,726,875,0],"keyphrases":["plenoptic image editing","interactive image editing operations","multiple images","physical 3D scene","modified scene","camera viewpoint","image-based scene representations","image painting","scissoring","morphing","plenoptic function","volumetric decomposition technique"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"246","title":"Adaptive and efficient mutual exclusion","abstract":"The paper presents adaptive algorithms for mutual exclusion using only read and write operations; the performance of the algorithms depends only on the point contention, i.e., the number of processes that are concurrently active during algorithm execution (and not on n, the total number of processes). Our algorithm has O(k) remote step complexity and O(log k) system response time, where k is the point contention. The remote step complexity is the maximal number of steps performed by a process where a wait is counted as one step. The system response time is the time interval between subsequent entries to the critical section, where one time unit is the minimal interval in which every active process performs at least one step. The space complexity of this algorithm is O(N log n), where N is the range of process names. We show how to make the space complexity of our algorithm depend solely on n, while preserving the other performance measures of the algorithm","tok_text":"adapt and effici mutual exclus \n the paper present adapt algorithm for mutual exclus use onli read and write oper ; the perform of the algorithm depend onli on the point content , i.e. , the number of process that are concurr activ dure algorithm execut ( and not on n , the total number of process ) . our algorithm ha o(k ) remot step complex and o(log k ) system respons time , where k is the point content . the remot step complex is the maxim number of step perform by a process where a wait is count as one step . the system respons time is the time interv between subsequ entri to the critic section , where one time unit is the minim interv in which everi activ process perform at least one step . the space complex of thi algorithm is o(n log n ) , where n is the rang of process name . we show how to make the space complex of our algorithm depend sole on n , while preserv the other perform measur of the algorithm","ordered_present_kp":[51,103,164,237,326,359,592,636,664,710,894],"keyphrases":["adaptive algorithms","write operations","point contention","algorithm execution","remote step complexity","system response time","critical section","minimal interval","active process","space complexity","performance measures","adaptive mutual exclusion","read operations"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"1938","title":"A method for solution of systems of linear algebraic equations with m-dimensional lambda -matrices","abstract":"A system of linear algebraic equations with m-dimensional lambda -matrices is considered. The proposed method of searching for the solution of this system lies in reducing it to a numerical system of a special kind","tok_text":"a method for solut of system of linear algebra equat with m-dimension lambda -matric \n a system of linear algebra equat with m-dimension lambda -matric is consid . the propos method of search for the solut of thi system lie in reduc it to a numer system of a special kind","ordered_present_kp":[32,241,58],"keyphrases":["linear algebraic equations","m-dimensional lambda -matrices","numerical system"],"prmu":["P","P","P"]}
{"id":"2032","title":"Adaptive digital watermarking using fuzzy logic techniques","abstract":"Digital watermarking has been proposed for copyright protection in our digital society. We propose an adaptive digital watermarking scheme based on the human visual system model and a fuzzy logic technique. The fuzzy logic approach is employed to obtain the different strengths and lengths of a watermark by the local characteristics of the image in our proposed scheme. In our experiments, this scheme provides a more robust and imperceptible watermark","tok_text":"adapt digit watermark use fuzzi logic techniqu \n digit watermark ha been propos for copyright protect in our digit societi . we propos an adapt digit watermark scheme base on the human visual system model and a fuzzi logic techniqu . the fuzzi logic approach is employ to obtain the differ strength and length of a watermark by the local characterist of the imag in our propos scheme . in our experi , thi scheme provid a more robust and impercept watermark","ordered_present_kp":[0,26,84,109,179,332,438],"keyphrases":["adaptive digital watermarking","fuzzy logic techniques","copyright protection","digital society","human visual system model","local characteristics","imperceptible watermark","robust watermark","image processing"],"prmu":["P","P","P","P","P","P","P","R","M"]}
{"id":"385","title":"Multiple model adaptive estimation with filter spawning","abstract":"Multiple model adaptive estimation (MMAE) with filter spawning is used to detect and estimate partial actuator failures on the VISTA F-16. The truth model is a full six-degree-of-freedom simulation provided by Calspan and General Dynamics. The design models are chosen as 13-state linearized models, including first order actuator models. Actuator failures are incorporated into the truth model and design model assuming a \"failure to free stream.\" Filter spawning is used to include additional filters with partial actuator failure hypotheses into the MMAE bank. The spawned filters are based on varying degrees of partial failures (in terms of effectiveness) associated with the complete-actuaton-failure hypothesis with the highest conditional probability of correctness at the current time. Thus, a blended estimate of the failure effectiveness is found using the filters' estimates based upon a no-failure hypothesis, a complete actuator failure hypothesis, and the spawned filters' partial-failure hypotheses. This yields substantial precision in effectiveness estimation, compared with what is possible without spawning additional filters, making partial failure adaptation a viable methodology","tok_text":"multipl model adapt estim with filter spawn \n multipl model adapt estim ( mmae ) with filter spawn is use to detect and estim partial actuat failur on the vista f-16 . the truth model is a full six-degree-of-freedom simul provid by calspan and gener dynam . the design model are chosen as 13-state linear model , includ first order actuat model . actuat failur are incorpor into the truth model and design model assum a \" failur to free stream . \" filter spawn is use to includ addit filter with partial actuat failur hypothes into the mmae bank . the spawn filter are base on vari degre of partial failur ( in term of effect ) associ with the complete-actuaton-failur hypothesi with the highest condit probabl of correct at the current time . thu , a blend estim of the failur effect is found use the filter ' estim base upon a no-failur hypothesi , a complet actuat failur hypothesi , and the spawn filter ' partial-failur hypothes . thi yield substanti precis in effect estim , compar with what is possibl without spawn addit filter , make partial failur adapt a viabl methodolog","ordered_present_kp":[0,31,126,155,172,194,232,244,298,74,591,696,829],"keyphrases":["multiple model adaptive estimation","filter spawning","MMAE","partial actuator failures","VISTA F-16","truth model","six-degree-of-freedom simulation","Calspan","General Dynamics","linearized models","partial failures","conditional probability","no-failure hypothesis","in-flight simulator","test aircraft","flight control systems"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","M","U","U"]}
{"id":"378","title":"Incremental motion control of linear synchronous motor","abstract":"In this study a particular incremental motion control problem, which is specified by the trapezoidal velocity profile using multisegment sliding mode control (MSSMC), is proposed to control a permanent magnet linear synchronous motor (PMLSM) servo drive system. First, the structure and operating principle of the PMLSM are described in detail. Second, a field-oriented control PMLSM servo drive is introduced. Then, each segment of the multisegment switching surfaces is designed to match the corresponding part of the trapezoidal velocity profile, thus the motor dynamics on the specified-segment switching surface have the desired velocity or acceleration corresponding part of the trapezoidal velocity profile. In addition, the proposed control system is implemented in a PC-based computer control system. Finally, the effectiveness of the proposed PMLSM servo drive system is demonstrated by some simulated and experimental results","tok_text":"increment motion control of linear synchron motor \n in thi studi a particular increment motion control problem , which is specifi by the trapezoid veloc profil use multiseg slide mode control ( mssmc ) , is propos to control a perman magnet linear synchron motor ( pmlsm ) servo drive system . first , the structur and oper principl of the pmlsm are describ in detail . second , a field-ori control pmlsm servo drive is introduc . then , each segment of the multiseg switch surfac is design to match the correspond part of the trapezoid veloc profil , thu the motor dynam on the specified-seg switch surfac have the desir veloc or acceler correspond part of the trapezoid veloc profil . in addit , the propos control system is implement in a pc-base comput control system . final , the effect of the propos pmlsm servo drive system is demonstr by some simul and experiment result","ordered_present_kp":[0,28,137,164,273,381,458,560],"keyphrases":["incremental motion control","linear synchronous motor","trapezoidal velocity profile","multisegment sliding mode control","servo drive system","field-oriented control","multisegment switching surfaces","motor dynamics","permanent magnet motor"],"prmu":["P","P","P","P","P","P","P","P","R"]}
{"id":"410","title":"Lossy SPICE models produce realistic averaged simulations","abstract":"In previous averaged models, the state-space averaging technique or switch waveforms analysis were usually applied over perfect elements, non-inclusive of the ohmic losses. However, if these elements play an active role in the DC transfer function, they affect the small-signal AC analysis by introducing various damping effects. A model is introduced in a boost voltage-mode application","tok_text":"lossi spice model produc realist averag simul \n in previou averag model , the state-spac averag techniqu or switch waveform analysi were usual appli over perfect element , non-inclus of the ohmic loss . howev , if these element play an activ role in the dc transfer function , they affect the small-sign ac analysi by introduc variou damp effect . a model is introduc in a boost voltage-mod applic","ordered_present_kp":[0,25,78,108,334,373,190,254],"keyphrases":["lossy SPICE models","realistic averaged simulations","state-space averaging technique","switch waveforms analysis","ohmic losses","DC transfer function","damping effects","boost voltage-mode application"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"265","title":"Pattern recognition strategies for molecular surfaces. II. Surface complementarity","abstract":"For pt.I see ibid., vol.23, p.1176-87 (2002). Fuzzy logic based algorithms for the quantitative treatment of complementarity of molecular surfaces are presented. Therein, the overlapping surface patches defined in part I of this series are used. The identification of complementary surface patches can be considered as a first step for the solution of molecular docking problems. Standard technologies can then be used for further optimization of the resulting complex structures. The algorithms are applied to 33 biomolecular complexes. After the optimization with a downhill simplex method, for all these complexes one structure was found, which is in very good agreement with the experimental results","tok_text":"pattern recognit strategi for molecular surfac . ii . surfac complementar \n for pt . i see ibid . , vol.23 , p.1176 - 87 ( 2002 ) . fuzzi logic base algorithm for the quantit treatment of complementar of molecular surfac are present . therein , the overlap surfac patch defin in part i of thi seri are use . the identif of complementari surfac patch can be consid as a first step for the solut of molecular dock problem . standard technolog can then be use for further optim of the result complex structur . the algorithm are appli to 33 biomolecular complex . after the optim with a downhil simplex method , for all these complex one structur wa found , which is in veri good agreement with the experiment result","ordered_present_kp":[0,54,132,167,30,249,538,469,584],"keyphrases":["pattern recognition strategies","molecular surfaces","surface complementarity","fuzzy logic based algorithms","quantitative treatment","overlapping surface","optimization","biomolecular complexes","downhill simplex method"],"prmu":["P","P","P","P","P","P","P","P","P"]}
{"id":"2197","title":"A context-aware decision engine for content adaptation","abstract":"Building a good content adaptation service for mobile devices poses many challenges. To meet these challenges, this quality-of-service-aware decision engine automatically negotiates for the appropriate adaptation decision for synthesizing an optimal content version","tok_text":"a context-awar decis engin for content adapt \n build a good content adapt servic for mobil devic pose mani challeng . to meet these challeng , thi quality-of-service-awar decis engin automat negoti for the appropri adapt decis for synthes an optim content version","ordered_present_kp":[31,85,147,15,242,215],"keyphrases":["decision engine","content adaptation","mobile devices","quality-of-service-aware","adaptation decision","optimal content version"],"prmu":["P","P","P","P","P","P"]}
{"id":"220","title":"How to avoid merger pitfalls","abstract":"Paul Diamond of consultancy KPMG explains why careful IT asset management is crucial to the success of mergers","tok_text":"how to avoid merger pitfal \n paul diamond of consult kpmg explain whi care it asset manag is crucial to the success of merger","ordered_present_kp":[45,53,75,13],"keyphrases":["mergers","consultancy","KPMG","IT asset management"],"prmu":["P","P","P","P"]}
{"id":"199","title":"On optimality in auditory information processing","abstract":"We study limits for the detection and estimation of weak sinusoidal signals in the primary part of the mammalian auditory system using a stochastic Fitzhugh-Nagumo model and an action-recovery model for synaptic depression. Our overall model covers the chain from a hair cell to a point just after the synaptic connection with a cell in the cochlear nucleus. The information processing performance of the system is evaluated using so-called phi -divergences from statistics that quantify \"dissimilarity\" between probability measures and are intimately related to a number of fundamental limits in statistics and information theory (IT). We show that there exists a set of parameters that can optimize several important phi -divergences simultaneously and that this set corresponds to a constant quiescent firing rate (QFR) of the spiral ganglion neuron. The optimal value of the QFR is frequency dependent but is essentially independent of the amplitude of the signal (for small amplitudes). Consequently, optimal processing according to several standard IT criteria can be accomplished for this model if and only if the parameters are \"tuned\" to values that correspond to one and the same QFR. This offers a new explanation for the QFR and can provide new insight into the role played by several other parameters of the peripheral auditory system","tok_text":"on optim in auditori inform process \n we studi limit for the detect and estim of weak sinusoid signal in the primari part of the mammalian auditori system use a stochast fitzhugh-nagumo model and an action-recoveri model for synapt depress . our overal model cover the chain from a hair cell to a point just after the synapt connect with a cell in the cochlear nucleu . the inform process perform of the system is evalu use so-cal phi -diverg from statist that quantifi \" dissimilar \" between probabl measur and are intim relat to a number of fundament limit in statist and inform theori ( it ) . we show that there exist a set of paramet that can optim sever import phi -diverg simultan and that thi set correspond to a constant quiescent fire rate ( qfr ) of the spiral ganglion neuron . the optim valu of the qfr is frequenc depend but is essenti independ of the amplitud of the signal ( for small amplitud ) . consequ , optim process accord to sever standard it criteria can be accomplish for thi model if and onli if the paramet are \" tune \" to valu that correspond to one and the same qfr . thi offer a new explan for the qfr and can provid new insight into the role play by sever other paramet of the peripher auditori system","ordered_present_kp":[81,129,161,199,1208,730,765],"keyphrases":["weak sinusoidal signals","mammalian auditory system","stochastic Fitzhugh-Nagumo model","action-recovery model","quiescent firing rate","spiral ganglion neuron","peripheral auditory system","brain"],"prmu":["P","P","P","P","P","P","P","U"]}
{"id":"298","title":"Defining electronic librarianship: a content analysis of job advertisements","abstract":"Advances in technology create dramatic changes within libraries. The complex issues surrounding this new electronic, end-user environment have major ramifications and require expert knowledge. Electronic services librarians and electronic resources librarians are two specialized titles that have recently emerged within the field of librarianship to fill this niche. Job advertisements listed in American Libraries from January 1989 to December 1998 were examined to identify responsibilities, qualifications, organizational and salary information relating to the newly emerging role of electronic librarian","tok_text":"defin electron librarianship : a content analysi of job advertis \n advanc in technolog creat dramat chang within librari . the complex issu surround thi new electron , end-us environ have major ramif and requir expert knowledg . electron servic librarian and electron resourc librarian are two special titl that have recent emerg within the field of librarianship to fill thi nich . job advertis list in american librari from januari 1989 to decemb 1998 were examin to identifi respons , qualif , organiz and salari inform relat to the newli emerg role of electron librarian","ordered_present_kp":[6,33,52,259,229,404,478,488,509],"keyphrases":["electronic librarianship","content analysis","job advertisements","electronic services librarians","electronic resources librarians","American Libraries","responsibilities","qualifications","salary information","electronic end-user environment","organizational information"],"prmu":["P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"36","title":"Model predictive control helps to regulate slow processes-robust barrel temperature control","abstract":"Slow temperature control is a challenging control problem. The problem becomes even more challenging when multiple zones are involved, such as in barrel temperature control for extruders. Often, strict closed-loop performance requirements (such as fast startup with no overshoot and maintaining tight temperature control during production) are given for such applications. When characteristics of the system are examined, it becomes clear that a commonly used proportional plus integral plus derivative (PID) controller cannot meet such performance specifications for this kind of system. The system either will overshoot or not maintain the temperature within the specified range during the production run. In order to achieve the required performance, a control strategy that utilizes techniques such as model predictive control, autotuning, and multiple parameter PID is formulated. This control strategy proves to be very effective in achieving the desired specifications, and is very robust","tok_text":"model predict control help to regul slow processes-robust barrel temperatur control \n slow temperatur control is a challeng control problem . the problem becom even more challeng when multipl zone are involv , such as in barrel temperatur control for extrud . often , strict closed-loop perform requir ( such as fast startup with no overshoot and maintain tight temperatur control dure product ) are given for such applic . when characterist of the system are examin , it becom clear that a commonli use proport plu integr plu deriv ( pid ) control can not meet such perform specif for thi kind of system . the system either will overshoot or not maintain the temperatur within the specifi rang dure the product run . in order to achiev the requir perform , a control strategi that util techniqu such as model predict control , autotun , and multipl paramet pid is formul . thi control strategi prove to be veri effect in achiev the desir specif , and is veri robust","ordered_present_kp":[0,251,828,842],"keyphrases":["model predictive control","extruders","autotuning","multiple parameter PID","slow processes regulation","robust barrel temperature control"],"prmu":["P","P","P","P","M","R"]}
{"id":"2112","title":"Allan variance and fractal Brownian motion","abstract":"Noise filtering is the subject of a voluminous literature in radio engineering. The methods of filtering require knowledge of the frequency response, which is usually unknown. D.W. Allan (see Proc. IEEE, vol.54, no.2, p.221-30, 1966; IEEE Trans. Instr. Measur., vol.IM-36, p.646-54, 1987) proposed a simple method of determining the interval between equally accurate observations which does without this information. In this method, the variances of the increments of noise and signal are equal, so that, in observations with a greater step, the variations caused by noise are smaller than those caused by the signal. This method is the standard accepted by the USA metrology community. The present paper is devoted to a statistical analysis of the Allan method and acquisition of additional information","tok_text":"allan varianc and fractal brownian motion \n nois filter is the subject of a volumin literatur in radio engin . the method of filter requir knowledg of the frequenc respons , which is usual unknown . d.w. allan ( see proc . ieee , vol.54 , no.2 , p.221 - 30 , 1966 ; ieee tran . instr . measur . , vol . im-36 , p.646 - 54 , 1987 ) propos a simpl method of determin the interv between equal accur observ which doe without thi inform . in thi method , the varianc of the increment of nois and signal are equal , so that , in observ with a greater step , the variat caus by nois are smaller than those caus by the signal . thi method is the standard accept by the usa metrolog commun . the present paper is devot to a statist analysi of the allan method and acquisit of addit inform","ordered_present_kp":[0,18,44,97,155,661,715],"keyphrases":["Allan variance","fractal Brownian motion","noise filtering","radio engineering","frequency response","USA metrology community","statistical analysis","white noise"],"prmu":["P","P","P","P","P","P","P","M"]}
{"id":"2157","title":"Shaping the future. BendWizard: a tool for off-line programming of robotic tending systems","abstract":"Setting up a robot to make metal cabinets or cases for desktop computers can be a complex operation. For instance, one expert might be required to carry out a feasibility study, and then another to actually program the robot. Understandably, the need for so much expertise, and the time that's required, generally limits the usefulness of automation to high-volume production. Workshops producing parts in batches smaller than 50 or so, or which rely heavily on semiskilled operators, are therefore often discouraged from investing in automation, and so miss out on its many advantages. What is needed is a software tool that operators without special knowledge of robotics, or with no more than rudimentary CAD skills, can use. One which allows easy offline programming and simulation of the work cell on a PC","tok_text":"shape the futur . bendwizard : a tool for off-lin program of robot tend system \n set up a robot to make metal cabinet or case for desktop comput can be a complex oper . for instanc , one expert might be requir to carri out a feasibl studi , and then anoth to actual program the robot . understand , the need for so much expertis , and the time that 's requir , gener limit the use of autom to high-volum product . workshop produc part in batch smaller than 50 or so , or which reli heavili on semiskil oper , are therefor often discourag from invest in autom , and so miss out on it mani advantag . what is need is a softwar tool that oper without special knowledg of robot , or with no more than rudimentari cad skill , can use . one which allow easi offlin program and simul of the work cell on a pc","ordered_present_kp":[61,104,225,393,414,709],"keyphrases":["robotic tending systems","metal cabinets","feasibility study","high-volume production","workshops","CAD skills","BendWizard offline programming tool","desktop computer cases","work cell simulation"],"prmu":["P","P","P","P","P","P","R","R","R"]}
{"id":"258","title":"Building digital collections at the OAC: current strategies with a view to future uses","abstract":"Providing a context for the exploration of user defined virtual collections, the article describes the history and recent development of the Online Archive of California (OAC). Stating that usability and user needs are primary factors in digital resource development, issues explored include collaborations to build digital collections, reliance upon professional standards for description and encoding, system architecture, interface design, the need for user tools, and the role of archivists as interpreters in the digital environment","tok_text":"build digit collect at the oac : current strategi with a view to futur use \n provid a context for the explor of user defin virtual collect , the articl describ the histori and recent develop of the onlin archiv of california ( oac ) . state that usabl and user need are primari factor in digit resourc develop , issu explor includ collabor to build digit collect , relianc upon profession standard for descript and encod , system architectur , interfac design , the need for user tool , and the role of archivist as interpret in the digit environ","ordered_present_kp":[6,27,65,112,164,198,256,288,378,423,444,475,533],"keyphrases":["digital collections","OAC","future uses","user defined virtual collections","history","Online Archive of California","user needs","digital resource","professional standards","system architecture","interface design","user tools","digital environment","Encoded Archival Description","archival descriptive standards","metadata standards","best practices","user studies"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","M","U","M"]}
{"id":"1963","title":"The variance of firm growth rates: the 'scaling' puzzle","abstract":"Recent evidence suggests that a power-law relationship exists between a firm's size and the variance of its growth rate. The flatness of the relation is regarded as puzzling, in that it suggests that large firms are not much more stable than small firms. It has been suggested that the powerlaw nature of the relationship reflects the presence of some form of correlation of growth rates across the firm's constituent businesses. Here, it is shown that a model of independent businesses which allows for the fact that these businesses vary in size, as modelled by a simple 'partitions of integers' model, provides a good representation of what is observed empirically","tok_text":"the varianc of firm growth rate : the ' scale ' puzzl \n recent evid suggest that a power-law relationship exist between a firm 's size and the varianc of it growth rate . the flat of the relat is regard as puzzl , in that it suggest that larg firm are not much more stabl than small firm . it ha been suggest that the powerlaw natur of the relationship reflect the presenc of some form of correl of growth rate across the firm 's constitu busi . here , it is shown that a model of independ busi which allow for the fact that these busi vari in size , as model by a simpl ' partit of integ ' model , provid a good represent of what is observ empir","ordered_present_kp":[15,83,175,389,430],"keyphrases":["firm growth rates","power-law","flatness","correlation","constituent businesses","scaling puzzle","partitions of integers model","size distribution","corporate growth"],"prmu":["P","P","P","P","P","R","R","M","M"]}
{"id":"300","title":"The plot thins: thin-client computer systems and academic libraries","abstract":"The few libraries that have tried thin client architectures have noted a number of compelling reasons to do so. For starters, thin client devices are far less expensive than most PCs. More importantly, thin client computing devices are believed to be far less expensive to manage and support than traditional PCs","tok_text":"the plot thin : thin-client comput system and academ librari \n the few librari that have tri thin client architectur have note a number of compel reason to do so . for starter , thin client devic are far less expens than most pc . more importantli , thin client comput devic are believ to be far less expens to manag and support than tradit pc","ordered_present_kp":[46,16],"keyphrases":["thin-client computer systems","academic libraries"],"prmu":["P","P"]}
{"id":"345","title":"In search of strategic operations research\/management science","abstract":"We define strategic OR\/MS as \"OR\/MS work that leads to a sustainable competitive advantage.\" We found evidence of strategic OR\/MS in the literature of strategic information systems (SIS) and OR\/MS. We examined 30 early examples of SIS, many of which contained OR\/MS work. Many of the most successful had high OR\/MS content, while the least successful contained none. The inclusion of OR\/MS work may be a key to sustaining an advantage from information technology. We also examined the Edelman Prize finalist articles published between 1990 and 1999. We found that 13 of the 42 private sector applications meet our definition of strategic OR\/MS","tok_text":"in search of strateg oper research \/ manag scienc \n we defin strateg or \/ ms as \" or \/ ms work that lead to a sustain competit advantag . \" we found evid of strateg or \/ ms in the literatur of strateg inform system ( si ) and or \/ ms . we examin 30 earli exampl of si , mani of which contain or \/ ms work . mani of the most success had high or \/ ms content , while the least success contain none . the inclus of or \/ ms work may be a key to sustain an advantag from inform technolog . we also examin the edelman prize finalist articl publish between 1990 and 1999 . we found that 13 of the 42 privat sector applic meet our definit of strateg or \/ ms","ordered_present_kp":[21,37,61,193,217],"keyphrases":["operations research","management science","strategic OR\/MS","strategic information systems","SIS"],"prmu":["P","P","P","P","P"]}
{"id":"1947","title":"Modelling user acceptance of building management systems","abstract":"This study examines user acceptance of building management systems (BMS) using a questionnaire survey. These systems are crucial for optimising building performance and yet it has been widely reported that users are not making full use of their systems' facilities. Established models of technology acceptance have been employed in this research, and the positive influence of user perceptions of ease of use and compatibility has been demonstrated. Previous research has indicated differing levels of importance of perceived ease of use relative to other factors. Here, perceived ease of use is shown generally to be more important, though the balance between this and compatibility is moderated by the user perceptions of voluntariness","tok_text":"model user accept of build manag system \n thi studi examin user accept of build manag system ( bm ) use a questionnair survey . these system are crucial for optimis build perform and yet it ha been wide report that user are not make full use of their system ' facil . establish model of technolog accept have been employ in thi research , and the posit influenc of user percept of eas of use and compat ha been demonstr . previou research ha indic differ level of import of perceiv eas of use rel to other factor . here , perceiv eas of use is shown gener to be more import , though the balanc between thi and compat is moder by the user percept of voluntari","ordered_present_kp":[21,106,365,381,396,649],"keyphrases":["building management systems","questionnaire survey","user perceptions","ease of use","compatibility","voluntariness","user acceptance modelling","technology acceptance model","innovation characteristics","information systems"],"prmu":["P","P","P","P","P","P","R","R","U","M"]}
{"id":"239","title":"Content standards for electronic books: the OEBF publication structure and the role of public interest participation","abstract":"In the emerging world of electronic publishing how we create, distribute, and read books will be in a large part determined by an underlying framework of content standards that establishes the range of technological opportunities and constraints for publishing and reading systems. But efforts to develop content standards based on sound engineering models must skillfully negotiate competing and sometimes apparently irreconcilable objectives if they are to produce results relevant to the rapidly changing course of technology. The Open eBook Forum's Publication Structure, an XML-based specification for electronic books, is an example of the sort of timely and innovative problem solving required for successful real-world standards development. As a result of this effort, the electronic book industry will not only happen sooner and on a larger scale than it would have otherwise, but the electronic books it produces will be more functional, more interoperable, and more accessible to all readers. Public interest participants have a critical role in this process","tok_text":"content standard for electron book : the oebf public structur and the role of public interest particip \n in the emerg world of electron publish how we creat , distribut , and read book will be in a larg part determin by an underli framework of content standard that establish the rang of technolog opportun and constraint for publish and read system . but effort to develop content standard base on sound engin model must skill negoti compet and sometim appar irreconcil object if they are to produc result relev to the rapidli chang cours of technolog . the open ebook forum 's public structur , an xml-base specif for electron book , is an exampl of the sort of time and innov problem solv requir for success real-world standard develop . as a result of thi effort , the electron book industri will not onli happen sooner and on a larger scale than it would have otherwis , but the electron book it produc will be more function , more interoper , and more access to all reader . public interest particip have a critic role in thi process","ordered_present_kp":[127,21,0,41,78,600],"keyphrases":["content standards","electronic books","OEBF Publication Structure","public interest participation","electronic publishing","XML-based specification","Open eBook Forum Publication Structure"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"2136","title":"A method of determining a sequence of the best solutions to the problems of optimization on finite sets and the problem of network reconstruction","abstract":"A method of determining a sequence of the best solutions to the problems of optimization on finite sets was proposed. Its complexity was estimated by a polynomial of the dimension of problem input, given number of sequence terms, and complexity of completing the design of the original extremal problem. The technique developed was applied to the typical problem of network reconstruction with the aim of increasing its throughput under restricted reconstruction costs","tok_text":"a method of determin a sequenc of the best solut to the problem of optim on finit set and the problem of network reconstruct \n a method of determin a sequenc of the best solut to the problem of optim on finit set wa propos . it complex wa estim by a polynomi of the dimens of problem input , given number of sequenc term , and complex of complet the design of the origin extrem problem . the techniqu develop wa appli to the typic problem of network reconstruct with the aim of increas it throughput under restrict reconstruct cost","ordered_present_kp":[38,67,76,105,228],"keyphrases":["best solutions","optimization","finite sets","network reconstruction","complexity"],"prmu":["P","P","P","P","P"]}
{"id":"281","title":"Factors contributing to preservice teachers' discomfort in a Web-based course structured as an inquiry","abstract":"A report is given of a qualitative emergent design study of a Science, Technology, Society Interaction (STS) Web-enhanced course. Students' discomfort during the pilot test provided insight into the intellectual scaffolding that preservice secondary science teachers needed to optimize their performance when required to develop understanding through open-ended inquiry in a Web environment. Eight factors identified contributed to student discomfort: computer skills, paradigm shifts, trust, time management, thinking about their own thinking, systematic inquiry, self-assessment, and scientific discourse. These factors suggested developing understanding through inquiry by conducting a self-designed, open-ended, systematic inquiry required autonomous learning involving metacognitive skills and time management skills. To the extent in which students either came into the course with this scaffolding, or developed it during the course, they were successful in learning about STS and its relationship to science teaching. Changes in the Web site made to accommodate learners' needs as they surfaced are described","tok_text":"factor contribut to preservic teacher ' discomfort in a web-bas cours structur as an inquiri \n a report is given of a qualit emerg design studi of a scienc , technolog , societi interact ( st ) web-enhanc cours . student ' discomfort dure the pilot test provid insight into the intellectu scaffold that preservic secondari scienc teacher need to optim their perform when requir to develop understand through open-end inquiri in a web environ . eight factor identifi contribut to student discomfort : comput skill , paradigm shift , trust , time manag , think about their own think , systemat inquiri , self-assess , and scientif discours . these factor suggest develop understand through inquiri by conduct a self-design , open-end , systemat inquiri requir autonom learn involv metacognit skill and time manag skill . to the extent in which student either came into the cours with thi scaffold , or develop it dure the cours , they were success in learn about st and it relationship to scienc teach . chang in the web site made to accommod learner ' need as they surfac are describ","ordered_present_kp":[56,118,194,479,278,303,408,430,500,515,532,540,553,583,602,620,758,779,800,70,323],"keyphrases":["Web-based course","STS","qualitative emergent design study","Web-enhanced course","intellectual scaffolding","preservice secondary science teachers","science teaching","open-ended inquiry","Web environment","student discomfort","computer skills","paradigm shifts","trust","time management","thinking","systematic inquiry","self-assessment","scientific discourse","autonomous learning","metacognitive skills","time management skills","preservice teacher discomfort","science technology society interaction course"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2173","title":"E-mail and the legal profession","abstract":"The widespread use of E-mail can be found in all areas of commerce, and the legal profession is one that has embraced this new medium of communication. E-mail is not without its drawbacks, however. Due to the nature of the technologies behind the medium, it is a less secure form of communication than many of those traditionally used by the legal profession, including DX, facsimile, and standard and registered post. There are a number of ways in which E-mails originating from the practice may be protected, including software encryption, hardware encryption and various methods of controlling and administering access to the E-mails","tok_text":"e-mail and the legal profess \n the widespread use of e-mail can be found in all area of commerc , and the legal profess is one that ha embrac thi new medium of commun . e-mail is not without it drawback , howev . due to the natur of the technolog behind the medium , it is a less secur form of commun than mani of those tradit use by the legal profess , includ dx , facsimil , and standard and regist post . there are a number of way in which e-mail origin from the practic may be protect , includ softwar encrypt , hardwar encrypt and variou method of control and administ access to the e-mail","ordered_present_kp":[0,15,498,516],"keyphrases":["E-mail","legal profession","software encryption","hardware encryption","secure communication","access control"],"prmu":["P","P","P","P","R","R"]}
{"id":"2093","title":"Fresh tracks [food processing]","abstract":"Bar code labels and wireless terminals linked to a centralized database accurately track meat products from receiving to customers for Farmland Foods","tok_text":"fresh track [ food process ] \n bar code label and wireless termin link to a central databas accur track meat product from receiv to custom for farmland food","ordered_present_kp":[14,31,50,143],"keyphrases":["food processing","bar code labels","wireless terminals","Farmland Foods","automatic data capture","Intermec Technologies"],"prmu":["P","P","P","P","U","U"]}
{"id":"324","title":"Using NetCloak to develop server-side Web-based experiments without writing CGI programs","abstract":"Server-side experiments use the Web server, rather than the participant's browser, to handle tasks such as random assignment, eliminating inconsistencies with Java and other client-side applications. Heretofore, experimenters wishing to create server-side experiments have had to write programs to create common gateway interface (CGI) scripts in programming languages such as Perl and C++. NetCloak uses simple, HTML-like commands to create CGIs. We used NetCloak to implement an experiment on probability estimation. Measurements of time on task and participants' IP addresses assisted quality control. Without prior training, in less than 1 month, we were able to use NetCloak to design and create a Web-based experiment and to help graduate students create three Web-based experiments of their own","tok_text":"use netcloak to develop server-sid web-bas experi without write cgi program \n server-sid experi use the web server , rather than the particip 's browser , to handl task such as random assign , elimin inconsist with java and other client-sid applic . heretofor , experiment wish to creat server-sid experi have had to write program to creat common gateway interfac ( cgi ) script in program languag such as perl and c++ . netcloak use simpl , html-like command to creat cgi . we use netcloak to implement an experi on probabl estim . measur of time on task and particip ' ip address assist qualiti control . without prior train , in less than 1 month , we were abl to use netcloak to design and creat a web-bas experi and to help graduat student creat three web-bas experi of their own","ordered_present_kp":[4,24,64,104,177,215,230,406,517,571,589,729],"keyphrases":["NetCloak","server-side Web-based experiments","CGI programs","Web server","random assignment","Java","client-side applications","Perl","probability estimation","IP addresses","quality control","graduate students","common gateway interface scripts","C++ language","HTML","Internet","behavioral data","psychology"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","R","R","U","U","U","U"]}
{"id":"361","title":"A pretopological approach for structural analysis","abstract":"The aim of this paper is to present a methodological approach for problems encountered in structural analysis. This approach is based upon the pretopological concepts of pseudoclosure and minimal closed subsets. The advantage of this approach is that it provides a framework which is general enough to model and formulate different types of connections that exist between the elements of a population. In addition, it has enabled us to develop a new structural analysis algorithm. An explanation of the definitions and properties of the pretopological concepts applied in this work is first shown and illustrated in sample settings. The structural analysis algorithm is then described and the results obtained in an economic study of the impact of geographic proximity on scientific collaborations are presented","tok_text":"a pretopolog approach for structur analysi \n the aim of thi paper is to present a methodolog approach for problem encount in structur analysi . thi approach is base upon the pretopolog concept of pseudoclosur and minim close subset . the advantag of thi approach is that it provid a framework which is gener enough to model and formul differ type of connect that exist between the element of a popul . in addit , it ha enabl us to develop a new structur analysi algorithm . an explan of the definit and properti of the pretopolog concept appli in thi work is first shown and illustr in sampl set . the structur analysi algorithm is then describ and the result obtain in an econom studi of the impact of geograph proxim on scientif collabor are present","ordered_present_kp":[2,26,213,196,350,673,703,722],"keyphrases":["pretopological approach","structural analysis","pseudoclosure","minimal closed subsets","connections","economic study","geographic proximity","scientific collaborations"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"319","title":"Designing a screening experiment for highly reliable products","abstract":"Within a reasonable life-testing time, how to improve the reliability of highly reliable products is one of the great challenges. By using a resolution III experiment together with degradation test, Tseng et al. (1995) presented a case study of improving the reliability of fluorescent lamps. However, in conducting such an experiment, they did not address the problem of how to choose the optimal settings of variables, such as sample size, inspection frequency, and termination time for each run, which are influential to the correct identification of significant factors and the experimental cost. Assuming that the product's degradation paths satisfy Wiener processes, this paper proposes a systematic approach to the aforementioned problem. First, an identification rule is proposed. Next, under the constraints of a minimum probability of correct decision and a maximum probability of incorrect decision of the proposed identification rule, the optimum test plan can be obtained by minimizing the total experimental cost. An example is provided to illustrate the proposed method","tok_text":"design a screen experi for highli reliabl product \n within a reason life-test time , how to improv the reliabl of highli reliabl product is one of the great challeng . by use a resolut iii experi togeth with degrad test , tseng et al . ( 1995 ) present a case studi of improv the reliabl of fluoresc lamp . howev , in conduct such an experi , they did not address the problem of how to choos the optim set of variabl , such as sampl size , inspect frequenc , and termin time for each run , which are influenti to the correct identif of signific factor and the experiment cost . assum that the product 's degrad path satisfi wiener process , thi paper propos a systemat approach to the aforement problem . first , an identif rule is propos . next , under the constraint of a minimum probabl of correct decis and a maximum probabl of incorrect decis of the propos identif rule , the optimum test plan can be obtain by minim the total experiment cost . an exampl is provid to illustr the propos method","ordered_present_kp":[9,27,208,624,440,463,291,774,813,716],"keyphrases":["screening experiment","highly reliable products","degradation tests","fluorescent lamps","inspection frequency","termination time","Wiener process","identification rule","minimum probability of correct decision","maximum probability of incorrect decision","resolution III design","optimal test plan"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2053","title":"Teaching management science with spreadsheets: From decision models to decision support","abstract":"The 1990s were a decade of enormous change for management science (MS) educators. While the outlook at the beginning of the decade was somewhat bleak, the renaissance in MS education brought about by the use of spreadsheets as the primary delivery vehicle for quantitative modeling techniques has resulted in a much brighter future. This paper takes inventory of the current state of MS education and suggests some promising new directions in the area of decision support systems for MS educators to consider for the future","tok_text":"teach manag scienc with spreadsheet : from decis model to decis support \n the 1990 were a decad of enorm chang for manag scienc ( ms ) educ . while the outlook at the begin of the decad wa somewhat bleak , the renaiss in ms educ brought about by the use of spreadsheet as the primari deliveri vehicl for quantit model techniqu ha result in a much brighter futur . thi paper take inventori of the current state of ms educ and suggest some promis new direct in the area of decis support system for ms educ to consid for the futur","ordered_present_kp":[6,221,24,304,471],"keyphrases":["management science","spreadsheets","MS education","quantitative modeling","decision support systems"],"prmu":["P","P","P","P","P"]}
{"id":"2016","title":"Fully automatic algorithm for region of interest location in camera calibration","abstract":"We present an automatic method for region of interest (ROI) location in camera calibration used in computer vision inspection. An intelligent ROI location algorithm based on the Radon transform is developed to automate the calibration process. The algorithm remains robust even if the anchor target has a notable rotation angle in the target plane. This method functions well although the anchor target is not carefully positioned. Several improvement methods are studied to avoid the algorithm's huge time\/space consumption problem. The algorithm runs about 100 times faster if these improvement methods are applied. Using this method fully automatic camera calibration is achieved without human interactive ROI specification. Experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically","tok_text":"fulli automat algorithm for region of interest locat in camera calibr \n we present an automat method for region of interest ( roi ) locat in camera calibr use in comput vision inspect . an intellig roi locat algorithm base on the radon transform is develop to autom the calibr process . the algorithm remain robust even if the anchor target ha a notabl rotat angl in the target plane . thi method function well although the anchor target is not care posit . sever improv method are studi to avoid the algorithm 's huge time \/ space consumpt problem . the algorithm run about 100 time faster if these improv method are appli . use thi method fulli automat camera calibr is achiev without human interact roi specif . experi show that thi algorithm can help to calibr the intrins paramet of the zoom len and the camera paramet quickli and automat","ordered_present_kp":[0,38,56,28,162,198,230,270,353,519,641,769,792,809],"keyphrases":["Fully automatic algorithm","region of interest location","interest location","camera calibration","computer vision inspection","ROI location algorithm","Radon transform","calibration process","rotation angle","time\/space consumption problem","fully automatic camera calibration","intrinsic parameters","zoom lens","camera parameters","human interactive specification"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"1987","title":"Virus hunting","abstract":"We all appreciate the need for, and hopefully we have all deployed, anti-virus software. The good news is that AV software has come a long way fast. Four or so years ago it was true to write that AV software could not detect Trojan Horses and similar intrusion attempts. Now it can and does. McAfee's VirusScan, for example, goes one further; it detects viruses, worms and Trojan Horses and deploys itself as a firewall to filter data packets, control access to Internet resources, activate rule sets for specific applications, in general to protect against hackers. But like so much software, we use it with little thought as to how it came to do its job. Behind the scenes there is an army of top notch programmers trying to stay ahead of the baddies who, at the last count, had produced some 60,000 viruses","tok_text":"viru hunt \n we all appreci the need for , and hope we have all deploy , anti-viru softwar . the good news is that av softwar ha come a long way fast . four or so year ago it wa true to write that av softwar could not detect trojan hors and similar intrus attempt . now it can and doe . mcafe 's virusscan , for exampl , goe one further ; it detect virus , worm and trojan hors and deploy itself as a firewal to filter data packet , control access to internet resourc , activ rule set for specif applic , in gener to protect against hacker . but like so much softwar , we use it with littl thought as to how it came to do it job . behind the scene there is an armi of top notch programm tri to stay ahead of the baddi who , at the last count , had produc some 60,000 virus","ordered_present_kp":[72,677,356,224],"keyphrases":["anti-virus software","Trojan Horses","worms","programmers"],"prmu":["P","P","P","P"]}
{"id":"241","title":"Perspectives on scholarly online books: the Columbia University Online Books Evaluation Project","abstract":"The Online Books Evaluation Project at Columbia University studied the potential for scholarly online books from 1995 to 1999. Issues included scholars' interest in using online books, the role they might play in scholarly life, features that scholars and librarians sought in online books, the costs of producing and owning print and online books, and potential marketplace arrangements. Scholars see potential for online books to make their research, learning, and teaching more efficient and effective. Librarians see potential to serve their scholars better. Librarians may face lower costs if they can serve their scholars with online books instead of print books. Publishers may be able to offer scholars greater opportunities to use their books while enhancing their own profitability","tok_text":"perspect on scholarli onlin book : the columbia univers onlin book evalu project \n the onlin book evalu project at columbia univers studi the potenti for scholarli onlin book from 1995 to 1999 . issu includ scholar ' interest in use onlin book , the role they might play in scholarli life , featur that scholar and librarian sought in onlin book , the cost of produc and own print and onlin book , and potenti marketplac arrang . scholar see potenti for onlin book to make their research , learn , and teach more effici and effect . librarian see potenti to serv their scholar better . librarian may face lower cost if they can serv their scholar with onlin book instead of print book . publish may be abl to offer scholar greater opportun to use their book while enhanc their own profit","ordered_present_kp":[39,12,674,352,410,479,490],"keyphrases":["scholarly online books","Columbia University Online Books Evaluation Project","costs","marketplace arrangements","research","learning","print books"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"204","title":"Self-calibration from image derivatives","abstract":"This study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters, where the image formation is modeled with a linear pinhole-camera model. The equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters. A set of parameters encoding the latter component is linearly related to the flow, and from these parameters the calibration can be determined. However, as for discrete motion, in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components. Geometrically, the ambiguity takes the form of a part of the rotational component being parallel to the translational component, and thus the scene can be reconstructed only up to a projective transformation. In general, for full calibration at least four successive image frames are necessary, with the 3D rotation changing between the measurements. The geometric analysis gives rise to a direct self-calibration method that avoids computation of optical flow or point correspondences and uses only normal flow measurements. New constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives, and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated. The calibration parameters are then estimated from the rotational components of several flow fields. As the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks, or as a tool for analyzing image sequences in large video databases","tok_text":"self-calibr from imag deriv \n thi studi investig the problem of estim camera calibr paramet from imag motion field induc by a rigidli move camera with unknown paramet , where the imag format is model with a linear pinhole-camera model . the equat obtain show the flow to be separ into a compon due to the translat and the calibr paramet and a compon due to the rotat and the calibr paramet . a set of paramet encod the latter compon is linearli relat to the flow , and from these paramet the calibr can be determin . howev , as for discret motion , in gener it is not possibl to decoupl imag measur obtain from onli two frame into translat and rotat compon . geometr , the ambigu take the form of a part of the rotat compon be parallel to the translat compon , and thu the scene can be reconstruct onli up to a project transform . in gener , for full calibr at least four success imag frame are necessari , with the 3d rotat chang between the measur . the geometr analysi give rise to a direct self-calibr method that avoid comput of optic flow or point correspond and use onli normal flow measur . new constraint on the smooth of the surfac in view are formul to relat structur and motion directli to imag deriv , and on the basi of these constraint the transform of the view geometri between consecut imag is estim . the calibr paramet are then estim from the rotat compon of sever flow field . as the propos techniqu neither requir a special set up nor need exact correspond it is potenti use for the calibr of activ vision system which have to acquir knowledg about their intrins paramet while they perform other task , or as a tool for analyz imag sequenc in larg video databas","ordered_present_kp":[70,97,126,179,207,77,587,743,644,987,1034,1048,1078,1514,1648,1664],"keyphrases":["camera calibration parameters","calibration parameters","image motion fields","rigidly moving camera","image formation","linear pinhole-camera model","image measurements","rotational components","translational components","direct self-calibration method","optical flow","point correspondences","normal flow measurements","active vision systems","image sequences","large video databases","depth distortion"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","U"]}
{"id":"2051","title":"Who wants to see a $million error?","abstract":"Inspired by the popular television show \"Who Wants to Be a Millionaire?\", this case discusses the monetary decisions contestants face on a game consisting of 15 increasingly difficult multiple choice questions. Since the game continues as long as a contestant answers correctly, this case, at its core, is one of sequential decision analysis, amenable to analysis via stochastic dynamic programming. The case is also suitable for a course dealing with single decision analysis, allowing for discussion of utility theory and Bayesian probability revision. In developing a story line for the case, the author has sprinkled in much background material on probability and statistics. This material is placed in a historical context, illuminating some of the influential scholars involved in the development of these subjects as well as the birth of operations research and the management sciences","tok_text":"who want to see a $ million error ? \n inspir by the popular televis show \" who want to be a millionair ? \" , thi case discuss the monetari decis contest face on a game consist of 15 increasingli difficult multipl choic question . sinc the game continu as long as a contest answer correctli , thi case , at it core , is one of sequenti decis analysi , amen to analysi via stochast dynam program . the case is also suitabl for a cours deal with singl decis analysi , allow for discuss of util theori and bayesian probabl revis . in develop a stori line for the case , the author ha sprinkl in much background materi on probabl and statist . thi materi is place in a histor context , illumin some of the influenti scholar involv in the develop of these subject as well as the birth of oper research and the manag scienc","ordered_present_kp":[782,335,371,629],"keyphrases":["decision analysis","stochastic dynamic programming","statistics","operations research","game theory","educational course","probabilistic models"],"prmu":["P","P","P","P","R","M","U"]}
{"id":"2014","title":"Adaptive filtering for noise reduction in hue saturation intensity color space","abstract":"Even though the hue saturation intensity (HSI) color model has been widely used in color image processing and analysis, the conversion formulas from the RGB color model to HSI are nonlinear and complicated in comparison with the conversion formulas of other color models. When an RGB image is degraded by random Gaussian noise, this nonlinearity leads to a nonuniform noise distribution in HSI, making accurate image analysis more difficult. We have analyzed the noise characteristics of the HSI color model and developed an adaptive spatial filtering method to reduce the magnitude of noise and the nonuniformity of noise variance in the HSI color space. With this adaptive filtering method, the filter kernel for each pixel is dynamically adjusted, depending on the values of intensity and saturation. In our experiments we have filtered the saturation and hue components and generated edge maps from color gradients. We have found that by using the adaptive filtering method, the minimum error rate in edge detection improves by approximately 15%","tok_text":"adapt filter for nois reduct in hue satur intens color space \n even though the hue satur intens ( hsi ) color model ha been wide use in color imag process and analysi , the convers formula from the rgb color model to hsi are nonlinear and complic in comparison with the convers formula of other color model . when an rgb imag is degrad by random gaussian nois , thi nonlinear lead to a nonuniform nois distribut in hsi , make accur imag analysi more difficult . we have analyz the nois characterist of the hsi color model and develop an adapt spatial filter method to reduc the magnitud of nois and the nonuniform of nois varianc in the hsi color space . with thi adapt filter method , the filter kernel for each pixel is dynam adjust , depend on the valu of intens and satur . in our experi we have filter the satur and hue compon and gener edg map from color gradient . we have found that by use the adapt filter method , the minimum error rate in edg detect improv by approxim 15 %","ordered_present_kp":[0,17,32,136,198,339,386,426,537,386,617,637,690,713,36,42,836,855,950,928],"keyphrases":["adaptive filtering","noise reduction","hue saturation intensity color space","saturation","intensity","color image processing","RGB color model","random Gaussian noise","nonuniform noise distribution","nonuniformity","accurate image analysis","adaptive spatial filtering method","noise variance","HSI color space","filter kernel","pixel","generated edge maps","color gradients","minimum error rate","edge detection","color image analysis"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"243","title":"BioOne: a new model for scholarly publishing","abstract":"This article describes a unique electronic journal publishing project involving the University of Kansas, the Big 12 Plus Libraries Consortium, the American Institute of Biological Sciences, Allen Press, and SPARC, the Scholarly Publishing and Academic Resources Coalition. This partnership has created BioOne, a database of 40 full-text society journals in the biological and environmental sciences, which was launched in April, 2001. The genesis and development of the project is described and financial, technical, and intellectual property models for the project are discussed. Collaborative strategies for the project are described","tok_text":"bioon : a new model for scholarli publish \n thi articl describ a uniqu electron journal publish project involv the univers of kansa , the big 12 plu librari consortium , the american institut of biolog scienc , allen press , and sparc , the scholarli publish and academ resourc coalit . thi partnership ha creat bioon , a databas of 40 full-text societi journal in the biolog and environment scienc , which wa launch in april , 2001 . the genesi and develop of the project is describ and financi , technic , and intellectu properti model for the project are discuss . collabor strategi for the project are describ","ordered_present_kp":[71,115,138,174,211,229,241,195,380,512,568],"keyphrases":["electronic journal publishing project","University of Kansas","Big 12 Plus Libraries Consortium","American Institute of Biological Sciences","biological sciences","Allen Press","SPARC","Scholarly Publishing and Academic Resources Coalition","environmental sciences","intellectual property models","collaborative strategies","BioOne full-text society journal database","scholarly publishing model","technical models","financial models"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","R","R"]}
{"id":"206","title":"Information architecture: looking ahead","abstract":"It may be a bit strange to consider where the field of information architecture (IA) is headed. After all, many would argue that it's too new to be considered as a field at all, or that it is mislabeled, and by no means is there a widely accepted definition of what information architecture actually is. Practicing information architects probably number in the thousands, and this vibrant group is already building various forms of communal infrastructure, ranging from an IA journal and a self-organizing \"library\" of resources to a passel of local professional groups and degree-granting academic programs. So the profession has achieved a beachhead that will enable it to stabilize and perhaps even grow during these difficult times","tok_text":"inform architectur : look ahead \n it may be a bit strang to consid where the field of inform architectur ( ia ) is head . after all , mani would argu that it 's too new to be consid as a field at all , or that it is mislabel , and by no mean is there a wide accept definit of what inform architectur actual is . practic inform architect probabl number in the thousand , and thi vibrant group is alreadi build variou form of commun infrastructur , rang from an ia journal and a self-organ \" librari \" of resourc to a passel of local profession group and degree-gr academ program . so the profess ha achiev a beachhead that will enabl it to stabil and perhap even grow dure these difficult time","ordered_present_kp":[0,0,424,526,553],"keyphrases":["information architecture","information architects","communal infrastructure","local professional groups","degree-granting academic programs"],"prmu":["P","P","P","P","P"]}
{"id":"1978","title":"Multilayered image representation: application to image compression","abstract":"The main contribution of this work is a new paradigm for image representation and image compression. We describe a new multilayered representation technique for images. An image is parsed into a superposition of coherent layers: piecewise smooth regions layer, textures layer, etc. The multilayered decomposition algorithm consists in a cascade of compressions applied successively to the image itself and to the residuals that resulted from the previous compressions. During each iteration of the algorithm, we code the residual part in a lossy way: we only retain the most significant structures of the residual part, which results in a sparse representation. Each layer is encoded independently with a different transform, or basis, at a different bitrate, and the combination of the compressed layers can always be reconstructed in a meaningful way. The strength of the multilayer approach comes from the fact that different sets of basis functions complement each others: some of the basis functions will give reasonable account of the large trend of the data, while others will catch the local transients, or the oscillatory patterns. This multilayered representation has a lot of beautiful applications in image understanding, and image and video coding. We have implemented the algorithm and we have studied its capabilities","tok_text":"multilay imag represent : applic to imag compress \n the main contribut of thi work is a new paradigm for imag represent and imag compress . we describ a new multilay represent techniqu for imag . an imag is pars into a superposit of coher layer : piecewis smooth region layer , textur layer , etc . the multilay decomposit algorithm consist in a cascad of compress appli success to the imag itself and to the residu that result from the previou compress . dure each iter of the algorithm , we code the residu part in a lossi way : we onli retain the most signific structur of the residu part , which result in a spars represent . each layer is encod independ with a differ transform , or basi , at a differ bitrat , and the combin of the compress layer can alway be reconstruct in a meaning way . the strength of the multilay approach come from the fact that differ set of basi function complement each other : some of the basi function will give reason account of the larg trend of the data , while other will catch the local transient , or the oscillatori pattern . thi multilay represent ha a lot of beauti applic in imag understand , and imag and video code . we have implement the algorithm and we have studi it capabl","ordered_present_kp":[36,157,9,247,278,303,502,612,873],"keyphrases":["image representation","image compression","multilayered representation","piecewise smooth regions layer","textures layer","multilayered decomposition algorithm","residual part","sparse representation","basis functions","wavelet transforms","cosine transforms","transform coding"],"prmu":["P","P","P","P","P","P","P","P","P","M","M","R"]}
{"id":"1985","title":"Prospective on computer applications in power","abstract":"The so-called \"deregulation\" and restructuring of the electric power industry have made it very difficult to keep up with industry changes and have made it much more difficult to envision the future. In this article, current key issues and major developments of the past few years are reviewed to provide perspective, and prospects for future computer applications in power are suggested. Technology changes are occurring at an exponential rate. The interconnected bulk electric systems are becoming integrated with vast networked information systems. This article discusses the skills that will be needed by future power engineers to keep pace with these developments and trends","tok_text":"prospect on comput applic in power \n the so-cal \" deregul \" and restructur of the electr power industri have made it veri difficult to keep up with industri chang and have made it much more difficult to envis the futur . in thi articl , current key issu and major develop of the past few year are review to provid perspect , and prospect for futur comput applic in power are suggest . technolog chang are occur at an exponenti rate . the interconnect bulk electr system are becom integr with vast network inform system . thi articl discuss the skill that will be need by futur power engin to keep pace with these develop and trend","ordered_present_kp":[12,385,438,497],"keyphrases":["computer applications","technology changes","interconnected bulk electric systems","networked information systems","electric power industry deregulation","electricity industry restructuring"],"prmu":["P","P","P","P","R","R"]}
{"id":"2109","title":"Internet-based psychological experimenting: five dos and five don'ts","abstract":"Internet-based psychological experimenting is presented as a method that needs careful consideration of a number of issues-from potential data corruption to revealing confidential information about participants. Ten issues are grouped into five areas of actions to be taken when developing an Internet experiment (dos) and five errors to be avoided (don'ts). Dos include: (a) utilizing dropout as a dependent variable, (b) the use of dropout to detect motivational confounding, (c) placement of questions for personal information, (d) using a collection of techniques, and (e) using Internet-based tools. Don'ts are about: (a) unprotected directories, (b) public access to confidential data, (c) revealing the experiment's structure, (d) ignoring the Internet's technical variance, and (e) improper use of form elements","tok_text":"internet-bas psycholog experi : five do and five don't \n internet-bas psycholog experi is present as a method that need care consider of a number of issues-from potenti data corrupt to reveal confidenti inform about particip . ten issu are group into five area of action to be taken when develop an internet experi ( do ) and five error to be avoid ( don't ) . do includ : ( a ) util dropout as a depend variabl , ( b ) the use of dropout to detect motiv confound , ( c ) placement of question for person inform , ( d ) use a collect of techniqu , and ( e ) use internet-bas tool . don't are about : ( a ) unprotect directori , ( b ) public access to confidenti data , ( c ) reveal the experi 's structur , ( d ) ignor the internet 's technic varianc , and ( e ) improp use of form element","ordered_present_kp":[0,169,384,449,498,606,13],"keyphrases":["Internet-based psychological experimenting","psychology","data corruption","dropout","motivational confounding","personal information","unprotected directories","data confidentiality","Web experiment","online research techniques"],"prmu":["P","P","P","P","P","P","P","R","M","M"]}
{"id":"2134","title":"Linear models of circuits based on the multivalued components","abstract":"Linearization and planarization of the circuit models is pivotal to the submicron technologies. On the other hand, the characteristics of the VLSI circuits can be sometimes improved by using the multivalued components. It was shown that any l-level circuit based on the multivalued components is representable as an algebraic model based on l linear arithmetic polynomials mapped correspondingly into l decision diagrams that are linear and planar by nature. Complexity of representing a circuit as the linear decision diagram was estimated as O(G) with G for the number of multivalued components in the circuit. The results of testing the LinearDesignMV algorithm on circuits of more than 8000 LGSynth 93 multivalued components were presented","tok_text":"linear model of circuit base on the multivalu compon \n linear and planar of the circuit model is pivot to the submicron technolog . on the other hand , the characterist of the vlsi circuit can be sometim improv by use the multivalu compon . it wa shown that ani l-level circuit base on the multivalu compon is represent as an algebra model base on l linear arithmet polynomi map correspondingli into l decis diagram that are linear and planar by natur . complex of repres a circuit as the linear decis diagram wa estim as o(g ) with g for the number of multivalu compon in the circuit . the result of test the lineardesignmv algorithm on circuit of more than 8000 lgsynth 93 multivalu compon were present","ordered_present_kp":[0,66,110,176,350,610,664],"keyphrases":["linearization","planarization","submicron technologies","VLSI circuits","linear arithmetic polynomials","LinearDesignMV algorithm","LGSynth 93 multivalued components","linear circuit model","linear planar decision diagrams","circuit representation complexity"],"prmu":["P","P","P","P","P","P","P","R","R","R"]}
{"id":"283","title":"Alien Rescue: a problem-based hypermedia learning environment for middle school science","abstract":"The article describes an innovative hypermedia product for sixth graders in space science: Alien Rescue. Using a problem-based learning approach that is highly interactive, Alien Rescue engages students in scientific investigations aimed at finding solutions to complex and meaningful problems. Problem-based learning (PBL) is an instructional strategy proven to be effective in medical and business fields, and it is increasingly popular in education. However, using PBL in K-12 classrooms is challenging and requires access to rich knowledge bases and cognitive tools. Alien Rescue is designed to provide such cognitive support for successful use of PBL in sixth-grade classrooms. The design and development of Alien Rescue is guided by current educational research. Research is an integral part of this project. Results of formative evaluation and research studies are being integrated into the development and improvement of the program. Alien Rescue is designed in accordance with the National Science Standards and the Texas Essential Knowledge and Skills (TEKS) for science. So far Alien Rescue has been field-tested by approximately 1400 sixth graders. More use in middle schools is in progress and more research on its use is planned","tok_text":"alien rescu : a problem-bas hypermedia learn environ for middl school scienc \n the articl describ an innov hypermedia product for sixth grader in space scienc : alien rescu . use a problem-bas learn approach that is highli interact , alien rescu engag student in scientif investig aim at find solut to complex and meaning problem . problem-bas learn ( pbl ) is an instruct strategi proven to be effect in medic and busi field , and it is increasingli popular in educ . howev , use pbl in k-12 classroom is challeng and requir access to rich knowledg base and cognit tool . alien rescu is design to provid such cognit support for success use of pbl in sixth-grad classroom . the design and develop of alien rescu is guid by current educ research . research is an integr part of thi project . result of form evalu and research studi are be integr into the develop and improv of the program . alien rescu is design in accord with the nation scienc standard and the texa essenti knowledg and skill ( tek ) for scienc . so far alien rescu ha been field-test by approxim 1400 sixth grader . more use in middl school is in progress and more research on it use is plan","ordered_present_kp":[0,16,57,146,130,263,352,364,415,488,536,559,610,731,801,57],"keyphrases":["Alien Rescue","problem-based hypermedia learning environment","middle school science","middle schools","sixth graders","space science","scientific investigations","PBL","instructional strategy","business fields","K-12 classrooms","rich knowledge bases","cognitive tools","cognitive support","educational research","formative evaluation","medical fields"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2171","title":"Evicting orang utans from the office [electronic storage of legal files]","abstract":"Having espoused the principle of the paperless office some time ago, we decided to apply it to our stored files. First we consulted the Law Society rules governing storage of files on electronic media. The next step was for us to draw up a protocol for scanning the files. The benefits of the exercise have been significant. The area previously used for storage has been freed for other use. Files are now available online, instantaneously. When we have needed to send out files to the client or following a change of solicitor, we have been able to do so almost immediately, by E-mail, retaining a copy for our future reference. The files are protected from loss or deterioration, back-up copies having been taken which are stored off site. The complete stored file archive can be put in your pocket (in CD-ROM format) or on a laptop, facilitating remote working","tok_text":"evict orang utan from the offic [ electron storag of legal file ] \n have espous the principl of the paperless offic some time ago , we decid to appli it to our store file . first we consult the law societi rule govern storag of file on electron media . the next step wa for us to draw up a protocol for scan the file . the benefit of the exercis have been signific . the area previous use for storag ha been freed for other use . file are now avail onlin , instantan . when we have need to send out file to the client or follow a chang of solicitor , we have been abl to do so almost immedi , by e-mail , retain a copi for our futur refer . the file are protect from loss or deterior , back-up copi have been taken which are store off site . the complet store file archiv can be put in your pocket ( in cd-rom format ) or on a laptop , facilit remot work","ordered_present_kp":[100,53,34,194,803,760],"keyphrases":["electronic storage","legal files","paperless office","Law Society rules","file archive","CD-ROM","file scanning"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"1945","title":"The development of a mobile manipulator imaging system for bridge crack inspection","abstract":"A mobile manipulator imaging system is developed for the automation of bridge crack inspection. During bridge safety inspections, an eyesight inspection is made for preliminary evaluation and screening before a more precise inspection. The inspection for cracks is an important part of the preliminary evaluation. Currently, the inspectors must stand on the platform of a bridge inspection vehicle or a temporarily erected scaffolding to examine the underside of a bridge. However, such a procedure is risky. To help automate the bridge crack inspection process, we installed two CCD cameras and a four-axis manipulator system on a mobile vehicle. The parallel cameras are used to detect cracks. The manipulator system is equipped with binocular charge coupled devices (CCD) for examining structures that may not be accessible to the eye. The system also reduces the danger of accidents to the human inspectors. The manipulator system consists of four arms. Balance weights are placed at the ends of arms 2 and 4, respectively, to maintain the center of gravity during operation. Mechanically, arms 2 and 4 can revolve smoothly. Experiments indicated that the system could be useful for bridge crack inspections","tok_text":"the develop of a mobil manipul imag system for bridg crack inspect \n a mobil manipul imag system is develop for the autom of bridg crack inspect . dure bridg safeti inspect , an eyesight inspect is made for preliminari evalu and screen befor a more precis inspect . the inspect for crack is an import part of the preliminari evalu . current , the inspector must stand on the platform of a bridg inspect vehicl or a temporarili erect scaffold to examin the undersid of a bridg . howev , such a procedur is riski . to help autom the bridg crack inspect process , we instal two ccd camera and a four-axi manipul system on a mobil vehicl . the parallel camera are use to detect crack . the manipul system is equip with binocular charg coupl devic ( ccd ) for examin structur that may not be access to the eye . the system also reduc the danger of accid to the human inspector . the manipul system consist of four arm . balanc weight are place at the end of arm 2 and 4 , respect , to maintain the center of graviti dure oper . mechan , arm 2 and 4 can revolv smoothli . experi indic that the system could be use for bridg crack inspect","ordered_present_kp":[17,31,47,116,178,575,592,640,725],"keyphrases":["mobile manipulator","imaging system","bridge crack inspection","automation","eyesight inspection","CCD cameras","four-axis manipulator","parallel cameras","charge coupled devices","binocular CCD"],"prmu":["P","P","P","P","P","P","P","P","P","R"]}
{"id":"2091","title":"Prospects for quantitative computed tomography imaging in the presence of foreign metal bodies using statistical image reconstruction","abstract":"X-ray computed tomography (CT) images of patients bearing metal intracavitary applicators or other metal foreign objects exhibit severe artifacts including streaks and aliasing. We have systematically evaluated via computer simulations the impact of scattered radiation, the polyenergetic spectrum, and measurement noise on the performance of three reconstruction algorithms: conventional filtered backprojection (FBP), deterministic iterative deblurring, and a new iterative algorithm, alternating minimization (AM), based on a CT detector model that includes noise, scatter, and polyenergetic spectra. Contrary to the dominant view of the literature, FBP streaking artifacts are due mostly to mismatches between FBP's simplified model of CT detector response and the physical process of signal acquisition. Artifacts on AM images are significantly mitigated as this algorithm substantially reduces detector-model mismatches. However, metal artifacts are reduced to acceptable levels only when prior knowledge of the metal object in the patient, including its pose, shape, and attenuation map, are used to constrain AM's iterations. AM image reconstruction, in combination with object-constrained CT to estimate the pose of metal objects in the patient, is a promising approach for effectively mitigating metal artifacts and making quantitative estimation of tissue attenuation coefficients a clinical possibility","tok_text":"prospect for quantit comput tomographi imag in the presenc of foreign metal bodi use statist imag reconstruct \n x-ray comput tomographi ( ct ) imag of patient bear metal intracavitari applic or other metal foreign object exhibit sever artifact includ streak and alias . we have systemat evalu via comput simul the impact of scatter radiat , the polyenerget spectrum , and measur nois on the perform of three reconstruct algorithm : convent filter backproject ( fbp ) , determinist iter deblur , and a new iter algorithm , altern minim ( am ) , base on a ct detector model that includ nois , scatter , and polyenerget spectra . contrari to the domin view of the literatur , fbp streak artifact are due mostli to mismatch between fbp 's simplifi model of ct detector respons and the physic process of signal acquisit . artifact on am imag are significantli mitig as thi algorithm substanti reduc detector-model mismatch . howev , metal artifact are reduc to accept level onli when prior knowledg of the metal object in the patient , includ it pose , shape , and attenu map , are use to constrain am 's iter . am imag reconstruct , in combin with object-constrain ct to estim the pose of metal object in the patient , is a promis approach for effect mitig metal artifact and make quantit estim of tissu attenu coeffici a clinic possibl","ordered_present_kp":[13,62,85,1144,505,522,554,379,324,605,1318,469,440],"keyphrases":["quantitative computed tomography imaging","foreign metal bodies","statistical image reconstruction","scatter","noise","filtered backprojection","deterministic iterative deblurring","iterative algorithm","alternating minimization","CT detector model","polyenergetic spectra","object-constrained CT","clinical possibility","metal artifact reduction","brachytherapy","medical diagnostic imaging","signal acquisition physical process"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","M","U","M","R"]}
{"id":"326","title":"Web-based experiments controlled by JavaScript: an example from probability learning","abstract":"JavaScript programs can be used to control Web experiments. This technique is illustrated by an experiment that tested the effects of advice on performance in the classic probability-learning paradigm. Previous research reported that people tested via the Web or in the lab tended to match the probabilities of their responses to the probabilities that those responses would be reinforced. The optimal strategy, however, is to consistently choose the more frequent event; probability matching produces suboptimal performance. We investigated manipulations we reasoned should improve performance. A horse race scenario in which participants predicted the winner in each of a series of races between two horses was compared with an abstract scenario used previously. Ten groups of learners received different amounts of advice, including all combinations of (1) explicit instructions concerning the optimal strategy, (2) explicit instructions concerning a monetary sum to maximize, and (3) accurate information concerning the probabilities of events. The results showed minimal effects of horse race versus abstract scenario. Both advice concerning the optimal strategy and probability information contributed significantly to performance in the task. This paper includes a brief tutorial on JavaScript, explaining with simple examples how to assemble a browser-based experiment","tok_text":"web-bas experi control by javascript : an exampl from probabl learn \n javascript program can be use to control web experi . thi techniqu is illustr by an experi that test the effect of advic on perform in the classic probability-learn paradigm . previou research report that peopl test via the web or in the lab tend to match the probabl of their respons to the probabl that those respons would be reinforc . the optim strategi , howev , is to consist choos the more frequent event ; probabl match produc suboptim perform . we investig manipul we reason should improv perform . a hors race scenario in which particip predict the winner in each of a seri of race between two hors wa compar with an abstract scenario use previous . ten group of learner receiv differ amount of advic , includ all combin of ( 1 ) explicit instruct concern the optim strategi , ( 2 ) explicit instruct concern a monetari sum to maxim , and ( 3 ) accur inform concern the probabl of event . the result show minim effect of hors race versu abstract scenario . both advic concern the optim strategi and probabl inform contribut significantli to perform in the task . thi paper includ a brief tutori on javascript , explain with simpl exampl how to assembl a browser-bas experi","ordered_present_kp":[0,26,54,185,810,54,1234],"keyphrases":["Web-based experiments","JavaScript","probability learning","probability","advice","explicit instructions","browser-based experiment","Internet-based research"],"prmu":["P","P","P","P","P","P","P","M"]}
{"id":"363","title":"Synthetic simultaneity - natural and artificial","abstract":"In control loops, each element introduces time delays. If those time delays are larger than the critical times for control of the system, a problem exists. I show a simple approach to mitigating this problem by basing the controller's decisions not on the observations themselves but on our projections as to what the observations will be at the time our controls reach the controlled system. Finally, I argue that synthetic simultaneity explains Libet's (1993) results better than Libet's explanation","tok_text":"synthet simultan - natur and artifici \n in control loop , each element introduc time delay . if those time delay are larger than the critic time for control of the system , a problem exist . i show a simpl approach to mitig thi problem by base the control 's decis not on the observ themselv but on our project as to what the observ will be at the time our control reach the control system . final , i argu that synthet simultan explain libet 's ( 1993 ) result better than libet 's explan","ordered_present_kp":[43,80,133,276,0],"keyphrases":["synthetic simultaneity","control loops","time delays","critical times","observations","controller decisions"],"prmu":["P","P","P","P","P","R"]}
{"id":"2029","title":"Block truncation image bit plane coding","abstract":"Block truncation coding (BTC) is a successful image compression technique due to its simple and fast computational burden. The bit rate is fixed to 2.0 bits\/pixel, whose performance is moderate in terms of compression ratio compared to other compression schemes such as discrete cosine transform (DCT), vector quantization (VQ), wavelet transform coding (WTC), etc. Two kinds of overheads are required for BTC coding: bit plane and quantization values, respectively. A new technique is presented to reduce the bit plane overhead. Conventional bit plane overhead is 1.0 bits\/pixel; we decrease it to 0.734 bits\/pixel while maintaining the same decoded quality as absolute moment BTC (AMBTC) does for the \"Lena\" image. Compared to other published bit plane coding strategies, the proposed method outperforms all of the existing methods","tok_text":"block truncat imag bit plane code \n block truncat code ( btc ) is a success imag compress techniqu due to it simpl and fast comput burden . the bit rate is fix to 2.0 bit \/ pixel , whose perform is moder in term of compress ratio compar to other compress scheme such as discret cosin transform ( dct ) , vector quantiz ( vq ) , wavelet transform code ( wtc ) , etc . two kind of overhead are requir for btc code : bit plane and quantiz valu , respect . a new techniqu is present to reduc the bit plane overhead . convent bit plane overhead is 1.0 bit \/ pixel ; we decreas it to 0.734 bit \/ pixel while maintain the same decod qualiti as absolut moment btc ( ambtc ) doe for the \" lena \" imag . compar to other publish bit plane code strategi , the propos method outperform all of the exist method","ordered_present_kp":[14,36,76,144,187,215,492,620,637,658,428],"keyphrases":["image bit plane coding","block truncation coding","image compression technique","bit rate","performance","compression ratio","quantization values","bit plane overhead","decoded quality","absolute moment BTC","AMBTC","Lena image"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"1961","title":"The influence of tollbooths on highway traffic","abstract":"We study the effects of tollbooths on the traffic flow. The highway traffic is simulated by the Nagel-Schreckenberg model. Various types of toll collection are examined, which can be characterized either by a waiting time or a reduced speed. A first-order phase transition is observed. The phase separation results a saturated flow, which is observed as a plateau region in the fundamental diagram. The effects of lane expansion near the tollbooth are examined. The full capacity of a highway can be restored. The emergence of vehicle queuing is studied. Besides the numerical results, we also obtain analytical expressions for various quantities. The numerical simulations can be well described by the analytical formulas. We also discuss the influence on the travel time and its variance. The tollbooth increases the travel time but decreases its variance. The differences between long- and short-distance travelers are also discussed","tok_text":"the influenc of tollbooth on highway traffic \n we studi the effect of tollbooth on the traffic flow . the highway traffic is simul by the nagel-schreckenberg model . variou type of toll collect are examin , which can be character either by a wait time or a reduc speed . a first-ord phase transit is observ . the phase separ result a satur flow , which is observ as a plateau region in the fundament diagram . the effect of lane expans near the tollbooth are examin . the full capac of a highway can be restor . the emerg of vehicl queu is studi . besid the numer result , we also obtain analyt express for variou quantiti . the numer simul can be well describ by the analyt formula . we also discuss the influenc on the travel time and it varianc . the tollbooth increas the travel time but decreas it varianc . the differ between long- and short-dist travel are also discuss","ordered_present_kp":[29,16,138,181,242,257,273,334,424,525,629],"keyphrases":["tollbooths","highway traffic","Nagel-Schreckenberg model","toll collection","waiting time","reduced speed","first-order phase transition","saturated flow","lane expansion","vehicle queuing","numerical simulations"],"prmu":["P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"2110","title":"Psychology and the Internet","abstract":"This article presents an overview of the way that the Internet is being used to assist psychological research and mediate psychological practice. It shows how psychologists are using the Internet to examine the interactions between people and computers, and highlights some of the ways that this research is important to the design and development of useable and acceptable computer systems. In particular, this introduction reviews the research presented at the International Conference on Psychology and the Internet held in the United Kingdom. The final part introduces the eight articles in this special edition. The articles are representative of the breadth of research being conducted on psychology and the Internet: there are two on methodological issues, three on group processes, one on organizational implications, and two on social implications of Internet use","tok_text":"psycholog and the internet \n thi articl present an overview of the way that the internet is be use to assist psycholog research and mediat psycholog practic . it show how psychologist are use the internet to examin the interact between peopl and comput , and highlight some of the way that thi research is import to the design and develop of useabl and accept comput system . in particular , thi introduct review the research present at the intern confer on psycholog and the internet held in the unit kingdom . the final part introduc the eight articl in thi special edit . the articl are repres of the breadth of research be conduct on psycholog and the internet : there are two on methodolog issu , three on group process , one on organiz implic , and two on social implic of internet use","ordered_present_kp":[18,109,762,0,734,711,684],"keyphrases":["psychology","Internet","psychological research","methodological issues","group processes","organizational implications","social implications","human-computer interactions","usability","online research"],"prmu":["P","P","P","P","P","P","P","M","U","M"]}
{"id":"2048","title":"An object-oriented version of SIMLIB (a simple simulation package)","abstract":"This paper introduces an object-oriented version of SIMLIB (an easy-to-understand discrete-event simulation package). The object-oriented version is preferable to the original procedural language versions of SIMLIB in that it is easier to understand and teach simulation from an object point of view. A single-server queue simulation is demonstrated using the object-oriented SIMLIB","tok_text":"an object-ori version of simlib ( a simpl simul packag ) \n thi paper introduc an object-ori version of simlib ( an easy-to-understand discrete-ev simul packag ) . the object-ori version is prefer to the origin procedur languag version of simlib in that it is easier to understand and teach simul from an object point of view . a single-serv queue simul is demonstr use the object-ori simlib","ordered_present_kp":[3,25,134,284],"keyphrases":["object-oriented version","SIMLIB","discrete-event simulation","teach simulation"],"prmu":["P","P","P","P"]}
{"id":"302","title":"Using Internet search engines to estimate word frequency","abstract":"The present research investigated Internet search engines as a rapid, cost-effective alternative for estimating word frequencies. Frequency estimates for 382 words were obtained and compared across four methods: (1) Internet search engines, (2) the Kucera and Francis (1967) analysis of a traditional linguistic corpus, (3) the CELEX English linguistic database (Baayen et al., 1995), and (4) participant ratings of familiarity. The results showed that Internet search engines produced frequency estimates that were highly consistent with those reported by Kucera and Francis and those calculated from CELEX, highly consistent across search engines, and very reliable over a 6 month period of time. Additional results suggested that Internet search engines are an excellent option when traditional word frequency analyses do not contain the necessary data (e.g., estimates for forenames and slang). In contrast, participants' familiarity judgments did not correspond well with the more objective estimates of word frequency. Researchers are advised to use search engines with large databases (e.g., AltaVista) to ensure the greatest representativeness of the frequency estimates","tok_text":"use internet search engin to estim word frequenc \n the present research investig internet search engin as a rapid , cost-effect altern for estim word frequenc . frequenc estim for 382 word were obtain and compar across four method : ( 1 ) internet search engin , ( 2 ) the kucera and franci ( 1967 ) analysi of a tradit linguist corpu , ( 3 ) the celex english linguist databas ( baayen et al . , 1995 ) , and ( 4 ) particip rate of familiar . the result show that internet search engin produc frequenc estim that were highli consist with those report by kucera and franci and those calcul from celex , highli consist across search engin , and veri reliabl over a 6 month period of time . addit result suggest that internet search engin are an excel option when tradit word frequenc analys do not contain the necessari data ( e.g. , estim for forenam and slang ) . in contrast , particip ' familiar judgment did not correspond well with the more object estim of word frequenc . research are advis to use search engin with larg databas ( e.g. , altavista ) to ensur the greatest repres of the frequenc estim","ordered_present_kp":[4,320,347,1022],"keyphrases":["Internet search engines","linguistic corpus","CELEX English linguistic database","large databases","word frequency estimation","participant familiarity ratings"],"prmu":["P","P","P","P","R","R"]}
{"id":"347","title":"From revenue management concepts to software systems","abstract":"In 1999, after developing and installing over 170 revenue management (RM) systems for more than 70 airlines, PROS Revenue Management, Inc. had the opportunity to develop RM systems for three companies in nonairline industries. PROS research and design department designed the opportunity analysis study (OAS), a mix of OR\/MS, consulting, and software development practices to determine the applicability of RM in new business situations. PROS executed OASs with the three companies. In all three cases, the OAS supported the value of RM and led to contracts for implementation of RM systems","tok_text":"from revenu manag concept to softwar system \n in 1999 , after develop and instal over 170 revenu manag ( rm ) system for more than 70 airlin , pro revenu manag , inc. had the opportun to develop rm system for three compani in nonairlin industri . pro research and design depart design the opportun analysi studi ( oa ) , a mix of or \/ ms , consult , and softwar develop practic to determin the applic of rm in new busi situat . pro execut oass with the three compani . in all three case , the oa support the valu of rm and led to contract for implement of rm system","ordered_present_kp":[5,29,195,143,289,314,330,354],"keyphrases":["revenue management concepts","software systems","PROS Revenue Management","RM systems","opportunity analysis study","OAS","OR\/MS","software development practices","Inc","consulting practices"],"prmu":["P","P","P","P","P","P","P","P","U","R"]}
{"id":"412","title":"Using virtual reality to teach disability awareness","abstract":"A desktop virtual reality (VR) program was designed and evaluated to teach children about the accessibility and attitudinal barriers encountered by their peers with mobility impairments. Within this software, children sitting in a virtual wheelchair experience obstacles such as stairs, narrow doors, objects too high to reach, and attitudinal barriers such as inappropriate comments. Using a collaborative research methodology, 15 youth with mobility impairments assisted in developing and beta-testing the software. The effectiveness of the program was then evaluated with 60 children in Grades 4-6 using a controlled pretest\/posttest design. The results indicated that the program was effective for increasing children's knowledge of accessibility barriers. Attitudes, grade level, familiarity with individuals with a disability, and gender were also investigated","tok_text":"use virtual realiti to teach disabl awar \n a desktop virtual realiti ( vr ) program wa design and evalu to teach children about the access and attitudin barrier encount by their peer with mobil impair . within thi softwar , children sit in a virtual wheelchair experi obstacl such as stair , narrow door , object too high to reach , and attitudin barrier such as inappropri comment . use a collabor research methodolog , 15 youth with mobil impair assist in develop and beta-test the softwar . the effect of the program wa then evalu with 60 children in grade 4 - 6 use a control pretest \/ posttest design . the result indic that the program wa effect for increas children 's knowledg of access barrier . attitud , grade level , familiar with individu with a disabl , and gender were also investig","ordered_present_kp":[4,113,132,242,390,188,772],"keyphrases":["virtual reality","children","accessibility","mobility impairments","virtual wheelchair","collaborative research methodology","gender","disability awareness teaching","software beta-testing","collaborative software development","computer aided instruction","software effectiveness"],"prmu":["P","P","P","P","P","P","P","R","R","R","U","R"]}
{"id":"2088","title":"Layer-based machining: recent development and support structure design","abstract":"There is growing interest in additive and subtractive shaping theories that are synthesized to integrate the layered manufacturing process and material removal process. Layer-based machining has emerged as a promising method for integrated additive and subtractive shaping theory. In the paper, major layer-based machining systems are reviewed and compared according to characteristics of stock layers, numerical control machining configurations, stacking operations, input format and raw materials. Support structure, a major issue in machining-based systems which has seldom been addressed in previous research, is investigated in the paper with considerations of four situations: floating overhang, cantilever, vaulted overhang and ceiling. Except for the floating overhang where a support structure should not be overlooked, the necessity for support structures for the other three situations is determined by stress and deflection analysis. This is demonstrated by the machining of a large castle model","tok_text":"layer-bas machin : recent develop and support structur design \n there is grow interest in addit and subtract shape theori that are synthes to integr the layer manufactur process and materi remov process . layer-bas machin ha emerg as a promis method for integr addit and subtract shape theori . in the paper , major layer-bas machin system are review and compar accord to characterist of stock layer , numer control machin configur , stack oper , input format and raw materi . support structur , a major issu in machining-bas system which ha seldom been address in previou research , is investig in the paper with consider of four situat : float overhang , cantilev , vault overhang and ceil . except for the float overhang where a support structur should not be overlook , the necess for support structur for the other three situat is determin by stress and deflect analysi . thi is demonstr by the machin of a larg castl model","ordered_present_kp":[0,38,100,153,182,388,402,434,447,464,640,657,668,687,848,859],"keyphrases":["layer-based machining","support structure design","subtractive shaping theories","layered manufacturing process","material removal process","stock layers","numerical control machining configurations","stacking operations","input format","raw materials","floating overhang","cantilever","vaulted overhang","ceiling","stress","deflection analysis","additive shaping theories"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2030","title":"Elimination of zero-order diffraction in digital holography","abstract":"A simple method to suppress the zero-order diffraction in the reconstructed image of digital holography is presented. In this method, the Laplacian of a detected hologram is used instead of the hologram itself for numerical reconstruction by computing the discrete Fresnel integral. This method can significantly improve the image quality and give better resolution and higher accuracy of the reconstructed image. The main advantages of this method are its simplicity in experimental requirements and convenience in data processing","tok_text":"elimin of zero-ord diffract in digit holographi \n a simpl method to suppress the zero-ord diffract in the reconstruct imag of digit holographi is present . in thi method , the laplacian of a detect hologram is use instead of the hologram itself for numer reconstruct by comput the discret fresnel integr . thi method can significantli improv the imag qualiti and give better resolut and higher accuraci of the reconstruct imag . the main advantag of thi method are it simplic in experiment requir and conveni in data process","ordered_present_kp":[106,176,191,281,346,394,512,31],"keyphrases":["digital holography","reconstructed image","Laplacian","detected hologram","discrete Fresnel integral","image quality","accuracy","data processing","zero-order diffraction suppression","numerical image reconstruction","image resolution","image. processing"],"prmu":["P","P","P","P","P","P","P","P","R","R","R","R"]}
{"id":"387","title":"Design and implementation of a brain-computer interface with high transfer rates","abstract":"This paper presents a brain-computer interface (BCI) that can help users to input phone numbers. The system is based on the steady-state visual evoked potential (SSVEP). Twelve buttons illuminated at different rates were displayed on a computer monitor. The buttons constituted a virtual telephone keypad, representing the ten digits 0-9, BACKSPACE, and ENTER. Users could input phone number by gazing at these buttons. The frequency-coded SSVEP was used to judge which button the user desired. Eight of the thirteen subjects succeeded in ringing the mobile phone using the system. The average transfer rate over all subjects was 27.15 bits\/min. The attractive features of the system are noninvasive signal recording, little training required for use, and high information transfer rate. Approaches to improve the performance of the system are discussed","tok_text":"design and implement of a brain-comput interfac with high transfer rate \n thi paper present a brain-comput interfac ( bci ) that can help user to input phone number . the system is base on the steady-st visual evok potenti ( ssvep ) . twelv button illumin at differ rate were display on a comput monitor . the button constitut a virtual telephon keypad , repres the ten digit 0 - 9 , backspac , and enter . user could input phone number by gaze at these button . the frequency-cod ssvep wa use to judg which button the user desir . eight of the thirteen subject succeed in ring the mobil phone use the system . the averag transfer rate over all subject wa 27.15 bit \/ min . the attract featur of the system are noninvas signal record , littl train requir for use , and high inform transfer rate . approach to improv the perform of the system are discuss","ordered_present_kp":[26,193,329,467,289],"keyphrases":["brain-computer interface with high transfer rates","steady-state visual evoked potential","computer monitor","virtual telephone keypad","frequency-coded SSVEP","phone numbers input","illuminated buttons","system performance improvement","mobile phone ringing"],"prmu":["P","P","P","P","P","R","R","R","R"]}
{"id":"2168","title":"Cyberobscenity and the ambit of English criminal law","abstract":"The author looks at a recent case and questions the Court of Appeal's approach. In the author's submission, the Court of Appeal's decision in Perrin was wrong. P published no material in England and Wales, and should not have been convicted of any offence under English law, even if it were proved that he sought to attract English subscribers to his site. That may be an unpalatable conclusion but, if the content of foreign-hosted Internet sites is to be controlled, the only sensible way forward is through international agreement and cooperation. The Council of Europe's Cybercrime Convention provides some indication of the limited areas over which widespread international agreement might be achieved","tok_text":"cyberobscen and the ambit of english crimin law \n the author look at a recent case and question the court of appeal 's approach . in the author 's submiss , the court of appeal 's decis in perrin wa wrong . p publish no materi in england and wale , and should not have been convict of ani offenc under english law , even if it were prove that he sought to attract english subscrib to hi site . that may be an unpalat conclus but , if the content of foreign-host internet site is to be control , the onli sensibl way forward is through intern agreement and cooper . the council of europ 's cybercrim convent provid some indic of the limit area over which widespread intern agreement might be achiev","ordered_present_kp":[0,37,100,462,589,569,535,230],"keyphrases":["cyberobscenity","criminal law","Court of Appeal","England","Internet sites","international agreement","Council of Europe","Cybercrime Convention"],"prmu":["P","P","P","P","P","P","P","P"]}
{"id":"34","title":"Design of PID-type controllers using multiobjective genetic algorithms","abstract":"The design of a PID controller is a multiobjective problem. A plant and a set of specifications to be satisfied are given. The designer has to adjust the parameters of the PID controller such that the feedback interconnection of the plant and the controller satisfies the specifications. These specifications are usually competitive and any acceptable solution requires a tradeoff among them. An approach for adjusting the parameters of a PID controller based on multiobjective optimization and genetic algorithms is presented in this paper. The MRCD (multiobjective robust control design) genetic algorithm has been employed. The approach can be easily generalized to design multivariable coupled and decentralized PID loops and has been successfully validated for a large number of experimental cases","tok_text":"design of pid-typ control use multiobject genet algorithm \n the design of a pid control is a multiobject problem . a plant and a set of specif to be satisfi are given . the design ha to adjust the paramet of the pid control such that the feedback interconnect of the plant and the control satisfi the specif . these specif are usual competit and ani accept solut requir a tradeoff among them . an approach for adjust the paramet of a pid control base on multiobject optim and genet algorithm is present in thi paper . the mrcd ( multiobject robust control design ) genet algorithm ha been employ . the approach can be easili gener to design multivari coupl and decentr pid loop and ha been success valid for a larg number of experiment case","ordered_present_kp":[10,30,238,529,661],"keyphrases":["PID-type controllers","multiobjective genetic algorithms","feedback interconnection","multiobjective robust control design","decentralized PID loops","multivariable coupled PID loops","tuning methods"],"prmu":["P","P","P","P","P","R","U"]}
{"id":"267","title":"An efficient parallel algorithm for the calculation of canonical MP2 energies","abstract":"We present the parallel version of a previous serial algorithm for the efficient calculation of canonical MP2 energies. It is based on the Saebo-Almlof direct-integral transformation, coupled with an efficient prescreening of the AO integrals. The parallel algorithm avoids synchronization delays by spawning a second set of slaves during the bin-sort prior to the second half-transformation. Results are presented for systems with up to 2000 basis functions. MP2 energies for molecules with 400-500 basis functions can be routinely calculated to microhartree accuracy on a small number of processors (6-8) in a matter of minutes with modern PC-based parallel computers","tok_text":"an effici parallel algorithm for the calcul of canon mp2 energi \n we present the parallel version of a previou serial algorithm for the effici calcul of canon mp2 energi . it is base on the saebo-almlof direct-integr transform , coupl with an effici prescreen of the ao integr . the parallel algorithm avoid synchron delay by spawn a second set of slave dure the bin-sort prior to the second half-transform . result are present for system with up to 2000 basi function . mp2 energi for molecul with 400 - 500 basi function can be routin calcul to microhartre accuraci on a small number of processor ( 6 - 8) in a matter of minut with modern pc-base parallel comput","ordered_present_kp":[10,47,190,267,308,385,455,53,547,641],"keyphrases":["parallel algorithm","canonical MP2 energies","MP2 energies","Saebo-Almlof direct-integral transformation","AO integrals","synchronization delays","second half-transformation","basis functions","microhartree accuracy","PC-based parallel computers"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"2195","title":"Modeling privacy control in context-aware systems","abstract":"Significant complexity issues challenge designers of context-aware systems with privacy control. Information spaces provide a way to organize information, resources, and services around important privacy-relevant contextual factors. In this article, we describe a theoretical model for privacy control in context-aware systems based on a core abstraction of information spaces. We have previously focused on deriving socially based privacy objectives in pervasive computing environments. Building on Ravi Sandhu's four-layer OM-AM (objectives, models, architectures, and mechanisms) idea, we aim to use information spaces to construct a model for privacy control that supports our socially based privacy objectives. We also discuss how we can introduce decentralization, a desirable property for many pervasive computing systems, into our information space model, using unified privacy tagging","tok_text":"model privaci control in context-awar system \n signific complex issu challeng design of context-awar system with privaci control . inform space provid a way to organ inform , resourc , and servic around import privacy-relev contextu factor . in thi articl , we describ a theoret model for privaci control in context-awar system base on a core abstract of inform space . we have previous focus on deriv social base privaci object in pervas comput environ . build on ravi sandhu 's four-lay om-am ( object , model , architectur , and mechan ) idea , we aim to use inform space to construct a model for privaci control that support our social base privaci object . we also discuss how we can introduc decentr , a desir properti for mani pervas comput system , into our inform space model , use unifi privaci tag","ordered_present_kp":[6,432,25,6],"keyphrases":["privacy","privacy control","context-aware systems","pervasive computing","smart office"],"prmu":["P","P","P","P","U"]}
{"id":"306","title":"Measuring keyboard response delays by comparing keyboard and joystick inputs","abstract":"The response characteristics of PC keyboards have to be identified when they are used as response devices in psychological experiments. In the past, the proposed method has been to check the characteristics independently by means of external measurement equipment. However, with the availability of different PC models and the rapid pace of model change, there is an urgent need for the development of convenient and accurate methods of checking. The method proposed consists of raising the precision of the PC's clock to the microsecond level and using a joystick connected to the MIDI terminal of a sound board to give the PC an independent timing function. Statistical processing of the data provided by this method makes it possible to estimate accurately the keyboard scanning interval time and the average keyboard delay time. The results showed that measured keyboard delay times varied from 11 to 73 msec, depending on the keyboard model, with most values being less than 30 msec","tok_text":"measur keyboard respons delay by compar keyboard and joystick input \n the respons characterist of pc keyboard have to be identifi when they are use as respons devic in psycholog experi . in the past , the propos method ha been to check the characterist independ by mean of extern measur equip . howev , with the avail of differ pc model and the rapid pace of model chang , there is an urgent need for the develop of conveni and accur method of check . the method propos consist of rais the precis of the pc 's clock to the microsecond level and use a joystick connect to the midi termin of a sound board to give the pc an independ time function . statist process of the data provid by thi method make it possibl to estim accur the keyboard scan interv time and the averag keyboard delay time . the result show that measur keyboard delay time vari from 11 to 73 msec , depend on the keyboard model , with most valu be less than 30 msec","ordered_present_kp":[53,98,168,359,230,575,592,622,731,765],"keyphrases":["joystick inputs","PC keyboards","psychological experiments","checking","model change","MIDI terminal","sound board","independent timing function","keyboard scanning interval time","average keyboard delay time","keyboard response delay measurement","keyboard inputs","PC clock precision","statistical data processing"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R","R","R"]}
{"id":"343","title":"Using the Small Business Innovation Research Program to turn your ideas into products","abstract":"The US Government's Small Business Innovation Research Program helps small businesses transform new ideas into commercial products. The program provides an ideal means for businesses and universities to obtaining funding for cooperative projects. Rules and information for the program are readily available, and I will give a few helpful hints to provide guidance","tok_text":"use the small busi innov research program to turn your idea into product \n the us govern 's small busi innov research program help small busi transform new idea into commerci product . the program provid an ideal mean for busi and univers to obtain fund for cooper project . rule and inform for the program are readili avail , and i will give a few help hint to provid guidanc","ordered_present_kp":[8,14,231,249,258,79],"keyphrases":["Small Business Innovation Research Program","businesses","US Government","universities","funding","cooperative projects","commercial product development","USA"],"prmu":["P","P","P","P","P","P","M","U"]}
{"id":"2009","title":"A survey of interactive mesh-cutting techniques and a new method for implementing generalized interactive mesh cutting using virtual tools","abstract":"In our experience, mesh-cutting methods can be distinguished by how their solutions address the following major issues: definition of the cut path, primitive removal and re-meshing, number of new primitives created, when re-meshing is performed, and representation of the cutting tool. Many researchers have developed schemes for interactive mesh cutting with the goals of reducing the number of new primitives created, creating new primitives with good aspect ratios, avoiding a disconnected mesh structure between primitives in the cut path, and representing the path traversed by the tool as accurately as possible. The goal of this paper is to explain how, by using a very simple framework, one can build a generalized cutting scheme. This method allows for any arbitrary cut to be made within a virtual object, and can simulate cutting surface, layered surface or tetrahedral objects using a virtual scalpel, scissors, or loop cautery tool. This method has been implemented in a real-time, haptic-rate surgical simulation system allowing arbitrary cuts to be made on high-resolution patient-specific models","tok_text":"a survey of interact mesh-cut techniqu and a new method for implement gener interact mesh cut use virtual tool \n in our experi , mesh-cut method can be distinguish by how their solut address the follow major issu : definit of the cut path , primit remov and re-mesh , number of new primit creat , when re-mesh is perform , and represent of the cut tool . mani research have develop scheme for interact mesh cut with the goal of reduc the number of new primit creat , creat new primit with good aspect ratio , avoid a disconnect mesh structur between primit in the cut path , and repres the path travers by the tool as accur as possibl . the goal of thi paper is to explain how , by use a veri simpl framework , one can build a gener cut scheme . thi method allow for ani arbitrari cut to be made within a virtual object , and can simul cut surfac , layer surfac or tetrahedr object use a virtual scalpel , scissor , or loop cauteri tool . thi method ha been implement in a real-tim , haptic-r surgic simul system allow arbitrari cut to be made on high-resolut patient-specif model","ordered_present_kp":[70,98,258,344,517,805,865,849,984,1047],"keyphrases":["generalized interactive mesh cutting","virtual tools","re-meshing","cutting tool","disconnected mesh structure","virtual object","layered surface","tetrahedral objects","haptic-rate surgical simulation system","high-resolution patient-specific models","cut path definition","real-time system","rendering","haptic interfaces"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R","U","U"]}
{"id":"2114","title":"Automation of the recovery of efficiency of complex structure systems","abstract":"Basic features are set forth of the method for automation of the serviceability recovery of systems of complex structures in real time without the interruption of operation. Specific features of the method are revealed in an important example of the system of control of hardware components of ships","tok_text":"autom of the recoveri of effici of complex structur system \n basic featur are set forth of the method for autom of the servic recoveri of system of complex structur in real time without the interrupt of oper . specif featur of the method are reveal in an import exampl of the system of control of hardwar compon of ship","ordered_present_kp":[119,35,315,297],"keyphrases":["complex structure systems","serviceability recovery","hardware components","ships","efficiency recovery"],"prmu":["P","P","P","P","R"]}
{"id":"2151","title":"Industrial\/sup IT\/ for performance buildings","abstract":"ABB has taken a close look at how buildings are used and has come up with a radical solution for the technical infrastructure that places the end-user's processes at the center and integrates all the building's systems around their needs. The new solution is based on the realization that tasks like setting up an office meeting, registering a hotel guest or moving a patient in a hospital, can all benefit from the same Industrial IT concepts employed by ABB to optimize manufacturing, for example in the automotive industry","tok_text":"industri \/ sup it\/ for perform build \n abb ha taken a close look at how build are use and ha come up with a radic solut for the technic infrastructur that place the end-us 's process at the center and integr all the build 's system around their need . the new solut is base on the realiz that task like set up an offic meet , regist a hotel guest or move a patient in a hospit , can all benefit from the same industri it concept employ by abb to optim manufactur , for exampl in the automot industri","ordered_present_kp":[0,39,128,409],"keyphrases":["Industrial\/sup IT\/","ABB","technical infrastructure","industrial IT concepts","building management system","building systems integration"],"prmu":["P","P","P","P","M","R"]}
{"id":"1998","title":"A friction compensator for pneumatic control valves","abstract":"A procedure that compensates for static friction (stiction) in pneumatic control valves is presented. The compensation is obtained by adding pulses to the control signal. The characteristics of the pulses are determined from the control action. The compensator is implemented in industrial controllers and control systems, and the industrial experiences show that the procedure reduces the control error during stick-slip motion significantly compared to standard control without stiction compensation","tok_text":"a friction compens for pneumat control valv \n a procedur that compens for static friction ( stiction ) in pneumat control valv is present . the compens is obtain by ad puls to the control signal . the characterist of the puls are determin from the control action . the compens is implement in industri control and control system , and the industri experi show that the procedur reduc the control error dure stick-slip motion significantli compar to standard control without stiction compens","ordered_present_kp":[2,23,474,293,407,449],"keyphrases":["friction compensator","pneumatic control valves","industrial controllers","stick-slip motion","standard control","stiction compensation","static friction compensation","control error reduction"],"prmu":["P","P","P","P","P","P","R","M"]}
{"id":"1965","title":"Stock market dynamics","abstract":"We elucidate on several empirical statistical observations of stock market returns. Moreover, we find that these properties are recurrent and are also present in invariant measures of low-dimensional dynamical systems. Thus, we propose that the returns are modeled by the first Poincare return time of a low-dimensional chaotic trajectory. This modeling, which captures the recurrent properties of the return fluctuations, is able to predict well the evolution of the observed statistical quantities. In addition, it explains the reason for which stocks present simultaneously dynamical properties and high uncertainties. In our analysis, we use data from the S&P 500 index and the Brazilian stock Telebras","tok_text":"stock market dynam \n we elucid on sever empir statist observ of stock market return . moreov , we find that these properti are recurr and are also present in invari measur of low-dimension dynam system . thu , we propos that the return are model by the first poincar return time of a low-dimension chaotic trajectori . thi model , which captur the recurr properti of the return fluctuat , is abl to predict well the evolut of the observ statist quantiti . in addit , it explain the reason for which stock present simultan dynam properti and high uncertainti . in our analysi , we use data from the s&p 500 index and the brazilian stock telebra","ordered_present_kp":[64,40,158,175,253,284,437,620],"keyphrases":["empirical statistical observations","stock market returns","invariant measures","low-dimensional dynamical systems","first Poincare return time","low-dimensional chaotic trajectory","statistical quantities","Brazilian stock","econophysics"],"prmu":["P","P","P","P","P","P","P","P","U"]}
{"id":"1958","title":"Option pricing from path integral for non-Gaussian fluctuations. Natural martingale and application to truncated Levy distributions","abstract":"Within a path integral formalism for non-Gaussian price fluctuations, we set up a simple stochastic calculus and derive a natural martingale for option pricing from the wealth balance of options, stocks, and bonds. The resulting formula is evaluated for truncated Levy distributions","tok_text":"option price from path integr for non-gaussian fluctuat . natur martingal and applic to truncat levi distribut \n within a path integr formal for non-gaussian price fluctuat , we set up a simpl stochast calculu and deriv a natur martingal for option price from the wealth balanc of option , stock , and bond . the result formula is evalu for truncat levi distribut","ordered_present_kp":[0,18,193,290,302,58,88],"keyphrases":["option pricing","path integrals","natural martingale","truncated Levy distributions","stochastic calculus","stocks","bonds","nonGaussian fluctuations"],"prmu":["P","P","P","P","P","P","P","M"]}
{"id":"263","title":"K-12 instruction and digital access to archival materials","abstract":"Providing K-12 schools with digital access to archival materials can strengthen both student learning and archival practice, although it cannot replace direct physical access to records. The article compares a variety of electronic and nonelectronic projects to promote teaching with primary source materials. The article also examines some of the different historiographical and pedagogical approaches used in archival Web sites geared for K-12 instruction, focusing on differences between the educational sites sponsored by the Library of Congress and the National Archives and Records Administration","tok_text":"k-12 instruct and digit access to archiv materi \n provid k-12 school with digit access to archiv materi can strengthen both student learn and archiv practic , although it can not replac direct physic access to record . the articl compar a varieti of electron and nonelectron project to promot teach with primari sourc materi . the articl also examin some of the differ historiograph and pedagog approach use in archiv web site gear for k-12 instruct , focus on differ between the educ site sponsor by the librari of congress and the nation archiv and record administr","ordered_present_kp":[0,18,34,124,142,263,304,186,387,411,480,505,533],"keyphrases":["K-12 instruction","digital access","archival materials","student learning","archival practice","direct physical access","nonelectronic projects","primary source materials","pedagogical approaches","archival Web","educational sites","Library of Congress","National Archives and Records Administration","electronic projects","historiographical approaches"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2191","title":"The role of speech input in wearable computing","abstract":"Speech recognition seems like an attractive input mechanism for wearable computers, and as we saw in this magazine's first issue, several companies are promoting products that use limited speech interfaces for specific tasks. However, we must overcome several challenges to using speech recognition in more general contexts, and interface designers must be wary of applying the technology to situations where speech is inappropriate","tok_text":"the role of speech input in wearabl comput \n speech recognit seem like an attract input mechan for wearabl comput , and as we saw in thi magazin 's first issu , sever compani are promot product that use limit speech interfac for specif task . howev , we must overcom sever challeng to use speech recognit in more gener context , and interfac design must be wari of appli the technolog to situat where speech is inappropri","ordered_present_kp":[28,12,45,28,209],"keyphrases":["speech input","wearable computing","wearable computing","speech recognition","speech interfaces","wearable computer","speech recognizers","mobile speech recognition","background noise"],"prmu":["P","P","P","P","P","P","M","M","U"]}
{"id":"226","title":"Online masquerade: whose e-mail is it?","abstract":"E-mails carrying viruses like the recent Klez worm use deceptively simple techniques and known vulnerabilities to spread from one computer to another with ease","tok_text":"onlin masquerad : whose e-mail is it ? \n e-mail carri virus like the recent klez worm use decept simpl techniqu and known vulner to spread from one comput to anoth with eas","ordered_present_kp":[24,76,54,122],"keyphrases":["e-mail","viruses","Klez worm","vulnerabilities"],"prmu":["P","P","P","P"]}
{"id":"30","title":"Improvements and critique on Sugeno's and Yasukawa's qualitative modeling","abstract":"Investigates Sugeno's and Yasukawa's (1993) qualitative fuzzy modeling approach. We propose some easily implementable solutions for the unclear details of the original paper, such as trapezoid approximation of membership functions, rule creation from sample data points, and selection of important variables. We further suggest an improved parameter identification algorithm to be applied instead of the original one. These details are crucial concerning the method's performance as it is shown in a comparative analysis and helps to improve the accuracy of the built-up model. Finally, we propose a possible further rule base reduction which can be applied successfully in certain cases. This improvement reduces the time requirement of the method by up to 16% in our experiments","tok_text":"improv and critiqu on sugeno 's and yasukawa 's qualit model \n investig sugeno 's and yasukawa 's ( 1993 ) qualit fuzzi model approach . we propos some easili implement solut for the unclear detail of the origin paper , such as trapezoid approxim of membership function , rule creation from sampl data point , and select of import variabl . we further suggest an improv paramet identif algorithm to be appli instead of the origin one . these detail are crucial concern the method 's perform as it is shown in a compar analysi and help to improv the accuraci of the built-up model . final , we propos a possibl further rule base reduct which can be appli success in certain case . thi improv reduc the time requir of the method by up to 16 % in our experi","ordered_present_kp":[48,114,228,250,272,370,618],"keyphrases":["qualitative modeling","fuzzy modeling","trapezoid approximation","membership functions","rule creation","parameter identification algorithm","rule base reduction","Sugeno-Yasukawa method"],"prmu":["P","P","P","P","P","P","P","M"]}
{"id":"2129","title":"A universal decomposition of the integration range for exponential functions","abstract":"The problem of determining the independent constants for decomposition of the integration range of exponential functions was solved on the basis of a similar approach to polynomials. The constants obtained enable one to decompose the integration range in two so that the integrals over them are equal independently of the function parameters. For the nontrigonometrical polynomials of even functions, an alternative approach was presented","tok_text":"a univers decomposit of the integr rang for exponenti function \n the problem of determin the independ constant for decomposit of the integr rang of exponenti function wa solv on the basi of a similar approach to polynomi . the constant obtain enabl one to decompos the integr rang in two so that the integr over them are equal independ of the function paramet . for the nontrigonometr polynomi of even function , an altern approach wa present","ordered_present_kp":[44,212,370,397],"keyphrases":["exponential functions","polynomials","nontrigonometrical polynomials","even functions","integration range universal decomposition","integration range decomposition"],"prmu":["P","P","P","P","R","R"]}
{"id":"383","title":"Outlier resistant adaptive matched filtering","abstract":"Robust adaptive matched filtering (AMF) whereby outlier data vectors are censored from the covariance matrix estimate is considered in a maximum likelihood estimation (MLE) setting. It is known that outlier data vectors whose steering vector is highly correlated with the desired steering vector, can significantly degrade the performance of AMF algorithms such as sample matrix inversion (SMI) or fast maximum likelihood (FML). Four new algorithms that censor outliers are presented which are derived via approximation to the MLE solution. Two algorithms each are related to using the SMI or the FML to estimate the unknown underlying covariance matrix. Results are presented using computer simulations which demonstrate the relative effectiveness of the four algorithms versus each other and also versus the SMI and FML algorithms in the presence of outliers and no outliers. It is shown that one of the censoring algorithms, called the reiterative censored fast maximum likelihood (CFML) technique is significantly superior to the other three censoring methods in stressful outlier scenarios","tok_text":"outlier resist adapt match filter \n robust adapt match filter ( amf ) wherebi outlier data vector are censor from the covari matrix estim is consid in a maximum likelihood estim ( mle ) set . it is known that outlier data vector whose steer vector is highli correl with the desir steer vector , can significantli degrad the perform of amf algorithm such as sampl matrix invers ( smi ) or fast maximum likelihood ( fml ) . four new algorithm that censor outlier are present which are deriv via approxim to the mle solut . two algorithm each are relat to use the smi or the fml to estim the unknown underli covari matrix . result are present use comput simul which demonstr the rel effect of the four algorithm versu each other and also versu the smi and fml algorithm in the presenc of outlier and no outlier . it is shown that one of the censor algorithm , call the reiter censor fast maximum likelihood ( cfml ) techniqu is significantli superior to the other three censor method in stress outlier scenario","ordered_present_kp":[0,118,235,357,388,838,866],"keyphrases":["outlier resistant adaptive matched filtering","covariance matrix estimate","steering vector","sample matrix inversion","fast maximum likelihood","censoring algorithms","reiterative censored fast maximum likelihood","maximum likelihood estimation setting"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"1981","title":"Optimal linear control in stabilizer design","abstract":"The most common method of improving stability of the power system is the synthesis of the turbine and generator control systems, because of the high effectiveness and relatively low cost of these elements. The synthesis and construction of the effective synchronous generator and turbine controller is a very difficult task. This paper describes the seven step mu -synthesis approach to PSS design enabling the synchronous generator to remain stable over a wide range of system operating conditions","tok_text":"optim linear control in stabil design \n the most common method of improv stabil of the power system is the synthesi of the turbin and gener control system , becaus of the high effect and rel low cost of these element . the synthesi and construct of the effect synchron gener and turbin control is a veri difficult task . thi paper describ the seven step mu -synthesi approach to pss design enabl the synchron gener to remain stabl over a wide rang of system oper condit","ordered_present_kp":[354,379,0],"keyphrases":["optimal linear control","mu -synthesis approach","PSS design","synchronous generator control system synthesis","turbine control system synthesis"],"prmu":["P","P","P","R","R"]}
{"id":"2148","title":"Information security policy - what do international information security standards say?","abstract":"One of the most important information security controls, is the information security policy. This vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. This results in the policy authors turning to existing sources for guidance. One of these sources is the various international information security standards. These standards are a good starting point for determining what the information security policy should consist of, but should not be relied upon exclusively for guidance. Firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. It is far more important the information security policy must fit in with the organisation's culture and must therefore be developed with this in mind","tok_text":"inform secur polici - what do intern inform secur standard say ? \n one of the most import inform secur control , is the inform secur polici . thi vital direction-giv document is , howev , not alway easi to develop and the author thereof battl with question such as what constitut a polici . thi result in the polici author turn to exist sourc for guidanc . one of these sourc is the variou intern inform secur standard . these standard are a good start point for determin what the inform secur polici should consist of , but should not be reli upon exclus for guidanc . firstli , they are not comprehens in their coverag and furthermor , tend to rather address the process need for success implement the inform secur polici . it is far more import the inform secur polici must fit in with the organis 's cultur and must therefor be develop with thi in mind","ordered_present_kp":[0,30],"keyphrases":["information security policy","international information security standards"],"prmu":["P","P"]}
{"id":"247","title":"The congenial talking philosophers problem in computer networks","abstract":"Group mutual exclusion occurs naturally in situations where a resource can be shared by processes of the same group, but not by processes of different groups. For example, suppose data is stored in a CD-jukebox. Then, when a disc is loaded for access, users that need data on the disc can concurrently access the disc, while users that need data on a different disc have to wait until the current disc is unloaded. The design issues for group mutual exclusion have been modeled as the Congenial Talking Philosophers problem, and solutions for shared memory models have been proposed (Y.-J. Young, 2000; P. Keane and M. Moir, 1999). As in ordinary mutual exclusion and many other problems in distributed systems, however, techniques developed for shared memory do not necessarily apply to message passing (and vice versa). We investigate solutions for Congenial Talking Philosophers in computer networks where processes communicate by asynchronous message passing. We first present a solution that is a straightforward adaptation from G. Ricart and A.K. Agrawala's (1981) algorithm for ordinary mutual exclusion. Then we show that the simple modification suffers a severe performance degradation that could cause the system to behave as though only one process of a group can be in the critical section at a time. We then present a more efficient and highly concurrent distributed algorithm for the problem, the first such solution in computer networks","tok_text":"the congeni talk philosoph problem in comput network \n group mutual exclus occur natur in situat where a resourc can be share by process of the same group , but not by process of differ group . for exampl , suppos data is store in a cd-jukebox . then , when a disc is load for access , user that need data on the disc can concurr access the disc , while user that need data on a differ disc have to wait until the current disc is unload . the design issu for group mutual exclus have been model as the congeni talk philosoph problem , and solut for share memori model have been propos ( y.-j. young , 2000 ; p. kean and m. moir , 1999 ) . as in ordinari mutual exclus and mani other problem in distribut system , howev , techniqu develop for share memori do not necessarili appli to messag pass ( and vice versa ) . we investig solut for congeni talk philosoph in comput network where process commun by asynchron messag pass . we first present a solut that is a straightforward adapt from g. ricart and a.k. agrawala 's ( 1981 ) algorithm for ordinari mutual exclus . then we show that the simpl modif suffer a sever perform degrad that could caus the system to behav as though onli one process of a group can be in the critic section at a time . we then present a more effici and highli concurr distribut algorithm for the problem , the first such solut in comput network","ordered_present_kp":[4,38,55,694,885,903,1220,1288],"keyphrases":["congenial talking philosophers problem","computer networks","group mutual exclusion","distributed systems","process communication","asynchronous message passing","critical section","concurrent distributed algorithm","resource sharing","shared-memory models"],"prmu":["P","P","P","P","P","P","P","P","R","M"]}
{"id":"202","title":"Estimation of error in curvature computation on multi-scale free-form surfaces","abstract":"A novel technique for multi-scale curvature computation on a free-form 3-D surface is presented. This is achieved by convolving local parametrisations of the surface with 2-D Gaussian filters iteratively. In our technique, semigeodesic coordinates are constructed at each vertex of the mesh. Smoothing results are shown for 3-D surfaces with different shapes indicating that surface noise is eliminated and surface details are removed gradually. A number of evolution properties of 3-D surfaces are described. Next, the surface Gaussian and mean curvature values are estimated accurately at multiple scales which are then mapped to colours and displayed directly on the surface. The performance of the technique when selecting different directions as an arbitrary direction for the geodesic at each vertex are also presented. The results indicate that the error observed for the estimation of Gaussian and mean curvatures is quite low after only one iteration. Furthermore, as the surface is smoothed iteratively, the error is further reduced. The results also show that the estimation error of Gaussian curvature is less than that of mean curvature. Our experiments demonstrate that estimation of smoothed surface curvatures are very accurate and not affected by the arbitrary direction of the first geodesic line when constructing semigeodesic coordinates. Our technique is independent of the underlying triangulation and is also more efficient than volumetric diffusion techniques since 2-D rather than 3-D convolutions are employed. Finally, the method presented here is a generalisation of the Curvature Scale Space method for 2-D contours. The CSS method has outperformed comparable techniques within the MPEG-7 evaluation framework. As a result, it has been selected for inclusion in the MPEG-7 package of standards","tok_text":"estim of error in curvatur comput on multi-scal free-form surfac \n a novel techniqu for multi-scal curvatur comput on a free-form 3-d surfac is present . thi is achiev by convolv local parametris of the surfac with 2-d gaussian filter iter . in our techniqu , semigeodes coordin are construct at each vertex of the mesh . smooth result are shown for 3-d surfac with differ shape indic that surfac nois is elimin and surfac detail are remov gradual . a number of evolut properti of 3-d surfac are describ . next , the surfac gaussian and mean curvatur valu are estim accur at multipl scale which are then map to colour and display directli on the surfac . the perform of the techniqu when select differ direct as an arbitrari direct for the geodes at each vertex are also present . the result indic that the error observ for the estim of gaussian and mean curvatur is quit low after onli one iter . furthermor , as the surfac is smooth iter , the error is further reduc . the result also show that the estim error of gaussian curvatur is less than that of mean curvatur . our experi demonstr that estim of smooth surfac curvatur are veri accur and not affect by the arbitrari direct of the first geodes line when construct semigeodes coordin . our techniqu is independ of the underli triangul and is also more effici than volumetr diffus techniqu sinc 2-d rather than 3-d convolut are employ . final , the method present here is a generalis of the curvatur scale space method for 2-d contour . the css method ha outperform compar techniqu within the mpeg-7 evalu framework . as a result , it ha been select for inclus in the mpeg-7 packag of standard","ordered_present_kp":[88,179,390,462,537,260,1275,1321,1371,1447,1549],"keyphrases":["multi-scale curvature computation","local parametrisations","semigeodesic coordinates","surface noise","evolution properties","mean curvature values","underlying triangulation","volumetric diffusion techniques","convolutions","Curvature Scale Space method","MPEG-7 evaluation framework","free-form 3D surface","2D Gaussian filters","surface Gaussian values"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","M","M","R"]}
{"id":"1939","title":"Compatibility of systems of linear constraints over the set of natural numbers","abstract":"Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types","tok_text":"compat of system of linear constraint over the set of natur number \n criteria of compat of a system of linear diophantin equat , strict inequ , and nonstrict inequ are consid . upper bound for compon of a minim set of solut and algorithm of construct of minim gener set of solut for all type of system are given . these criteria and the correspond algorithm for construct a minim support set of solut can be use in solv all the consid type of system and system of mix type","ordered_present_kp":[20,47,103,129,148,177,254],"keyphrases":["linear constraints","set of natural numbers","linear Diophantine equations","strict inequations","nonstrict inequations","upper bounds","minimal generating sets"],"prmu":["P","P","P","P","P","P","P"]}
{"id":"2055","title":"Causes of the decline of the business school management science course","abstract":"The business school management science course is suffering serious decline. The traditional model- and algorithm-based course fails to meet the needs of MBA programs and students. Poor student mathematical preparation is a reality, and is not an acceptable justification for poor teaching outcomes. Management science Ph.D.s are often poorly prepared to teach in a general management program, having more experience and interest in algorithms than management. The management science profession as a whole has focused its attention on algorithms and a narrow subset of management problems for which they are most applicable. In contrast, MBA's rarely encounter problems that are suitable for straightforward application of management science tools, living instead in a world where problems are ill-defined, data is scarce, time is short, politics is dominant, and rational \"decision makers\" are non-existent. The root cause of the profession's failure to address these issues seems to be (in Russell Ackoff's words) a habit of professional introversion that caused the profession to be uninterested in what MBA's really do on the job and how management science can help them","tok_text":"caus of the declin of the busi school manag scienc cours \n the busi school manag scienc cours is suffer seriou declin . the tradit model- and algorithm-bas cours fail to meet the need of mba program and student . poor student mathemat prepar is a realiti , and is not an accept justif for poor teach outcom . manag scienc ph . d. are often poorli prepar to teach in a gener manag program , have more experi and interest in algorithm than manag . the manag scienc profess as a whole ha focus it attent on algorithm and a narrow subset of manag problem for which they are most applic . in contrast , mba 's rare encount problem that are suitabl for straightforward applic of manag scienc tool , live instead in a world where problem are ill-defin , data is scarc , time is short , polit is domin , and ration \" decis maker \" are non-exist . the root caus of the profess 's failur to address these issu seem to be ( in russel ackoff 's word ) a habit of profession introvers that caus the profess to be uninterest in what mba 's realli do on the job and how manag scienc can help them","ordered_present_kp":[26,187,38,463],"keyphrases":["business school management science course","management science","MBA programs","profession","MBA students"],"prmu":["P","P","P","P","R"]}
{"id":"2010","title":"Scale-invariant segmentation of dynamic contrast-enhanced perfusion MR images with inherent scale selection","abstract":"Selection of the best set of scales is problematic when developing signal-driven approaches for pixel-based image segmentation. Often, different possibly conflicting criteria need to be fulfilled in order to obtain the best trade-off between uncertainty (variance) and location accuracy. The optimal set of scales depends on several factors: the noise level present in the image material, the prior distribution of the different types of segments, the class-conditional distributions associated with each type of segment as well as the actual size of the (connected) segments. We analyse, theoretically and through experiments, the possibility of using the overall and class-conditional error rates as criteria for selecting the optimal sampling of the linear and morphological scale spaces. It is shown that the overall error rate is optimized by taking the prior class distribution in the image material into account. However, a uniform (ignorant) prior distribution ensures constant class-conditional error rates. Consequently, we advocate for a uniform prior class distribution when an uncommitted, scale-invariant segmentation approach is desired. Experiments with a neural net classifier developed for segmentation of dynamic magnetic resonance (MR) images, acquired with a paramagnetic tracer, support the theoretical results. Furthermore, the experiments show that the addition of spatial features to the classifier, extracted from the linear or morphological scale spaces, improves the segmentation result compared to a signal-driven approach based solely on the dynamic MR signal. The segmentation results obtained from the two types of features are compared using two novel quality measures that characterize spatial properties of labelled images","tok_text":"scale-invari segment of dynam contrast-enhanc perfus mr imag with inher scale select \n select of the best set of scale is problemat when develop signal-driven approach for pixel-bas imag segment . often , differ possibl conflict criteria need to be fulfil in order to obtain the best trade-off between uncertainti ( varianc ) and locat accuraci . the optim set of scale depend on sever factor : the nois level present in the imag materi , the prior distribut of the differ type of segment , the class-condit distribut associ with each type of segment as well as the actual size of the ( connect ) segment . we analys , theoret and through experi , the possibl of use the overal and class-condit error rate as criteria for select the optim sampl of the linear and morpholog scale space . it is shown that the overal error rate is optim by take the prior class distribut in the imag materi into account . howev , a uniform ( ignor ) prior distribut ensur constant class-condit error rate . consequ , we advoc for a uniform prior class distribut when an uncommit , scale-invari segment approach is desir . experi with a neural net classifi develop for segment of dynam magnet reson ( mr ) imag , acquir with a paramagnet tracer , support the theoret result . furthermor , the experi show that the addit of spatial featur to the classifi , extract from the linear or morpholog scale space , improv the segment result compar to a signal-driven approach base sole on the dynam mr signal . the segment result obtain from the two type of featur are compar use two novel qualiti measur that character spatial properti of label imag","ordered_present_kp":[0,24,66,172,399,682,733,639,1117,1207,1562,1612,495],"keyphrases":["scale-invariant segmentation","dynamic contrast-enhanced perfusion MR images","inherent scale selection","pixel-based image segmentation","noise level","class-conditional distributions","experiments","class-conditional error rates","optimal sampling","neural net classifier","paramagnetic tracer","quality measures","labelled images","dynamic magnetic resonance images"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2068","title":"Modeling the labor market as an evolving institution: model ARTEMIS","abstract":"A stylized French labor market is modeled as an endogenously evolving institution. Boundedly rational firms and individuals strive to decrease the cost or increase utility. The labor market is coordinated by a search process and decentralized setting of hiring standards, but intermediaries can speed up matching. The model reproduces the dynamics of the gross flows and spectacular changes in mobility patterns of some demographic groups when the oil crisis in the 1970's occurred, notably the sudden decline of the integration in good jobs. The internal labor markets of large firms are shown to increase unemployment if the secondary (temporary or bad) jobs do not exist","tok_text":"model the labor market as an evolv institut : model artemi \n a styliz french labor market is model as an endogen evolv institut . boundedli ration firm and individu strive to decreas the cost or increas util . the labor market is coordin by a search process and decentr set of hire standard , but intermediari can speed up match . the model reproduc the dynam of the gross flow and spectacular chang in mobil pattern of some demograph group when the oil crisi in the 1970 's occur , notabl the sudden declin of the integr in good job . the intern labor market of larg firm are shown to increas unemploy if the secondari ( temporari or bad ) job do not exist","ordered_present_kp":[70,105,530,382,403,425],"keyphrases":["French labor market","endogenously evolving institution","spectacular changes","mobility patterns","demographic groups","jobs","ARTEMIS model","simulation model","endogenous intermediary"],"prmu":["P","P","P","P","P","P","R","M","R"]}
{"id":"2095","title":"Global stability of the attracting set of an enzyme-catalysed reaction system","abstract":"The essential feature of enzymatic reactions is a nonlinear dependency of reaction rate on metabolite concentration taking the form of saturation kinetics. Recently, it has been shown that this feature is associated with the phenomenon of \"loss of system coordination\" (Liu, 1999). In this paper, we study a system of ordinary differential equations representing a branched biochemical system of enzyme-mediated reactions. We show that this system can become very sensitive to changes in certain maximum enzyme activities. In particular, we show that the system exhibits three distinct responses: a unique, globally-stable steady-state, large amplitude oscillations, and asymptotically unbounded solutions, with the transition between these states being almost instantaneous. It is shown that the appearance of large amplitude, stable limit cycles occurs due to a \"false\" bifurcation or canard explosion. The subsequent disappearance of limit cycles corresponds to the collapse of the domain of attraction of the attracting set for the system and occurs due to a global bifurcation in the flow, namely, a saddle connection. Subsequently, almost all nonnegative data become unbounded under the action of the dynamical system and correspond exactly to loss of system coordination. We discuss the relevance of these results to the possible consequences of modulating such systems","tok_text":"global stabil of the attract set of an enzyme-catalys reaction system \n the essenti featur of enzymat reaction is a nonlinear depend of reaction rate on metabolit concentr take the form of satur kinet . recent , it ha been shown that thi featur is associ with the phenomenon of \" loss of system coordin \" ( liu , 1999 ) . in thi paper , we studi a system of ordinari differenti equat repres a branch biochem system of enzyme-medi reaction . we show that thi system can becom veri sensit to chang in certain maximum enzym activ . in particular , we show that the system exhibit three distinct respons : a uniqu , globally-st steady-st , larg amplitud oscil , and asymptot unbound solut , with the transit between these state be almost instantan . it is shown that the appear of larg amplitud , stabl limit cycl occur due to a \" fals \" bifurc or canard explos . the subsequ disappear of limit cycl correspond to the collaps of the domain of attract of the attract set for the system and occur due to a global bifurc in the flow , name , a saddl connect . subsequ , almost all nonneg data becom unbound under the action of the dynam system and correspond exactli to loss of system coordin . we discuss the relev of these result to the possibl consequ of modul such system","ordered_present_kp":[94,116,153,189,400,358,418,1037,793,834],"keyphrases":["enzymatic reactions","nonlinear dependency","metabolite concentration","saturation kinetics","ordinary differential equations","biochemical system","enzyme-mediated reactions","stable limit cycles","bifurcation","saddle connection"],"prmu":["P","P","P","P","P","P","P","P","P","P"]}
{"id":"322","title":"Toward an Experimental Timing Standards Lab: benchmarking precision in the real world","abstract":"Much discussion has taken place over the relative merits of various platforms and operating systems for real-time data collection. Most would agree that, provided great care is taken, many are capable of millisecond timing precision. However, to date, much of this work has focused on the theoretical aspects of raw performance. It is our belief that researchers would be better informed if they could place confidence limits on their own specific paradigms in situ and without modification. To this end, we have developed a millisecond precision test rig that can control and time experiments on a second presentation machine. We report on the specialist hardware and software used. We elucidate the importance of the approach in relation to real-world experimentation","tok_text":"toward an experiment time standard lab : benchmark precis in the real world \n much discuss ha taken place over the rel merit of variou platform and oper system for real-tim data collect . most would agre that , provid great care is taken , mani are capabl of millisecond time precis . howev , to date , much of thi work ha focus on the theoret aspect of raw perform . it is our belief that research would be better inform if they could place confid limit on their own specif paradigm in situ and without modif . to thi end , we have develop a millisecond precis test rig that can control and time experi on a second present machin . we report on the specialist hardwar and softwar use . we elucid the import of the approach in relat to real-world experiment","ordered_present_kp":[41,10,148,164,259],"keyphrases":["Experimental Timing Standards Lab","benchmarking precision","operating systems","real-time data collection","millisecond timing precision","performance evaluation","Event Generation software"],"prmu":["P","P","P","P","P","M","M"]}
{"id":"367","title":"A study on meaning processing of dialogue with an example of development of travel consultation system","abstract":"This paper describes an approach to processing meaning instead of processing information in computing. Human intellectual activity is supported by linguistic activities in the brain. Therefore, processing the meaning of language instead of processing information should allow us to realize human intelligence on a computer. As an example of the proposed framework for processing meaning, we build a travel consultation dialogue system which can understand utterance by a user and retrieve information through dialogue. Through a simulation example of the system, we show that both information processing and language processing are integrated","tok_text":"a studi on mean process of dialogu with an exampl of develop of travel consult system \n thi paper describ an approach to process mean instead of process inform in comput . human intellectu activ is support by linguist activ in the brain . therefor , process the mean of languag instead of process inform should allow us to realiz human intellig on a comput . as an exampl of the propos framework for process mean , we build a travel consult dialogu system which can understand utter by a user and retriev inform through dialogu . through a simul exampl of the system , we show that both inform process and languag process are integr","ordered_present_kp":[11,172,209,426,587,606],"keyphrases":["meaning processing","human intellectual activity","linguistic activities","travel consultation dialogue system","information processing","language processing","user utterance understanding","information retrieval"],"prmu":["P","P","P","P","P","P","R","R"]}
{"id":"2188","title":"A nonlinear modulation strategy for hybrid AC\/DC power systems","abstract":"A nonlinear control strategy to improve transient stability of a multi-machine AC power system with several DC links terminated in the presence of large disturbances is presented. The approach proposed in this paper is based on differential geometric theory, and the HVDC systems are taken as a variable admittance connected at the inverter or rectifier AC bus. After deriving the analytical description of the relationship between the variable admittance and active power flows of each generator, the traditional generator dynamic equations can thus be expressed with the variable admittance of HVDC systems as an additional state variable and changed to an affine form, which is suitable for global linearization method being used to determine its control variable. An important feature of the proposed method is that, the modulated DC power is an adaptive and non-linear function of AC system states, and it can be realized by local feedback and less transmitted data from, adjacent generators. The design procedure is tested on a dual-infeed hybrid AC\/DC system","tok_text":"a nonlinear modul strategi for hybrid ac \/ dc power system \n a nonlinear control strategi to improv transient stabil of a multi-machin ac power system with sever dc link termin in the presenc of larg disturb is present . the approach propos in thi paper is base on differenti geometr theori , and the hvdc system are taken as a variabl admitt connect at the invert or rectifi ac bu . after deriv the analyt descript of the relationship between the variabl admitt and activ power flow of each gener , the tradit gener dynam equat can thu be express with the variabl admitt of hvdc system as an addit state variabl and chang to an affin form , which is suitabl for global linear method be use to determin it control variabl . an import featur of the propos method is that , the modul dc power is an adapt and non-linear function of ac system state , and it can be realiz by local feedback and less transmit data from , adjac gener . the design procedur is test on a dual-infe hybrid ac \/ dc system","ordered_present_kp":[63,100,122,162,2,31,265,301,328,358,368,467,511,629,663,872,917,964],"keyphrases":["nonlinear modulation strategy","hybrid AC\/DC power systems","nonlinear control strategy","transient stability","multi-machine AC power system","DC links","differential geometric theory","HVDC systems","variable admittance","inverter","rectifier AC bus","active power flows","generator dynamic equations","affine form","global linearization method","local feedback","adjacent generators","dual-infeed hybrid AC\/DC system"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"1941","title":"Descriptological foundations of programming","abstract":"Descriptological foundations of programming are constructed. An explication of the concept of a descriptive process is given. The operations of introduction and elimination of abstraction at the level of processes are refined. An intensional concept of a bipolar function is introduced. An explication of the concept of introduction and extraction of abstraction at the bipole level is given. On this basis, a complete set of descriptological operations is constructed","tok_text":"descriptolog foundat of program \n descriptolog foundat of program are construct . an explic of the concept of a descript process is given . the oper of introduct and elimin of abstract at the level of process are refin . an intension concept of a bipolar function is introduc . an explic of the concept of introduct and extract of abstract at the bipol level is given . on thi basi , a complet set of descriptolog oper is construct","ordered_present_kp":[0,24,112,224,247,347],"keyphrases":["descriptological foundations","programming","descriptive process","intensional concept","bipolar function","bipole level"],"prmu":["P","P","P","P","P","P"]}
{"id":"2130","title":"Synchronizing experiments with linear interval systems","abstract":"Concerns generalized control problems without exact information. <P>A method of constructing a minimal synchronizing sequence for a linear interval system over the field of real numbers is developed. This problem is reduced to a system of linear inequalities","tok_text":"synchron experi with linear interv system \n concern gener control problem without exact inform . < p > a method of construct a minim synchron sequenc for a linear interv system over the field of real number is develop . thi problem is reduc to a system of linear inequ","ordered_present_kp":[0,21,195,256,52,58],"keyphrases":["synchronizing experiments","linear interval systems","generalized control problems","controllability","real numbers","linear inequalities","minimal synchronizing sequence construction"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"29","title":"Fuzzy polynomial neural networks: hybrid architectures of fuzzy modeling","abstract":"We introduce a concept of fuzzy polynomial neural networks (FPNNs), a hybrid modeling architecture combining polynomial neural networks (PNNs) and fuzzy neural networks (FNNs). The development of the FPNNs dwells on the technologies of computational intelligence (CI), namely fuzzy sets, neural networks, and genetic algorithms. The structure of the FPNN results from a synergistic usage of FNN and PNN. FNNs contribute to the formation of the premise part of the rule-based structure of the FPNN. The consequence part of the FPNN is designed using PNNs. The structure of the PNN is not fixed in advance as it usually takes place in the case of conventional neural networks, but becomes organized dynamically to meet the required approximation error. We exploit a group method of data handling (GMDH) to produce this dynamic topology of the network. The performance of the FPNN is quantified through experimentation that exploits standard data already used in fuzzy modeling. The obtained experimental results reveal that the proposed networks exhibit high accuracy and generalization capabilities in comparison to other similar fuzzy models","tok_text":"fuzzi polynomi neural network : hybrid architectur of fuzzi model \n we introduc a concept of fuzzi polynomi neural network ( fpnn ) , a hybrid model architectur combin polynomi neural network ( pnn ) and fuzzi neural network ( fnn ) . the develop of the fpnn dwell on the technolog of comput intellig ( ci ) , name fuzzi set , neural network , and genet algorithm . the structur of the fpnn result from a synergist usag of fnn and pnn . fnn contribut to the format of the premis part of the rule-bas structur of the fpnn . the consequ part of the fpnn is design use pnn . the structur of the pnn is not fix in advanc as it usual take place in the case of convent neural network , but becom organ dynam to meet the requir approxim error . we exploit a group method of data handl ( gmdh ) to produc thi dynam topolog of the network . the perform of the fpnn is quantifi through experiment that exploit standard data alreadi use in fuzzi model . the obtain experiment result reveal that the propos network exhibit high accuraci and gener capabl in comparison to other similar fuzzi model","ordered_present_kp":[0,32,54,285,315,348,751,780,801],"keyphrases":["fuzzy polynomial neural networks","hybrid architectures","fuzzy modeling","computational intelligence","fuzzy sets","genetic algorithms","group method of data handling","GMDH","dynamic topology","highly nonlinear rule-based models","fuzzy inference method","learning","standard backpropagation","membership functions","learning rates","momentum coefficients","genetic optimization"],"prmu":["P","P","P","P","P","P","P","P","P","M","M","U","M","U","U","U","M"]}
{"id":"287","title":"Loudspeaker voice-coil inductance losses: circuit models, parameter estimation, and effect on frequency response","abstract":"When the series resistance is separated and treated as a separate element, it is shown that losses in an inductor require the ratio of the flux to MMF in the core to be frequency dependent. For small-signal operation, this dependence leads to a circuit model composed of a lossless inductor and a resistor in parallel, both of which are frequency dependent. Mathematical expressions for these elements are derived under the assumption that the ratio of core flux to MMF varies as omega \/sup n-1\/, where n is a constant. A linear regression technique is described for extracting the model parameters from measured data. Experimental data are presented to justify the model for the lossy inductance of a loudspeaker voice-coil. A SPICE example is presented to illustrate the effects of voice-coil inductor losses on the frequency response of a typical driver","tok_text":"loudspeak voice-coil induct loss : circuit model , paramet estim , and effect on frequenc respons \n when the seri resist is separ and treat as a separ element , it is shown that loss in an inductor requir the ratio of the flux to mmf in the core to be frequenc depend . for small-sign oper , thi depend lead to a circuit model compos of a lossless inductor and a resistor in parallel , both of which are frequenc depend . mathemat express for these element are deriv under the assumpt that the ratio of core flux to mmf vari as omega \/sup n-1\/ , where n is a constant . a linear regress techniqu is describ for extract the model paramet from measur data . experiment data are present to justifi the model for the lossi induct of a loudspeak voice-coil . a spice exampl is present to illustr the effect of voice-coil inductor loss on the frequenc respons of a typic driver","ordered_present_kp":[0,35,51,81,109,274,572,713,756,339],"keyphrases":["loudspeaker voice-coil inductance losses","circuit models","parameter estimation","frequency response","series resistance","small-signal operation","lossless inductor","linear regression","lossy inductance","SPICE","loudspeaker driver","core flux to MMF ratio"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"2175","title":"7 key tests in choosing your Web site firm","abstract":"Most legal firms now have a Web site and are starting to evaluate the return on their investment. The paper looks at factors involved when choosing a firm to help set up or improve a Web site. (1) Look for a company that combines technical skills and business experience. (2) Look for a company that offers excellent customer service. (3) Check that the Web site firm is committed to developing and proactively updating the Web site. (4) Make sure the firm has a proven track record and a good portfolio. (5) Look for a company with both a breadth as well as depth of skills. (6) Make sure the firm can deliver work on target, in budget and to specification. (7) Ensure that you will enjoy working and feel comfortable with the Web site firm staff","tok_text":"7 key test in choos your web site firm \n most legal firm now have a web site and are start to evalu the return on their invest . the paper look at factor involv when choos a firm to help set up or improv a web site . ( 1 ) look for a compani that combin technic skill and busi experi . ( 2 ) look for a compani that offer excel custom servic . ( 3 ) check that the web site firm is commit to develop and proactiv updat the web site . ( 4 ) make sure the firm ha a proven track record and a good portfolio . ( 5 ) look for a compani with both a breadth as well as depth of skill . ( 6 ) make sure the firm can deliv work on target , in budget and to specif . ( 7 ) ensur that you will enjoy work and feel comfort with the web site firm staff","ordered_present_kp":[25,328,404,46,254,272],"keyphrases":["Web site","legal firms","technical skills","business experience","customer service","proactive updating","return on investment"],"prmu":["P","P","P","P","P","P","R"]}
{"id":"2097","title":"An algorithm to generate all spanning trees with flow","abstract":"Spanning tree enumeration in undirected graphs is an important issue and task in many problems encountered in computer network and circuit analysis. This paper discusses the spanning tree with flow for the case that there are flow requirements between each node pair. An algorithm based on minimal paths (MPs) is proposed to generate all spanning trees without flow. The proposed algorithm is a structured approach, which splits the system into structural MPs first, and also all steps in it are easy to follow","tok_text":"an algorithm to gener all span tree with flow \n span tree enumer in undirect graph is an import issu and task in mani problem encount in comput network and circuit analysi . thi paper discuss the span tree with flow for the case that there are flow requir between each node pair . an algorithm base on minim path ( mp ) is propos to gener all span tree without flow . the propos algorithm is a structur approach , which split the system into structur mp first , and also all step in it are easi to follow","ordered_present_kp":[68,26,302,156],"keyphrases":["spanning trees","undirected graphs","circuit analysis","minimal paths","computer network analysis"],"prmu":["P","P","P","P","R"]}
{"id":"320","title":"Warranty reserves for nonstationary sales processes","abstract":"Estimation of warranty costs, in the event of product failure within the warranty period, is of importance to the manufacturer. Costs associated with replacement or repair of the product are usually drawn from a warranty reserve fund created by the manufacturer. Considering a stochastic sales process, first and second moments (and thereby the variance) are derived for the manufacturer's total discounted warranty cost of a single sale for single-component items under four different warranty policies from a manufacturer's point of view. These servicing strategies represent a renewable free-replacement, nonrenewable free-replacement, renewable pro-rata, and a nonrenewable minimal-repair warranty plans. The results are extended to determine the mean and variance of total discounted warranty costs for the total sales over the life cycle of the product. Furthermore, using a normal approximation, warranty reserves necessary for a certain protection level, so that reserves are not completely depleted, are found. Results and their managerial implications are studied through an extensive example","tok_text":"warranti reserv for nonstationari sale process \n estim of warranti cost , in the event of product failur within the warranti period , is of import to the manufactur . cost associ with replac or repair of the product are usual drawn from a warranti reserv fund creat by the manufactur . consid a stochast sale process , first and second moment ( and therebi the varianc ) are deriv for the manufactur 's total discount warranti cost of a singl sale for single-compon item under four differ warranti polici from a manufactur 's point of view . these servic strategi repres a renew free-replac , nonrenew free-replac , renew pro-rata , and a nonrenew minimal-repair warranti plan . the result are extend to determin the mean and varianc of total discount warranti cost for the total sale over the life cycl of the product . furthermor , use a normal approxim , warranti reserv necessari for a certain protect level , so that reserv are not complet deplet , are found . result and their manageri implic are studi through an extens exampl","ordered_present_kp":[20,0,90,295,329,361,403,452,548,573,593,616,639,403,840,983],"keyphrases":["warranty reserves","nonstationary sales processes","product failure","stochastic sales process","second moments","variance","total discounted warranty cost","total discounted warranty cost","single-component items","servicing strategies","renewable free-replacement","nonrenewable free-replacement","renewable pro-rata","nonrenewable minimal-repair warranty plans","normal approximation","managerial implications","warranty costs estimation","product replacement","product repair","first moments","total discounted warranty costs","product life cycle"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R","R","P","R"]}
{"id":"365","title":"Self-organizing feature maps predicting sea levels","abstract":"In this paper, a new method for predicting sea levels employing self-organizing feature maps is introduced. For that purpose the maps are transformed from an unsupervised learning procedure to a supervised one. Two concepts, originally developed to solve the problems of convergence of other network types, are proposed to be applied to Kohonen networks: a functional relationship between the number of neurons and the number of learning examples and a criterion to break off learning. The latter one can be shown to be conform with the process of self-organization by using U-matrices for visualization of the learning procedure. The predictions made using these neural models are compared for accuracy with observations and with the prognoses prepared using six models: two hydrodynamic models, a statistical model, a nearest neighbor model, the persistence model, and the verbal forecasts that are broadcast and kept on record by the Sea Level Forecast Service of the Federal Maritime and Hydrography Agency (BSH) in Hamburg. Before training the maps, the meteorological and oceanographic situation has to be condensed as well as possible, and the weight and learning vectors have to be made as small as possible. The self-organizing feature maps predict sea levels better than all six models of comparison","tok_text":"self-organ featur map predict sea level \n in thi paper , a new method for predict sea level employ self-organ featur map is introduc . for that purpos the map are transform from an unsupervis learn procedur to a supervis one . two concept , origin develop to solv the problem of converg of other network type , are propos to be appli to kohonen network : a function relationship between the number of neuron and the number of learn exampl and a criterion to break off learn . the latter one can be shown to be conform with the process of self-organ by use u-matric for visual of the learn procedur . the predict made use these neural model are compar for accuraci with observ and with the prognos prepar use six model : two hydrodynam model , a statist model , a nearest neighbor model , the persist model , and the verbal forecast that are broadcast and kept on record by the sea level forecast servic of the feder maritim and hydrographi agenc ( bsh ) in hamburg . befor train the map , the meteorolog and oceanograph situat ha to be condens as well as possibl , and the weight and learn vector have to be made as small as possibl . the self-organ featur map predict sea level better than all six model of comparison","ordered_present_kp":[0,337,401,556,569,724,745,763,792,816,877,910,1008,1084],"keyphrases":["self-organizing feature maps","Kohonen networks","neurons","U-matrices","visualization","hydrodynamic models","statistical model","nearest neighbor model","persistence model","verbal forecasts","Sea Level Forecast Service","Federal Maritime and Hydrography Agency","oceanographic situation","learning vectors","sea level prediction","supervised learning","meteorological situation"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","R","R"]}
{"id":"398","title":"An unconditionally stable extended (USE) finite-element time-domain solution of active nonlinear microwave circuits using perfectly matched layers","abstract":"This paper proposes an extension of the unconditionally stable finite-element time-domain (FETD) method for the global electromagnetic analysis of active microwave circuits. This formulation has two advantages. First, the time-step size is no longer governed by the spatial discretization of the mesh, but rather by the Nyquist sampling criterion. Second, the implementation of the truncation by the perfectly matched layers (PML) is straightforward. An anisotropic PML absorbing material is presented for the truncation of FETD lattices. Reflection less than -50 dB is obtained numerically over the entire propagation bandwidth in waveguide and microstrip line. A benchmark test on a microwave amplifier indicates that this extended FETD algorithm is not only superior to finite-difference time-domain-based algorithm in mesh flexibility and simulation accuracy, but also reduces computation time dramatically","tok_text":"an uncondit stabl extend ( use ) finite-el time-domain solut of activ nonlinear microwav circuit use perfectli match layer \n thi paper propos an extens of the uncondit stabl finite-el time-domain ( fetd ) method for the global electromagnet analysi of activ microwav circuit . thi formul ha two advantag . first , the time-step size is no longer govern by the spatial discret of the mesh , but rather by the nyquist sampl criterion . second , the implement of the truncat by the perfectli match layer ( pml ) is straightforward . an anisotrop pml absorb materi is present for the truncat of fetd lattic . reflect less than -50 db is obtain numer over the entir propag bandwidth in waveguid and microstrip line . a benchmark test on a microwav amplifi indic that thi extend fetd algorithm is not onli superior to finite-differ time-domain-bas algorithm in mesh flexibl and simul accuraci , but also reduc comput time dramat","ordered_present_kp":[220,64,408,318,101,533,681,694,734,855,872],"keyphrases":["active nonlinear microwave circuits","perfectly matched layers","global electromagnetic analysis","time-step size","Nyquist sampling criterion","anisotropic PML absorbing material","waveguide","microstrip line","microwave amplifier","mesh flexibility","simulation accuracy","unconditionally stable FETD method","finite-element time-domain method","global EM analysis","PML truncation","FETD lattices truncation","computation time reduction"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","R","R","M","R","R","M"]}
{"id":"2132","title":"Fault-tolerant computer-aided control systems with multiversion-threshold adaptation: adaptation methods, reliability estimation, and choice of an architecture","abstract":"For multiversion majority-redundant computer-aided control systems, systematization of adaptation methods that are stable to hardware and software failures, a method for estimating their reliability from an event graph model, and a method for selecting a standard architecture with regard for reliability requirements are studied","tok_text":"fault-toler computer-aid control system with multiversion-threshold adapt : adapt method , reliabl estim , and choic of an architectur \n for multivers majority-redund computer-aid control system , systemat of adapt method that are stabl to hardwar and softwar failur , a method for estim their reliabl from an event graph model , and a method for select a standard architectur with regard for reliabl requir are studi","ordered_present_kp":[0,45,91,123,141,310],"keyphrases":["fault-tolerant computer-aided control systems","multiversion-threshold adaptation","reliability estimation","architecture","multiversion majority-redundant computer-aided control systems","event graph model","hardware failure stability","software failure stability"],"prmu":["P","P","P","P","P","P","M","M"]}
{"id":"285","title":"Presentation media, information complexity, and learning outcomes","abstract":"Multimedia computing provides a variety of information presentation modality combinations. Educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text-only and text with static pictures in facilitating optimal human information processing and, therefore, comprehension. The article reports the findings from a 3 (text-only, overhead slides, and multimedia presentation)*2 (high and low information complexity) factorial experiment. Subjects read a text script, viewed an acetate overhead slide presentation, or viewed a multimedia presentation depicting the greenhouse effect (low complexity) or photocopier operation (high complexity). Multimedia was superior to text-only and overhead slides for comprehension. Information complexity diminished comprehension and perceived presentation quality. Multimedia was able to reduce the negative impact of information complexity on comprehension and increase the extent of sustained attention to the presentation. These findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity. Moreover, multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention; the quality and effectiveness of information processing attained (i.e., use of verbal and visual working memory) is also significant","tok_text":"present media , inform complex , and learn outcom \n multimedia comput provid a varieti of inform present modal combin . educ have observ that visual enhanc learn which suggest that multimedia present should be superior to text-onli and text with static pictur in facilit optim human inform process and , therefor , comprehens . the articl report the find from a 3 ( text-onli , overhead slide , and multimedia presentation)*2 ( high and low inform complex ) factori experi . subject read a text script , view an acet overhead slide present , or view a multimedia present depict the greenhous effect ( low complex ) or photocopi oper ( high complex ) . multimedia wa superior to text-onli and overhead slide for comprehens . inform complex diminish comprehens and perceiv present qualiti . multimedia wa abl to reduc the neg impact of inform complex on comprehens and increas the extent of sustain attent to the present . these find suggest that multimedia present invok the use of both the verbal and visual work memori channel result in a reduct of the cognit load impos by increas inform complex . moreov , multimedia superior in facilit comprehens goe beyond it abil to increas sustain attent ; the qualiti and effect of inform process attain ( i.e. , use of verbal and visual work memori ) is also signific","ordered_present_kp":[0,16,37,1001,52,90,120,181,246,271,378,490,512,181,582,618,1054,1109,889],"keyphrases":["presentation media","information complexity","learning outcomes","multimedia computing","information presentation modality combinations","educators","multimedia presentations","multimedia presentations","static pictures","optimal human information processing","overhead slides","text script","acetate overhead slide presentation","greenhouse effect","photocopier operation","sustained attention","visual working memory channel","cognitive load","multimedia superiority","cognitive processing limitations","human working memory","verbal working memory channel","multimedia presentation"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","M","R","R","P"]}
{"id":"2177","title":"A humanist's legacy in medical informatics: visions and accomplishments of Professor Jean-Raoul Scherrer","abstract":"The objective is to report on the work of Prof. Jean-Raoul Scherrer, and show how his humanist vision, medical skills and scientific background have enabled and shaped the development of medical informatics over the last 30 years. Starting with the mainframe-based patient-centred hospital information system DIOGENE in the 70s, Prof. Scherrer developed, implemented and evolved innovative concepts of man-machine interfaces, distributed and federated environments, leading the way with information systems that obstinately focused on the support of care providers and patients. Through a rigorous design of terminologies and ontologies, the DIOGENE data would then serve as a basis for the development of clinical research, data mining, and lead to innovative natural language processing techniques. In parallel, Prof. Scherrer supported the development of medical image management, ranging from a distributed picture archiving and communication systems (PACS) to molecular imaging of protein electrophoreses. Recognizing the need for improving the quality and trustworthiness of medical information of the Web, Prof. Scherrer created the Health-On-the Net (HON) foundation. These achievements, made possible thanks to his visionary mind, deep humanism, creativity, generosity and determination, have made of Prof. Scherrer a true pioneer and leader of the human-centered, patient-oriented application of information technology for improving healthcare","tok_text":"a humanist 's legaci in medic informat : vision and accomplish of professor jean-raoul scherrer \n the object is to report on the work of prof. jean-raoul scherrer , and show how hi humanist vision , medic skill and scientif background have enabl and shape the develop of medic informat over the last 30 year . start with the mainframe-bas patient-centr hospit inform system diogen in the 70 , prof. scherrer develop , implement and evolv innov concept of man-machin interfac , distribut and feder environ , lead the way with inform system that obstin focus on the support of care provid and patient . through a rigor design of terminolog and ontolog , the diogen data would then serv as a basi for the develop of clinic research , data mine , and lead to innov natur languag process techniqu . in parallel , prof. scherrer support the develop of medic imag manag , rang from a distribut pictur archiv and commun system ( pac ) to molecular imag of protein electrophores . recogn the need for improv the qualiti and trustworthi of medic inform of the web , prof. scherrer creat the health-on-th net ( hon ) foundat . these achiev , made possibl thank to hi visionari mind , deep human , creativ , generos and determin , have made of prof. scherrer a true pioneer and leader of the human-cent , patient-ori applic of inform technolog for improv healthcar","ordered_present_kp":[66,24,846,921,455,731,761],"keyphrases":["Medical Informatics","Professor Jean-Raoul Scherrer","man-machine interfaces","data mining","natural language processing","medical image management","PACS","mainframe based patient centered hospital information system","Internet","DIOGENE system","distributed systems","federated systems"],"prmu":["P","P","P","P","P","P","P","M","U","R","R","R"]}
{"id":"1943","title":"I-WAP: an intelligent WAP site management system","abstract":"The popularity regarding wireless communications is such that more and more WAP sites have been developed with wireless markup language (WML). Meanwhile, to translate hypertext markup language (HTML) pages into proper WML ones becomes imperative since it is difficult for WAP users to read most contents designed for PC users via their mobile phone screens. However, for those sites that have been maintained with hypertext markup language (HTML), considerable time and manpower costs will be incurred to rebuild them with WML. In this paper, we propose an intelligent WAP site management system to cope with these problems. With the help of the intelligent management system, the original contents of HTML Web sites can be automatically translated to proper WAP content in an efficient way. As a consequence, the costs associated with maintaining WAP sites could be significantly reduced. The management system also allows the system manager to define the relevance of numerals and keywords for removing unimportant or meaningless contents. The original contents will be reduced and reorganized to fit the size of mobile phone screens, thus reducing the communication cost and enhancing readability. Numerical results gained through various experiments have evinced the effective performance of the WAP management system","tok_text":"i-wap : an intellig wap site manag system \n the popular regard wireless commun is such that more and more wap site have been develop with wireless markup languag ( wml ) . meanwhil , to translat hypertext markup languag ( html ) page into proper wml one becom imper sinc it is difficult for wap user to read most content design for pc user via their mobil phone screen . howev , for those site that have been maintain with hypertext markup languag ( html ) , consider time and manpow cost will be incur to rebuild them with wml . in thi paper , we propos an intellig wap site manag system to cope with these problem . with the help of the intellig manag system , the origin content of html web site can be automat translat to proper wap content in an effici way . as a consequ , the cost associ with maintain wap site could be significantli reduc . the manag system also allow the system manag to defin the relev of numer and keyword for remov unimport or meaningless content . the origin content will be reduc and reorgan to fit the size of mobil phone screen , thu reduc the commun cost and enhanc readabl . numer result gain through variou experi have evinc the effect perform of the wap manag system","ordered_present_kp":[11,0,63,138,195,350,1077,1100],"keyphrases":["I-WAP","intelligent WAP site management system","wireless communication","wireless markup language","hypertext markup language","mobile phone","communication cost","readability","HTML pages","wireless mobile Internet"],"prmu":["P","P","P","P","P","P","P","P","R","M"]}
{"id":"278","title":"Novel ZE-isomerism descriptors derived from molecular topology and their application to QSAR analysis","abstract":"We introduce several series of novel ZE-isomerism descriptors derived directly from two-dimensional molecular topology. These descriptors make use of a quantity named ZE-isomerism correction, which is added to the vertex degrees of atoms connected by double bonds in Z and E configurations. This approach is similar to the one described previously for topological chirality descriptors (Golbraikh, A., et al. J. Chem. Inf. Comput. Sci. 2001, 41, 147-158). The ZE-isomerism descriptors include modified molecular connectivity indices, overall Zagreb indices, extended connectivity, overall connectivity, and topological charge indices. They can be either real or complex numbers. Mathematical properties of different subgroups of ZE-isomerism descriptors are discussed. These descriptors circumvent the inability of conventional topological indices to distinguish between Z and E isomers. The applicability of ZE-isomerism descriptors to QSAR analysis is demonstrated in the studies of a series of 131 anticancer agents inhibiting tubulin polymerization","tok_text":"novel ze-isomer descriptor deriv from molecular topolog and their applic to qsar analysi \n we introduc sever seri of novel ze-isomer descriptor deriv directli from two-dimension molecular topolog . these descriptor make use of a quantiti name ze-isomer correct , which is ad to the vertex degre of atom connect by doubl bond in z and e configur . thi approach is similar to the one describ previous for topolog chiral descriptor ( golbraikh , a. , et al . j. chem . inf . comput . sci . 2001 , 41 , 147 - 158 ) . the ze-isomer descriptor includ modifi molecular connect indic , overal zagreb indic , extend connect , overal connect , and topolog charg indic . they can be either real or complex number . mathemat properti of differ subgroup of ze-isomer descriptor are discuss . these descriptor circumv the inabl of convent topolog indic to distinguish between z and e isom . the applic of ze-isomer descriptor to qsar analysi is demonstr in the studi of a seri of 131 anticanc agent inhibit tubulin polymer","ordered_present_kp":[6,164,76,243,282,545,578,600,617,638,687,970,993],"keyphrases":["ZE-isomerism descriptors","QSAR analysis","two-dimensional molecular topology","ZE-isomerism correction","vertex degrees","modified molecular connectivity indices","overall Zagreb indices","extended connectivity","overall connectivity","topological charge indices","complex numbers","anticancer agents","tubulin polymerization","quantitative structure-activity relationship","double bond connected atoms","descriptor pharmacophore","chemical databases","molecular graphs","computer-assisted drug design","toxicities","combinatorial chemical libraries"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","U","R","M","U","M","U","U","U"]}
{"id":"245","title":"Support vector machines model for classification of thermal error in machine tools","abstract":"This paper addresses a change in the concept of machine tool thermal error prediction which has been hitherto carried out by directly mapping them with the temperature of critical elements on the machine. The model developed herein using support vector machines, a powerful data-training algorithm, seeks to account for the impact of specific operating conditions, in addition to temperature variation, on the effective prediction of thermal errors. Several experiments were conducted to study the error pattern, which was found to change significantly with variation in operating conditions. This model attempts to classify the error based on operating conditions. Once classified, the error is then predicted based on the temperature states. This paper also briefly describes the concept of the implementation of such a comprehensive model along with an on-line error assessment and calibration system in a PC-based open-architecture controller environment, so that it could be employed in regular production for the purpose of periodic calibration of machine tools","tok_text":"support vector machin model for classif of thermal error in machin tool \n thi paper address a chang in the concept of machin tool thermal error predict which ha been hitherto carri out by directli map them with the temperatur of critic element on the machin . the model develop herein use support vector machin , a power data-train algorithm , seek to account for the impact of specif oper condit , in addit to temperatur variat , on the effect predict of thermal error . sever experi were conduct to studi the error pattern , which wa found to chang significantli with variat in oper condit . thi model attempt to classifi the error base on oper condit . onc classifi , the error is then predict base on the temperatur state . thi paper also briefli describ the concept of the implement of such a comprehens model along with an on-lin error assess and calibr system in a pc-base open-architectur control environ , so that it could be employ in regular product for the purpos of period calibr of machin tool","ordered_present_kp":[0,118,321,511,872],"keyphrases":["support vector machines model","machine tool thermal error prediction","data-training algorithm","error pattern","PC-based open-architecture controller environment","SVM","thermal error classification","critical element temperature","online error assessment","online calibration system"],"prmu":["P","P","P","P","P","U","R","R","M","M"]}
{"id":"200","title":"Preintegration lateral inhibition enhances unsupervised learning","abstract":"A large and influential class of neural network architectures uses postintegration lateral inhibition as a mechanism for competition. We argue that these algorithms are computationally deficient in that they fail to generate, or learn, appropriate perceptual representations under certain circumstances. An alternative neural network architecture is presented here in which nodes compete for the right to receive inputs rather than for the right to generate outputs. This form of competition, implemented through preintegration lateral inhibition, does provide appropriate coding properties and can be used to learn such representations efficiently. Furthermore, this architecture is consistent with both neuroanatomical and neuropsychological data. We thus argue that preintegration lateral inhibition has computational advantages over conventional neural network architectures while remaining equally biologically plausible","tok_text":"preintegr later inhibit enhanc unsupervis learn \n a larg and influenti class of neural network architectur use postintegr later inhibit as a mechan for competit . we argu that these algorithm are comput defici in that they fail to gener , or learn , appropri perceptu represent under certain circumst . an altern neural network architectur is present here in which node compet for the right to receiv input rather than for the right to gener output . thi form of competit , implement through preintegr later inhibit , doe provid appropri code properti and can be use to learn such represent effici . furthermor , thi architectur is consist with both neuroanatom and neuropsycholog data . we thu argu that preintegr later inhibit ha comput advantag over convent neural network architectur while remain equal biolog plausibl","ordered_present_kp":[80,111,152,0,80,31],"keyphrases":["preintegration lateral inhibition","unsupervised learning","neural network architectures","neural network","postintegration lateral inhibition","competition"],"prmu":["P","P","P","P","P","P"]}
{"id":"1983","title":"Power electronics spark new simulation challenges","abstract":"This article discusses some of the changes that have taken place in power systems and explores some of the inherent requirements for simulation technologies in order to keep up with this rapidly changing environment. The authors describe how energy utilities are realizing that, with the appropriate tools, they can train and sustain engineers who can maintain a great insight into system dynamics","tok_text":"power electron spark new simul challeng \n thi articl discuss some of the chang that have taken place in power system and explor some of the inher requir for simul technolog in order to keep up with thi rapidli chang environ . the author describ how energi util are realiz that , with the appropri tool , they can train and sustain engin who can maintain a great insight into system dynam","ordered_present_kp":[0,25,157],"keyphrases":["power electronics","simulation challenges","simulation technologies","power system computer simulation","electric utilities"],"prmu":["P","P","P","M","M"]}
{"id":"2057","title":"Four factors influencing the fair market value of out-of print books. 2","abstract":"Fot pt.1 see ibid., p.71-8 (2002). Data from the fifty-six titles examined qualitatively in the Patterson study are examined quantitatively. In addition to the four factors of edition, condition, dust jacket, and autograph that were hypothesized to influence the value of a book, four other factors for which information was available in the data were examined","tok_text":"four factor influenc the fair market valu of out-of print book . 2 \n fot pt.1 see ibid . , p.71 - 8 ( 2002 ) . data from the fifty-six titl examin qualit in the patterson studi are examin quantit . in addit to the four factor of edit , condit , dust jacket , and autograph that were hypothes to influenc the valu of a book , four other factor for which inform wa avail in the data were examin","ordered_present_kp":[25],"keyphrases":["fair market value","out-of-print books","quantitative analysis","pricing","economics","publisher"],"prmu":["P","M","M","U","U","U"]}
{"id":"2012","title":"Strain contouring using Gabor filters: principle and algorithm","abstract":"Moire interferometry is a powerful technique for high sensitivity in-plane deformation contouring. However, from an engineering viewpoint, the derivatives of displacement, i.e., strain, are the desired parameter. Thus there is a need to differentiate the displacement field. Optical and digital methods have been proposed for this differentiation. Optical methods provide contours that still need to be quantified, while digital methods suffer from drawbacks inherent in the digital differentiation process. We describe a novel approach of strain segmentation for the moire pattern using a multichannel Gabor filter. Appropriate filter design allows for user-specific segmentation, which is essentially in engineering design and analysis","tok_text":"strain contour use gabor filter : principl and algorithm \n moir interferometri is a power techniqu for high sensit in-plan deform contour . howev , from an engin viewpoint , the deriv of displac , i.e. , strain , are the desir paramet . thu there is a need to differenti the displac field . optic and digit method have been propos for thi differenti . optic method provid contour that still need to be quantifi , while digit method suffer from drawback inher in the digit differenti process . we describ a novel approach of strain segment for the moir pattern use a multichannel gabor filter . appropri filter design allow for user-specif segment , which is essenti in engin design and analysi","ordered_present_kp":[0,19,47,59,103,187,275,301,352,260,466,524,566,603,627,669],"keyphrases":["strain contouring","Gabor filters","algorithm","moire interferometry","high sensitivity in-plane deformation contouring","displacement","differentiation","displacement field","digital methods","optical methods","digital differentiation process","strain segmentation","multichannel Gabor filter","filter design","user-specific segmentation","engineering design","engineering analysis","image segmentation","spatial filters"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R","M","M"]}
{"id":"358","title":"Correlation of intuitionistic fuzzy sets by centroid method","abstract":"In this paper, we propose a method to calculate the correlation coefficient of intuitionistic fuzzy sets by means of \"centroid\". This value obtained from our formula tell us not only the strength of relationship between the intuitionistic fuzzy sets, but also whether the intuitionistic fuzzy sets are positively or negatively related. This approach looks better than previous methods which only evaluate the strength of the relation. Furthermore, we extend the \"centroid\" method to interval-valued intuitionistic fuzzy sets. The value of the correlation coefficient between interval-valued intuitionistic fuzzy sets lies in the interval [-1, 1], as computed from our formula","tok_text":"correl of intuitionist fuzzi set by centroid method \n in thi paper , we propos a method to calcul the correl coeffici of intuitionist fuzzi set by mean of \" centroid \" . thi valu obtain from our formula tell us not onli the strength of relationship between the intuitionist fuzzi set , but also whether the intuitionist fuzzi set are posit or neg relat . thi approach look better than previou method which onli evalu the strength of the relat . furthermor , we extend the \" centroid \" method to interval-valu intuitionist fuzzi set . the valu of the correl coeffici between interval-valu intuitionist fuzzi set lie in the interv [ -1 , 1 ] , as comput from our formula","ordered_present_kp":[102,10,36,495],"keyphrases":["intuitionistic fuzzy sets","centroid method","correlation coefficient","interval-valued intuitionistic fuzzy sets"],"prmu":["P","P","P","P"]}
{"id":"32","title":"Analysis and efficient implementation of a linguistic fuzzy c-means","abstract":"The paper is concerned with a linguistic fuzzy c-means (FCM) algorithm with vectors of fuzzy numbers as inputs. This algorithm is based on the extension principle and the decomposition theorem. It turns out that using the extension principle to extend the capability of the standard membership update equation to deal with a linguistic vector has a huge computational complexity. In order to cope with this problem, an efficient method based on fuzzy arithmetic and optimization has been developed and analyzed. We also carefully examine and prove that the algorithm behaves in a way similar to the FCM in the degenerate linguistic case. Synthetic data sets and the iris data set have been used to illustrate the behavior of this linguistic version of the FCM","tok_text":"analysi and effici implement of a linguist fuzzi c-mean \n the paper is concern with a linguist fuzzi c-mean ( fcm ) algorithm with vector of fuzzi number as input . thi algorithm is base on the extens principl and the decomposit theorem . it turn out that use the extens principl to extend the capabl of the standard membership updat equat to deal with a linguist vector ha a huge comput complex . in order to cope with thi problem , an effici method base on fuzzi arithmet and optim ha been develop and analyz . we also care examin and prove that the algorithm behav in a way similar to the fcm in the degener linguist case . synthet data set and the iri data set have been use to illustr the behavior of thi linguist version of the fcm","ordered_present_kp":[141,194,218,381,459,478,355],"keyphrases":["fuzzy numbers","extension principle","decomposition theorem","linguistic vectors","computational complexity","fuzzy arithmetic","optimization","linguistic fuzzy c-means algorithm"],"prmu":["P","P","P","P","P","P","P","R"]}
{"id":"261","title":"Union outreach - a pilgrim's progress","abstract":"As the American labor movement continues on its path toward reorganization and rejuvenation, archivists are challenged to ensure that the organizational, political, and cultural changes labor unions are experiencing are fully documented. The article examines the need for labor archivists to reach out actively to unions and the problems they face in getting their message across, not only to union leadership but also to union members. Outreach by labor archivists is vital on three critical fronts: the need to secure union funding in support of labor archival programs; obtaining union cooperation in reviewing and amending obsolete deposit agreements; and coordinating efforts with unions to save the records of closing district and local union offices. Attempting to resolve these outstanding issues, labor archivists are pulled between two distinct institutional cultures (one academic in nature, the other enmeshed in a union bureaucracy) and often have their own labor archival programs compromised by the internal dynamics and politics inherent in administering large academic libraries and unions. If labor archivists are to be successful, they must find their collective voice within the labor movement and establish their relevancy to unions during a period of momentous change and restructuring. Moreover, archivists need to give greater thought to designing and implementing outreach programs that bridge the fundamental \"disconnect\" between union bureaucracies and the rank and file, and unions and the public","tok_text":"union outreach - a pilgrim 's progress \n as the american labor movement continu on it path toward reorgan and rejuven , archivist are challeng to ensur that the organiz , polit , and cultur chang labor union are experienc are fulli document . the articl examin the need for labor archivist to reach out activ to union and the problem they face in get their messag across , not onli to union leadership but also to union member . outreach by labor archivist is vital on three critic front : the need to secur union fund in support of labor archiv program ; obtain union cooper in review and amend obsolet deposit agreement ; and coordin effort with union to save the record of close district and local union offic . attempt to resolv these outstand issu , labor archivist are pull between two distinct institut cultur ( one academ in natur , the other enmesh in a union bureaucraci ) and often have their own labor archiv program compromis by the intern dynam and polit inher in administ larg academ librari and union . if labor archivist are to be success , they must find their collect voic within the labor movement and establish their relev to union dure a period of moment chang and restructur . moreov , archivist need to give greater thought to design and implement outreach program that bridg the fundament \" disconnect \" between union bureaucraci and the rank and file , and union and the public","ordered_present_kp":[48,120,183,196,274,385,414,508,533,563,596,701,801,863,946,987,1079],"keyphrases":["American labor movement","archivists","cultural changes","labor unions","labor archivists","union leadership","union members","union funding","labor archival programs","union cooperation","obsolete deposit agreements","union offices","institutional cultures","union bureaucracy","internal dynamics","large academic libraries","collective voice","political changes"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","P","R"]}
{"id":"2193","title":"Integrating virtual and physical context to support knowledge workers","abstract":"The Kimura system augments and integrates independent tools into a pervasive computing system that monitors a user's interactions with the computer, an electronic whiteboard, and a variety of networked peripheral devices and data sources","tok_text":"integr virtual and physic context to support knowledg worker \n the kimura system augment and integr independ tool into a pervas comput system that monitor a user 's interact with the comput , an electron whiteboard , and a varieti of network peripher devic and data sourc","ordered_present_kp":[121,45,234,195,67,261],"keyphrases":["knowledge workers","Kimura system","pervasive computing","electronic whiteboard","networked peripheral devices","data sources"],"prmu":["P","P","P","P","P","P"]}
{"id":"224","title":"Java portability put to the test","abstract":"Sun Microsystems' recently launched Java Verification Program aims to enable companies to assess the cross-platform portability of applications written in Java, and to help software vendors ensure that their solutions can run in heterogenous J2EE application server environments","tok_text":"java portabl put to the test \n sun microsystem ' recent launch java verif program aim to enabl compani to assess the cross-platform portabl of applic written in java , and to help softwar vendor ensur that their solut can run in heterogen j2ee applic server environ","ordered_present_kp":[31,63,117],"keyphrases":["Sun Microsystems","Java Verification Program","cross-platform portability"],"prmu":["P","P","P"]}
{"id":"339","title":"An automated parallel image registration technique based on the correlation of wavelet features","abstract":"With the increasing importance of multiple multiplatform remote sensing missions, fast and automatic integration of digital data from disparate sources has become critical to the success of these endeavors. Our work utilizes maxima of wavelet coefficients to form the basic features of a correlation-based automatic registration algorithm. Our wavelet-based registration algorithm is tested successfully with data from the National Oceanic and Atmospheric Administration (NOAA) Advanced Very High Resolution Radiometer (AVHRR) and the Landsat Thematic Mapper (TM), which differ by translation and\/or rotation. By the choice of high-frequency wavelet features, this method is similar to an edge-based correlation method, but by exploiting the multiresolution nature of a wavelet decomposition, our method achieves higher computational speeds for comparable accuracies. This algorithm has been implemented on a single-instruction multiple-data (SIMD) massively parallel computer, the MasPar MP-2, as well as on the CrayT3D, the Cray T3E, and a Beowulf cluster of Pentium workstations","tok_text":"an autom parallel imag registr techniqu base on the correl of wavelet featur \n with the increas import of multipl multiplatform remot sens mission , fast and automat integr of digit data from dispar sourc ha becom critic to the success of these endeavor . our work util maxima of wavelet coeffici to form the basic featur of a correlation-bas automat registr algorithm . our wavelet-bas registr algorithm is test success with data from the nation ocean and atmospher administr ( noaa ) advanc veri high resolut radiomet ( avhrr ) and the landsat themat mapper ( tm ) , which differ by translat and\/or rotat . by the choic of high-frequ wavelet featur , thi method is similar to an edge-bas correl method , but by exploit the multiresolut natur of a wavelet decomposit , our method achiev higher comput speed for compar accuraci . thi algorithm ha been implement on a single-instruct multiple-data ( simd ) massiv parallel comput , the maspar mp-2 , as well as on the crayt3d , the cray t3e , and a beowulf cluster of pentium workstat","ordered_present_kp":[3,52,62,128,343,522,538,749],"keyphrases":["automated parallel image registration","correlation","wavelet feature","remote sensing","automatic registration algorithm","AVHRR","Landsat Thematic Mapper","wavelet decomposition","geophysical measurement technique","land surface","terrain mapping","optical imaging","microwave radiometry","image processing","SIMD massively parallel computing"],"prmu":["P","P","P","P","P","P","P","P","M","U","U","M","U","M","R"]}
{"id":"414","title":"The efficacy of electronic telecommunications in fostering interpersonal relationships","abstract":"The effectiveness of electronic telecommunications as a supplementary aid to instruction and as a communication link between students, and between students and instructors in fostering interpersonal relationships was explored in this study. More specifically, the impacts of e-mail, one of the most accessible, convenient, and easy to use computer-mediated communications, on student attitudes toward the instructor, group-mates, and other classmates were investigated. A posttest-only experimental design was adopted. In total, 68 prospective teachers enrolling in a \"Computers in Education\" course participated in the study for a whole semester. Results from the study provided substantial evidence supporting e-mail's beneficial effects on student attitudes toward the instructor and other classmates","tok_text":"the efficaci of electron telecommun in foster interperson relationship \n the effect of electron telecommun as a supplementari aid to instruct and as a commun link between student , and between student and instructor in foster interperson relationship wa explor in thi studi . more specif , the impact of e-mail , one of the most access , conveni , and easi to use computer-medi commun , on student attitud toward the instructor , group-mat , and other classmat were investig . a posttest-onli experiment design wa adopt . in total , 68 prospect teacher enrol in a \" comput in educ \" cours particip in the studi for a whole semest . result from the studi provid substanti evid support e-mail 's benefici effect on student attitud toward the instructor and other classmat","ordered_present_kp":[46,25,304,364,390],"keyphrases":["telecommunications","interpersonal relationships","e-mail","computer-mediated communications","student attitudes","student communication link","Computers in Education course","educational technology"],"prmu":["P","P","P","P","P","R","R","M"]}
{"id":"2036","title":"Computer processing of data on mental impairments during the acute period of concussion","abstract":"The article presents results of computer processing of experimental information obtained from patients during the acute period of concussion. A number of computational procedures are described","tok_text":"comput process of data on mental impair dure the acut period of concuss \n the articl present result of comput process of experiment inform obtain from patient dure the acut period of concuss . a number of comput procedur are describ","ordered_present_kp":[0,26,49,205],"keyphrases":["computer processing","mental impairments","acute period of concussion","computational procedures"],"prmu":["P","P","P","P"]}
{"id":"381","title":"Robust fuzzy controlled photovoltaic power inverter with Taguchi method","abstract":"This paper presents design and implementation of a robust fuzzy controlled photovoltaic (PV) power inverter with Taguchi tuned scaling factors. To achieve fast transient response, small steady-state error and system robustness, a robust fuzzy controller is adopted, in which its input and output scaling factors are determined efficiently by using the Taguchi-tuning algorithm. The proposed system can operate in different modes, grid-connection mode and stand-alone mode, and can accommodate wide load variations. Simulation results and hardware measurements obtained from a prototype with a microcontroller (Intel 80196KC) are presented to verify the theoretical discussions, and its adaptivity, robustness and feasibility","tok_text":"robust fuzzi control photovolta power invert with taguchi method \n thi paper present design and implement of a robust fuzzi control photovolta ( pv ) power invert with taguchi tune scale factor . to achiev fast transient respons , small steady-st error and system robust , a robust fuzzi control is adopt , in which it input and output scale factor are determin effici by use the taguchi-tun algorithm . the propos system can oper in differ mode , grid-connect mode and stand-alon mode , and can accommod wide load variat . simul result and hardwar measur obtain from a prototyp with a microcontrol ( intel 80196kc ) are present to verifi the theoret discuss , and it adapt , robust and feasibl","ordered_present_kp":[0,50,176,211,237,257,329,448,470,510,586,668,687],"keyphrases":["robust fuzzy controlled photovoltaic power inverter","Taguchi method","tuned scaling factors","transient response","steady-state error","system robustness","output scaling factors","grid-connection mode","stand-alone mode","load variations","microcontroller","adaptivity","feasibility"],"prmu":["P","P","P","P","P","P","P","P","P","P","P","P","P"]}
{"id":"304","title":"A Web-accessible database of characteristics of the 1,945 basic Japanese kanji","abstract":"In 1981, the Japanese government published a list of the 1,945 basic Japanese kanji (Jooyoo Kanji-hyo), including specifications of pronunciation. This list was established as the standard for kanji usage in print. The database for 1,945 basic Japanese kanji provides 30 cells that explain in detail the various characteristics of kanji. Means, standard deviations, distributions, and information related to previous research concerning these kanji are provided in this paper. The database is saved as a Microsoft Excel 2000 file for Windows. This kanji database is accessible on the Web site of the Oxford Text Archive, Oxford University (http:\/\/ota.ahds.ac.uk). Using this database, researchers and educators will be able to conduct planned experiments and organize classroom instruction on the basis of the known characteristics of selected kanji","tok_text":"a web-access databas of characterist of the 1,945 basic japanes kanji \n in 1981 , the japanes govern publish a list of the 1,945 basic japanes kanji ( jooyoo kanji-hyo ) , includ specif of pronunci . thi list wa establish as the standard for kanji usag in print . the databas for 1,945 basic japanes kanji provid 30 cell that explain in detail the variou characterist of kanji . mean , standard deviat , distribut , and inform relat to previou research concern these kanji are provid in thi paper . the databas is save as a microsoft excel 2000 file for window . thi kanji databas is access on the web site of the oxford text archiv , oxford univers ( http:\/\/ota.ahds.ac.uk ) . use thi databas , research and educ will be abl to conduct plan experi and organ classroom instruct on the basi of the known characterist of select kanji","ordered_present_kp":[2,50,151,189,316,379,386,404,524,759],"keyphrases":["Web-accessible database","basic Japanese kanji","Jooyoo Kanji-hyo","pronunciation","cells","means","standard deviations","distributions","Microsoft Excel 2000 file for Windows","classroom instruction","kanji usage print","Oxford Text Archive Web site"],"prmu":["P","P","P","P","P","P","P","P","P","P","R","R"]}
{"id":"341","title":"How should team captains order golfers on the final day of the Ryder Cup matches?","abstract":"I used game theory to examine how team captains should select their slates for the final day of the Ryder Cup matches. Under the assumption that golfers have different abilities and are not influenced by pressure or momentum, I found that drawing names from a hat will do no worse than any other strategy","tok_text":"how should team captain order golfer on the final day of the ryder cup match ? \n i use game theori to examin how team captain should select their slate for the final day of the ryder cup match . under the assumpt that golfer have differ abil and are not influenc by pressur or momentum , i found that draw name from a hat will do no wors than ani other strategi","ordered_present_kp":[87,146],"keyphrases":["game theory","slate","golf","golfer ordering","Ryder Cup final day"],"prmu":["P","P","U","R","R"]}
{"id":"219","title":"Firewall card shields data","abstract":"The SlotShield 3000 firewall on a PCI card saves power and space, but might not offer enough security for large networks","tok_text":"firewal card shield data \n the slotshield 3000 firewal on a pci card save power and space , but might not offer enough secur for larg network","ordered_present_kp":[31,60,119,129],"keyphrases":["SlotShield 3000 firewall","PCI card","security","large networks"],"prmu":["P","P","P","P"]}
{"id":"1967","title":"Modeling daily realized futures volatility with singular spectrum analysis","abstract":"Using singular spectrum analysis (SSA), we model the realized volatility and logarithmic standard deviations of two important futures return series. The realized volatility and logarithmic standard deviations are constructed following the methodology of Andersen et al. [J. Am. Stat. Ass. 96 (2001) 42-55] using intra-day transaction data. We find that SSA decomposes the volatility series quite well and effectively captures both the market trend (accounting for about 34-38% of the total variance in the series) and, more importantly, a number of underlying market periodicities. Reliable identification of any periodicities is extremely important for options pricing and risk management and we believe that SSA can be a useful addition to the financial practitioners' toolbox","tok_text":"model daili realiz futur volatil with singular spectrum analysi \n use singular spectrum analysi ( ssa ) , we model the realiz volatil and logarithm standard deviat of two import futur return seri . the realiz volatil and logarithm standard deviat are construct follow the methodolog of andersen et al . [ j. am . stat . ass . 96 ( 2001 ) 42 - 55 ] use intra-day transact data . we find that ssa decompos the volatil seri quit well and effect captur both the market trend ( account for about 34 - 38 % of the total varianc in the seri ) and , more importantli , a number of underli market period . reliabl identif of ani period is extrem import for option price and risk manag and we believ that ssa can be a use addit to the financi practition ' toolbox","ordered_present_kp":[6,38,98,138,184,458,581,665,648,725],"keyphrases":["daily realized futures volatility","singular spectrum analysis","SSA","logarithmic standard deviations","return series","market trend","market periodicities","options pricing","risk management","financial practitioners","intraday transaction data","econophysics","asset return"],"prmu":["P","P","P","P","P","P","P","P","P","P","M","U","M"]}
{"id":"2116","title":"Optimization of the characteristics of computational processes in scalable resources","abstract":"The scalableness of resources is taken to mean the possibility of the prior change in the obtained dynamic characteristics of computational processes for a certain basic set of processors and the communication medium in an effort to optimize the dynamics of software applications. A method is put forward for the generation of optimal strategies-a set of the versions of the fulfillment of programs on the basis of a vector criterion. The method is urgent for the effective use of resources of computational clusters and metacomputational media and also for dynamic control of processes in real time on the basis of the static scaling","tok_text":"optim of the characterist of comput process in scalabl resourc \n the scalabl of resourc is taken to mean the possibl of the prior chang in the obtain dynam characterist of comput process for a certain basic set of processor and the commun medium in an effort to optim the dynam of softwar applic . a method is put forward for the gener of optim strategies-a set of the version of the fulfil of program on the basi of a vector criterion . the method is urgent for the effect use of resourc of comput cluster and metacomput media and also for dynam control of process in real time on the basi of the static scale","ordered_present_kp":[29,47,150,232,281,419,492,511,541,598],"keyphrases":["computational processes","scalable resources","dynamic characteristics","communication medium","software applications","vector criterion","computational clusters","metacomputational media","dynamic control","static scaling","optimal strategies"],"prmu":["P","P","P","P","P","P","P","P","P","P","M"]}
{"id":"2153","title":"Post-haste. 100th robotic containerization system installed in US mail sorting center","abstract":"Spot welding, machine tending, material handling, picking, packing, painting, palletizing, assembly...the list of tasks being performed by ABB robots keeps on growing. Adding to this portfolio is a new robot containerization system (RCS) that ABB developed specifically for the United States Postal Service (USPS). The RCS has brought new levels of speed, accuracy, efficiency and productivity to the process of sorting and containerizing mail and packages. Recently, the 100th ABB RCS was installed at the USPS processing and distribution center in Columbus, Ohio","tok_text":"post-hast . 100th robot container system instal in us mail sort center \n spot weld , machin tend , materi handl , pick , pack , paint , pallet , assembl ... the list of task be perform by abb robot keep on grow . ad to thi portfolio is a new robot container system ( rc ) that abb develop specif for the unit state postal servic ( usp ) . the rc ha brought new level of speed , accuraci , effici and product to the process of sort and container mail and packag . recent , the 100th abb rc wa instal at the usp process and distribut center in columbu , ohio","ordered_present_kp":[54,18,188,304,54],"keyphrases":["robotic containerization system","mail sorting center","mail sorting","ABB robots","United States Postal Service","USA","packages sorting"],"prmu":["P","P","P","P","P","U","R"]}
